import sys
import os
from pathlib import Path

# –î–æ–¥–∞—î–º–æ –∫–æ—Ä—ñ–Ω—å –ø—Ä–æ–µ–∫—Ç—É –≤ —à–ª—è—Ö, —â–æ–± Python –ø–æ–±–∞—á–∏–≤ –ø–∞–ø–∫—É 'supertonic'
sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.data.tokenizer import CharacterTokenizer

# –Ü–º—ñ—Ç—É—î–º–æ —Ç–≤—ñ–π –∫–æ–Ω—Ñ—ñ–≥
print("Initializing tokenizer with ['uk', 'en']...")
tokenizer = CharacterTokenizer(languages=["uk", "en"])

uk_text = "–ü—Ä–∏–≤—ñ—Ç, —è–∫ —Å–ø—Ä–∞–≤–∏?"
en_text = "Hello, how are you?"

uk_ids = tokenizer.encode(uk_text)
en_ids = tokenizer.encode(en_text)

print(f"\nVocab size: {tokenizer.vocab_size}")

print(f"\nüá∫üá¶ UK: '{uk_text}'")
print(f"IDs: {uk_ids}")

print(f"\nüá¨üáß EN: '{en_text}'")
print(f"IDs: {en_ids}")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ UNKNOWN
# –ó–∞–∑–≤–∏—á–∞–π ID=0 - —Ü–µ padding, ID=1/2 - —Ü–µ unknown (–∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó)
unique_en = set(en_ids.tolist())
unique_uk = set(uk_ids.tolist())

print(f"\nUnique EN tokens: {len(unique_en)}")
print(f"Unique UK tokens: {len(unique_uk)}")

if len(unique_en) <= 2:
    print("\n‚ùå –ü–ò–ó–î–ê! –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ –º–æ–≤–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –Ω–∞ —Å–º—ñ—Ç—Ç—è (–æ–¥–Ω–∞–∫–æ–≤—ñ —Ç–æ–∫–µ–Ω–∏)!")
    print("–¢–≤–æ—è –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ –≤—á–∏—Ç–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É, –±–æ –Ω–µ –±–∞—á–∏—Ç—å –ª—ñ—Ç–µ—Ä.")
elif len(unique_uk) <= 2:
    print("\n‚ùå –ü–ò–ó–î–ê! –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –Ω–∞ —Å–º—ñ—Ç—Ç—è!")
else:
    print("\n‚úÖ –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –ø—Ä–∞—Ü—é—î –∫–æ—Ä–µ–∫—Ç–Ω–æ. –õ—ñ—Ç–µ—Ä–∏ –∫–æ–¥—É—é—Ç—å—Å—è —Ä—ñ–∑–Ω–∏–º–∏ —Ü–∏—Ñ—Ä–∞–º–∏.")