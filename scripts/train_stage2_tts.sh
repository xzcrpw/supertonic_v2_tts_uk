#!/bin/bash
# RTX PRO 6000 - Stage 2: Text-to-Latent Training
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î pretrained autoencoder –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É

set -e

echo "üß† Stage 2: Text-to-Latent Training"
echo "==============================================="
echo "Autoencoder: checkpoint_75000.pt"
echo "Batch size: 64 (optimal for 96GB)"
echo "==============================================="

# CUDA optimization
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=0
export WANDB_MODE=disabled

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ–∫–ø–æ—ñ–Ω—Ç–∞
AUTOENCODER_CKPT="checkpoints/autoencoder/checkpoint_75000.pt"

if [ ! -f "$AUTOENCODER_CKPT" ]; then
    # –®—É–∫–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —á–µ–∫–ø–æ—ñ–Ω—Ç
    AUTOENCODER_CKPT=$(ls -t checkpoints/autoencoder/checkpoint_*.pt 2>/dev/null | head -1)
    if [ -z "$AUTOENCODER_CKPT" ]; then
        echo "‚ùå No autoencoder checkpoint found!"
        exit 1
    fi
    echo "üì¶ Using latest checkpoint: $AUTOENCODER_CKPT"
fi

# Training
python train_text_to_latent.py \
    --config config/rtx6000_optimal.yaml \
    --autoencoder-checkpoint "$AUTOENCODER_CKPT" \
    --batch-size 64 \
    --no-wandb

echo "‚úÖ Training complete!"
