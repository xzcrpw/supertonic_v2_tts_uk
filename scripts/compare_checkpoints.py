#!/usr/bin/env python3
"""
–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–≤–æ—Ö checkpoint'—ñ–≤ –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä–∞.
–ì–µ–Ω–µ—Ä—É—î audio –¥–ª—è –æ–¥–Ω–∏—Ö —ñ —Ç–∏—Ö —Å–∞–º–∏—Ö —Ñ–∞–π–ª—ñ–≤ —ñ –ø–æ–∫–∞–∑—É—î –º–µ—Ç—Ä–∏–∫–∏.
"""

import torch
import torchaudio
import json
from pathlib import Path
import sys
import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.speech_autoencoder import SpeechAutoencoder

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model(checkpoint_path):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å –∑ checkpoint."""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    config = checkpoint.get("config", {})
    audio_cfg = config.get("audio", {})
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –∑ –¥–µ—Ñ–æ–ª—Ç–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (latent_dim=24)
    model = SpeechAutoencoder(
        sample_rate=audio_cfg.get("sample_rate", 44100),
        n_fft=audio_cfg.get("n_fft", 2048),
        hop_length=audio_cfg.get("hop_length", 512),
        n_mels=228,
        latent_dim=24,
        hidden_dim=512,
    ).to(device)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∞–≥–∏
    model.encoder.load_state_dict(checkpoint["encoder"])
    model.decoder.load_state_dict(checkpoint["decoder"])
    model.eval()
    
    iteration = checkpoint.get("iteration", "?")
    return model, iteration


def analyze_frequency(audio, sr=44100):
    """–ê–Ω–∞–ª—ñ–∑—É—î —á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ø–µ–∫—Ç—Ä."""
    spec_transform = torchaudio.transforms.Spectrogram(n_fft=2048, hop_length=512, power=2)
    spec = spec_transform(audio).squeeze().numpy()
    spec_db = 10 * np.log10(spec + 1e-10)
    
    freq_bins = np.fft.rfftfreq(2048, 1/sr)
    
    results = {}
    ranges = [
        ("low_0_500", 0, 500),
        ("mid_500_2000", 500, 2000),
        ("high_2000_5000", 2000, 5000),
        ("vhigh_5000_10000", 5000, 10000),
    ]
    
    for name, low, high in ranges:
        mask = (freq_bins >= low) & (freq_bins < high)
        results[name] = spec_db[mask].mean()
    
    return results


def compare_audio(original, recon1, recon2, sr=44100):
    """–ü–æ—Ä—ñ–≤–Ω—é—î –¥–≤–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–π–æ–≤–∞–Ω–∏—Ö –∞—É–¥—ñ–æ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª–æ–º."""
    min_len = min(original.shape[-1], recon1.shape[-1], recon2.shape[-1])
    original = original[..., :min_len]
    recon1 = recon1[..., :min_len]
    recon2 = recon2[..., :min_len]
    
    # L1 loss
    l1_1 = torch.nn.functional.l1_loss(recon1, original).item()
    l1_2 = torch.nn.functional.l1_loss(recon2, original).item()
    
    # Frequency analysis
    orig_freq = analyze_frequency(original.cpu())
    recon1_freq = analyze_frequency(recon1.cpu())
    recon2_freq = analyze_frequency(recon2.cpu())
    
    freq_diff1 = {k: recon1_freq[k] - orig_freq[k] for k in orig_freq}
    freq_diff2 = {k: recon2_freq[k] - orig_freq[k] for k in orig_freq}
    
    return {
        "l1_1": l1_1,
        "l1_2": l1_2,
        "freq_diff1": freq_diff1,
        "freq_diff2": freq_diff2,
    }


def main():
    # Checkpoint paths
    ckpt1_path = "checkpoints/autoencoder/checkpoint_80000.pt"
    ckpt2_path = "checkpoints/autoencoder/checkpoint_90000.pt"
    
    print("="*70)
    print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø CHECKPOINT'–Ü–í")
    print("="*70)
    
    # Load models
    print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è checkpoint 1: {ckpt1_path}")
    model1, iter1 = load_model(ckpt1_path)
    print(f"  ‚Üí Iteration: {iter1}")
    
    print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è checkpoint 2: {ckpt2_path}")
    model2, iter2 = load_model(ckpt2_path)
    print(f"  ‚Üí Iteration: {iter2}")
    
    # Test files
    manifest = Path("data/manifests/val.json")
    with open(manifest) as f:
        samples = json.load(f)[:5]
    
    output_dir = Path("test_outputs/compare_checkpoints")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    print("="*70)
    
    all_results = []
    
    for i, sample in enumerate(samples):
        audio_path = Path(sample["audio_path"])
        if not audio_path.exists():
            continue
        
        print(f"\n[{i+1}] {audio_path.name}")
        
        # Load audio
        audio, sr = torchaudio.load(str(audio_path))
        if audio.dim() == 2:
            audio = audio.mean(dim=0)
        if sr != 44100:
            audio = torchaudio.functional.resample(audio, sr, 44100)
        audio = audio[:44100*10].unsqueeze(0).to(device)
        
        # Reconstruct with both models
        with torch.no_grad():
            latent1 = model1.encode(audio)
            recon1 = model1.decode(latent1)
            
            latent2 = model2.encode(audio)
            recon2 = model2.decode(latent2)
        
        # Compare
        results = compare_audio(audio, recon1, recon2)
        all_results.append(results)
        
        print(f"    Audio L1 loss:")
        print(f"      Checkpoint {iter1}: {results['l1_1']:.4f}")
        print(f"      Checkpoint {iter2}: {results['l1_2']:.4f}")
        improvement = (results['l1_1'] - results['l1_2']) / results['l1_1'] * 100
        print(f"      {'üìà –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è' if improvement > 0 else 'üìâ –ü–æ–≥—ñ—Ä—à–µ–Ω–Ω—è'}: {abs(improvement):.1f}%")
        
        print(f"    –ß–∞—Å—Ç–æ—Ç–Ω–∏–π –±–∞–ª–∞–Ω—Å (—Ä—ñ–∑–Ω–∏—Ü—è –≤—ñ–¥ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É, dB):")
        print(f"      {'–î—ñ–∞–ø–∞–∑–æ–Ω':20} | {f'Ckpt {iter1}':>10} | {f'Ckpt {iter2}':>10} | –ö—Ä–∞—â–µ?")
        print(f"      {'-'*20}-+-{'-'*10}-+-{'-'*10}-+-------")
        
        for key in results['freq_diff1']:
            d1 = results['freq_diff1'][key]
            d2 = results['freq_diff2'][key]
            better = "‚úÖ" if abs(d2) < abs(d1) else "‚ùå" if abs(d2) > abs(d1) else "="
            name = key.replace("_", " ").replace("low", "–ù–∏–∑—å–∫—ñ").replace("mid", "–°–µ—Ä–µ–¥–Ω—ñ").replace("high", "–í–∏—Å–æ–∫—ñ").replace("vhigh", "–î—É–∂–µ –≤–∏—Å.")
            print(f"      {name:20} | {d1:>+10.2f} | {d2:>+10.2f} | {better}")
        
        # Save audio files
        min_len = min(audio.shape[-1], recon1.shape[-1], recon2.shape[-1])
        torchaudio.save(str(output_dir / f"{i+1}_original.wav"), audio[..., :min_len].cpu(), 44100)
        torchaudio.save(str(output_dir / f"{i+1}_ckpt{iter1}.wav"), recon1[..., :min_len].cpu(), 44100)
        torchaudio.save(str(output_dir / f"{i+1}_ckpt{iter2}.wav"), recon2[..., :min_len].cpu(), 44100)
    
    # Summary
    print("\n" + "="*70)
    print("–ó–ê–ì–ê–õ–¨–ù–ò–ô –ü–Ü–î–°–£–ú–û–ö")
    print("="*70)
    
    avg_l1_1 = np.mean([r['l1_1'] for r in all_results])
    avg_l1_2 = np.mean([r['l1_2'] for r in all_results])
    
    print(f"\n–°–µ—Ä–µ–¥–Ω—ñ–π Audio L1 loss:")
    print(f"  Checkpoint {iter1}: {avg_l1_1:.4f}")
    print(f"  Checkpoint {iter2}: {avg_l1_2:.4f}")
    
    improvement = (avg_l1_1 - avg_l1_2) / avg_l1_1 * 100
    if improvement > 0:
        print(f"  üìà –ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvement:.1f}%")
    else:
        print(f"  üìâ –ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è: {abs(improvement):.1f}%")
    
    # Average frequency differences
    print(f"\n–°–µ—Ä–µ–¥–Ω—è —á–∞—Å—Ç–æ—Ç–Ω–∞ —Ä—ñ–∑–Ω–∏—Ü—è –≤—ñ–¥ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É:")
    for key in all_results[0]['freq_diff1']:
        avg_d1 = np.mean([r['freq_diff1'][key] for r in all_results])
        avg_d2 = np.mean([r['freq_diff2'][key] for r in all_results])
        better = "‚úÖ" if abs(avg_d2) < abs(avg_d1) else "‚ùå"
        name = key.replace("_", " ")
        print(f"  {name:20}: {avg_d1:+.2f} ‚Üí {avg_d2:+.2f} dB  {better}")
    
    print(f"\n‚úì –ê—É–¥—ñ–æ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {output_dir}/")
    print("\n–ü–û–°–õ–£–•–ê–ô —Ñ–∞–π–ª–∏ —â–æ–± –æ—Ü—ñ–Ω–∏—Ç–∏ —Ä—ñ–∑–Ω–∏—Ü—é!")


if __name__ == "__main__":
    main()
