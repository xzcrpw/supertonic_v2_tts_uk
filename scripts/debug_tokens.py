# scripts/debug_tokens.py
from supertonic.data.tokenizer import CharacterTokenizer

# –Ü–º—ñ—Ç—É—î–º–æ —Ç–≤—ñ–π –∫–æ–Ω—Ñ—ñ–≥
tokenizer = CharacterTokenizer(languages=["uk", "en"])

uk_text = "–ü—Ä–∏–≤—ñ—Ç, —è–∫ —Å–ø—Ä–∞–≤–∏?"
en_text = "Hello, how are you?"

uk_ids = tokenizer.encode(uk_text)
en_ids = tokenizer.encode(en_text)

print(f"Vocab size: {tokenizer.vocab_size}")
print(f"\nüá∫üá¶ UK: '{uk_text}'")
print(f"IDs: {uk_ids}")

print(f"\nüá¨üáß EN: '{en_text}'")
print(f"IDs: {en_ids}")

# –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê UNKNOWN (0 –∞–±–æ 1 –∑–∞–∑–≤–∏—á–∞–π)
if all(x == 0 for x in en_ids) or len(set(en_ids)) <= 2:
    print("\n‚ùå –ü–ò–ó–î–ê! –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ –º–æ–≤–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –Ω–∞ —Å–º—ñ—Ç—Ç—è/–Ω—É–ª—ñ!")
else:
    print("\n‚úÖ –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –ø—Ä–∞—Ü—é—î –∫–æ—Ä–µ–∫—Ç–Ω–æ.")