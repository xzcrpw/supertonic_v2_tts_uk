# –°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–£ –¢–ê –ö–û–î
# –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ: 2026-01-28 15:20:09
# –ü—Ä–æ–µ–∫—Ç: supertonic_v2_tts_uk
================================================================================


==================================================
–§–ê–ô–õ: config/22khz_optimal.yaml
–†–û–ó–ú–Ü–†: 8.62 KB
==================================================

# ============================================================================
# SUPERTONIC V2 TTS - 22kHz OPTIMAL CONFIGURATION
# ============================================================================
# Based on SupertonicTTS paper (arXiv:2503.23108v3)
#
# Differences from paper (44.1kHz):
#   - 22050Hz sample rate - reduces compute, sufficient for TTS
#   - FFT sizes scaled by 0.5: [512, 1024, 2048] instead of [1024, 2048, 4096]
#
# Paper training config (Section 4.2):
#   - Batch size: 128
#   - Learning rate: 2√ó10‚Åª‚Å¥
#   - Iterations: 1.5M (we use 200k for budget)
#   - Disc crop: 0.19s
#   - Loss weights: Œª_recon=45, Œª_adv=1, Œª_fm=0.1
#   - Mel bands: [64, 128, 128] per FFT resolution
# ============================================================================

# ========== AUDIO CONFIGURATION ==========
audio:
  sample_rate: 22050          # TTS standard - 11kHz Nyquist is enough for speech
  n_fft: 1024                 # Adjusted for 22kHz (46.4ms window)
  hop_length: 256             # 11.6ms hop - same timing as 44.1kHz/512
  win_length: 1024
  n_mels: 100                 # Standard for 22kHz TTS (paper uses 228 @ 44.1kHz)
  mel_fmin: 20.0
  mel_fmax: 11025.0           # Nyquist limit for 22kHz

# ========== MODEL ARCHITECTURE ==========
model:
  autoencoder:
    # Encoder: mel(100) ‚Üí hidden(512) ‚Üí latent(24)
    encoder:
      input_dim: 100          # n_mels (scaled for 22kHz)
      hidden_dim: 512
      output_dim: 24          # latent dimension (paper spec)
      num_blocks: 10          # 10 ConvNeXt blocks (paper spec)
      kernel_size: 7
      intermediate_mult: 4    # 512 * 4 = 2048 intermediate
      gradient_checkpointing: false  # 96GB - not needed
    
    # Decoder: latent(24) ‚Üí hidden(512) ‚Üí HiFi-GAN ‚Üí waveform
    # NOTE: Using HiFi-GAN instead of WaveNeXt to eliminate metallic artifacts!
    decoder:
      input_dim: 24
      hidden_dim: 512
      num_blocks: 10
      kernel_size: 7
      intermediate_mult: 4
      # Dilations from paper: receptive field ~1 second
      dilations: [1, 2, 4, 1, 2, 4, 1, 1, 1, 1]
      n_fft: 1024
      hop_length: 256
      causal: true            # For streaming inference
      gradient_checkpointing: false
      
      # ========== HiFi-GAN GENERATOR CONFIG ==========
      # CRITICAL: product of upsample_rates MUST equal hop_length!
      # 8 * 8 * 2 * 2 = 256 ‚úì
      use_hifigan: true       # Set false to use old WaveNeXtHead (causes metallic sound!)
      upsample_rates: [8, 8, 2, 2]
      upsample_kernel_sizes: [16, 16, 4, 4]  # kernel >= 2*stride to avoid checkerboard
      upsample_initial_channel: 512
      resblock_kernel_sizes: [3, 7, 11]      # Multi-receptive field fusion
      resblock_dilation_sizes: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]

    # Discriminators (HiFi-GAN style)
    discriminator:
      mpd_periods: [2, 3, 5, 7, 11]
      mrd_fft_sizes: [512, 1024, 2048]  # Paper spec - NOT scaled by sample rate!
  
  # Text-to-Latent (Stage 2)
  text_to_latent:
    reference_encoder:
      input_dim: 144          # Compressed latent (24 * 6)
      hidden_dim: 128
      num_convnext_blocks: 6
      num_cross_attn_layers: 2
      num_output_vectors: 50  # Fixed reference vectors
      kernel_size: 5
      intermediate_mult: 4

    text_encoder:
      vocab_size: 512         # Extended multilingual
      embed_dim: 128
      hidden_dim: 512
      num_convnext_blocks: 6
      num_self_attn_blocks: 4
      num_cross_attn_layers: 2
      num_heads: 4
      kernel_size: 5

    vector_field:
      hidden_dim: 512
      num_blocks: 8
      kernel_size: 7
      dilations: [1, 2, 4, 8, 1, 2, 4, 8]
      num_heads: 4

  # Duration Predictor (Stage 3)
  duration_predictor:
    text_dim: 512
    hidden_dim: 256
    num_layers: 4
    kernel_size: 3
    dropout: 0.1

# ========== LAROPE CONFIGURATION ==========
larope:
  gamma: 10                   # Paper optimal for TTS alignment
  base: 10000

# ========== FLOW MATCHING ==========
flow_matching:
  sigma_min: 1.0e-8
  p_uncond: 0.05              # CFG dropout probability
  cfg_scale: 3.0              # Classifier-free guidance scale
  nfe: 32                     # Number of function evaluations

# ========== LANGUAGES ==========
languages:
  supported: ["uk", "en"]     # Ukrainian + English for zero-shot from LibriTTS
  embedding_dim: 4

# ========== TRAINING: AUTOENCODER (Stage 1) ==========
train_autoencoder:
  # Batch size - PAPER: 128 total
  # Fig.2 shows larger batch DEGRADES alignment in early training!
  batch_size: 32              # Per GPU (32√ó4=128 effective batch) - EXACT paper value
  segment_length: 88200       # 4 seconds at 22050Hz
  gradient_accumulation_steps: 1
  
  # Optimizer - Paper uses lr=2e-4
  learning_rate: 2.0e-4       # Paper: "learning rate of 2√ó10‚Åª‚Å¥"
  optimizer:
    betas: [0.8, 0.99]        # AdamW betas from HiFi-GAN
    weight_decay: 0.01
  
  # Schedule - OPTIMIZED FOR BUDGET
  # Paper: 1.5M @ batch128. We have batch192 (1.5√ó more efficient)
  # 200k steps ‚âà 300k paper steps - enough for good autoencoder
  total_iterations: 200000    # Budget-friendly, check quality at 50k/100k
  warmup_steps: 5000          # LR warmup for fresh training
  checkpoint_interval: 1000   # Every 1k - don't lose progress!
  validation_interval: 10000  # Validate every 10k
  log_interval: 50            # Log every 50 steps for better monitoring
  
  # Discriminator warmup - generator trains alone first
  discriminator_start_steps: 5000  # Gen needs time to learn HiFi-GAN head
  
  # Loss weights - EXACT PAPER VALUES (Appendix B.1)
  loss_weights:
    reconstruction: 45.0       # Paper: Œª_recon = 45
    adversarial: 1.0           # Paper: Œª_adv = 1
    feature_matching: 0.1      # Paper: Œª_fm = 0.1
  
  # Multi-resolution Mel Loss configuration
  # Paper @ 44.1kHz: FFT [1024, 2048, 4096], Mels [64, 128, 128]
  # Scaled for 22kHz: FFT sizes halved
  loss_fft_sizes: [512, 1024, 2048]
  loss_n_mels: [64, 128, 128]  # Paper: different per resolution!
  
  # Discriminator crop for adversarial training
  # Paper: "we randomly cropped segments of real and generated speech to 0.19s"
  disc_crop_length: 4189       # 0.19s at 22050Hz (paper spec)
  
  # AMP
  amp:
    enabled: true
    dtype: bfloat16           # Best for Ampere+ GPUs
  
  # DataLoader - Balance speed vs memory
  num_workers: 12               # Per GPU (4√ó4=16 workers) - minimal for stability
  pin_memory: true
  cache_audio: false           # DISABLED - workers duplicate cache = OOM
  prefetch_factor: 6           # Low prefetch to reduce memory

# ========== TRAINING: TEXT-TO-LATENT (Stage 2) ==========
train_tts:
  batch_size: 32              # Per GPU (32√ó4 expansion = 128 effective per GPU)
  expansion_factor: 4         # Ke for context sharing
  effective_batch: 512        # batch √ó GPUs √ó expansion
  
  learning_rate: 5.0e-4
  optimizer:
    type: AdamW
    betas: [0.9, 0.999]
    weight_decay: 0.01
  
  total_iterations: 500000
  lr_halve_interval: 200000
  checkpoint_interval: 2000
  validation_interval: 5000
  
  amp:
    enabled: true
    dtype: bfloat16

# ========== TRAINING: DURATION (Stage 3) ==========
train_duration:
  batch_size: 256
  learning_rate: 5.0e-4
  total_iterations: 5000
  checkpoint_interval: 1000
  
  optimizer:
    type: AdamW
    betas: [0.9, 0.999]
    weight_decay: 0.01

# ========== INFERENCE ==========
inference:
  num_steps: 32               # ODE steps
  solver: euler
  cfg_scale: 3.0

# ========== HARDWARE ==========
hardware:
  device: cuda
  num_gpus: 1
  
optimization:
  use_amp: true
  amp_dtype: bfloat16
  gradient_checkpointing: false  # 96GB - not needed
  compile_model: false           # torch.compile experimental

# ========== PATHS ==========
paths:
  data_dir: data
  manifest_train: data/manifests/train.json
  manifest_val: data/manifests/val.json
  checkpoint_dir: checkpoints
  log_dir: logs
  sample_dir: samples

# Alias for compatibility with train scripts
output:
  checkpoint_dir: checkpoints
  sample_dir: samples
  log_dir: logs

# Compatibility alias for training config  
training:
  segment_length: 88200       # 4 seconds at 22050Hz
  autoencoder:
    num_workers: 8

# ========== DATA CONSTRAINTS ==========
data:
  train_manifest: data/manifests/train.json
  val_manifest: data/manifests/val.json
  min_audio_duration: 0.5     # Skip very short clips
  max_audio_duration: 15.0    # Skip very long clips
  max_text_length: 500

# ========== LOGGING ==========
logging:
  project: supertonic-v2-uk-22k
  log_interval: 100
  use_wandb: true



==================================================
–§–ê–ô–õ: config/default.yaml
–†–û–ó–ú–Ü–†: 4.88 KB
==================================================

# Supertonic v2 TTS - –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
# –†–µ—ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏

# ====== AUDIO CONFIGURATION ======
audio:
  sample_rate: 44100
  n_fft: 2048
  hop_length: 512
  win_length: 2048
  n_mels: 228
  fmin: 0
  fmax: 22050
  window: "hann"
  center: true
  pad_mode: "reflect"
  norm: "slaney"
  mel_scale: "slaney"

# ====== LATENT SPACE ======
latent:
  dim: 24                    # –õ–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä
  temporal_compression: 6    # Kc - —Å—Ç–µ–∫ —Ñ—Ä–µ–π–º—ñ–≤
  compressed_dim: 144        # dim √ó compression = 24 √ó 6

# ====== SPEECH AUTOENCODER ======
autoencoder:
  encoder:
    input_dim: 228           # Mel channels
    hidden_dim: 512
    output_dim: 24           # Latent dim
    num_blocks: 10
    kernel_size: 7
    intermediate_mult: 4     # 512 √ó 4 = 2048

  decoder:
    input_dim: 24
    hidden_dim: 512
    num_blocks: 10
    kernel_size: 7
    intermediate_mult: 4
    dilations: [1, 2, 4, 1, 2, 4, 1, 1, 1, 1]
    causal: true             # –î–ª—è streaming –ø—ñ–¥—Ç—Ä–∏–º–∫–∏

  discriminator:
    mpd_periods: [2, 3, 5, 7, 11]
    mrd_fft_sizes: [512, 1024, 2048]

# ====== TEXT-TO-LATENT MODULE ======
text_to_latent:
  reference_encoder:
    input_dim: 144           # Compressed latent
    hidden_dim: 128
    num_convnext_blocks: 6
    num_cross_attn_layers: 2
    num_output_vectors: 50   # Fixed reference size
    kernel_size: 5
    intermediate_mult: 4

  text_encoder:
    vocab_size: 512          # Extended for multilingual
    embed_dim: 128
    hidden_dim: 512
    num_convnext_blocks: 6
    num_self_attn_blocks: 4
    num_cross_attn_layers: 2
    num_heads: 4
    kernel_size: 5

  vector_field:
    hidden_dim: 512
    num_blocks: 8
    kernel_size: 7
    dilations: [1, 2, 4, 8, 1, 2, 4, 8]
    num_heads: 4

# ====== DURATION PREDICTOR ======
duration_predictor:
  hidden_dim: 256
  num_convnext_blocks: 4
  kernel_size: 7
  intermediate_mult: 4

# ====== LAROPE CONFIGURATION ======
larope:
  gamma: 10                  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è diagonal alignment
  base: 10000

# ====== FLOW MATCHING ======
flow_matching:
  sigma_min: 1.0e-8
  p_uncond: 0.05             # CFG unconditional probability
  cfg_scale: 3.0             # Classifier-free guidance scale
  nfe: 32                    # Number of function evaluations (inference)

# ====== TRAINING: AUTOENCODER ======
train_autoencoder:
  batch_size: 16
  learning_rate: 2.0e-4
  total_iterations: 1500000  # 1.5M
  checkpoint_interval: 50000
  validation_interval: 10000

  loss_weights:
    reconstruction: 45.0     # Œª_recon (mel loss)
    waveform: 10.0           # Œª_wave (NEW - helps preserve low frequencies)
    adversarial: 1.0         # Œª_adv
    feature_matching: 0.1    # Œª_fm

  optimizer:
    type: "AdamW"
    betas: [0.8, 0.99]
    weight_decay: 0.01

  amp:
    enabled: true
    dtype: "bfloat16"

# ====== TRAINING: TEXT-TO-LATENT ======
train_tts:
  batch_size: 64
  expansion_factor: 4        # Ke - context-sharing
  effective_batch: 256       # batch_size √ó expansion_factor
  learning_rate: 5.0e-4
  total_iterations: 700000
  lr_halve_interval: 300000
  checkpoint_interval: 50000
  validation_interval: 10000

  optimizer:
    type: "AdamW"
    betas: [0.9, 0.999]
    weight_decay: 0.01

  amp:
    enabled: true
    dtype: "bfloat16"

# ====== TRAINING: DURATION PREDICTOR ======
train_duration:
  batch_size: 64
  learning_rate: 5.0e-4
  total_iterations: 3000
  checkpoint_interval: 1000

  optimizer:
    type: "AdamW"
    betas: [0.9, 0.999]
    weight_decay: 0.01

# ====== LANGUAGES ======
languages:
  supported: ["en", "ko", "es", "pt", "fr", "uk"]
  embedding_dim: 4           # Optional language embeddings

  # –ú–æ–≤–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
  datasets:
    en:
      - name: "LJSpeech"
        hours: 24
        speakers: 1
      - name: "VCTK"
        hours: 44
        speakers: 109
      - name: "Hi-Fi TTS"
        hours: 300
        speakers: 10
      - name: "LibriTTS"
        hours: 585
        speakers: 2456
    uk:
      - name: "M-AILABS Ukrainian"
        hours: 20
        speakers: 2
      - name: "Common Voice Ukrainian"
        hours: 80
        speakers: 1000

# ====== DATA PATHS ======
data:
  train_manifest: "data/train_manifest.json"
  val_manifest: "data/val_manifest.json"
  test_manifest: "data/test_manifest.json"
  cache_dir: "cache/"

  # Audio settings
  max_audio_duration: 30.0   # seconds
  min_audio_duration: 0.5
  max_text_length: 500

# ====== CHECKPOINTS & LOGGING ======
output:
  checkpoint_dir: "checkpoints/"
  log_dir: "logs/"
  sample_dir: "samples/"

logging:
  backend: "wandb"           # "wandb" or "tensorboard"
  project: "supertonic-v2-uk"
  log_interval: 100
  audio_log_interval: 5000

# ====== DISTRIBUTED TRAINING ======
distributed:
  enabled: true
  backend: "nccl"
  find_unused_parameters: false

# ====== INFERENCE ======
inference:
  nfe: 32
  cfg_scale: 3.0
  max_duration: 30.0



==================================================
–§–ê–ô–õ: export_onnx.py
–†–û–ó–ú–Ü–†: 15.72 KB
==================================================

"""
ONNX Export –¥–ª—è Supertonic v2 TTS

–ï–∫—Å–ø–æ—Ä—Ç—É—î –≤—Å—ñ –º–æ–¥—É–ª—ñ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç –¥–ª—è production inference:
1. Text Encoder
2. Reference Encoder
3. Vector Field Estimator
4. Latent Decoder (Vocoder)
5. Duration Predictor

ONNX –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:
- Opset version: 17
- Dynamic axes –¥–ª—è batch/sequence
- FP16 optimization –æ–ø—Ü—ñ–π–Ω–æ

–†–µ—Ñ–µ—Ä–µ–Ω—Å: –û—Ñ—ñ—Ü—ñ–π–Ω—ñ ONNX –º–æ–¥–µ–ª—ñ (~260MB)
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional, Dict, Tuple, List

import torch
import torch.nn as nn
import onnx
import onnxruntime as ort
from omegaconf import OmegaConf

sys.path.insert(0, str(Path(__file__).parent))

from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder
from supertonic.models.text_to_latent import TextToLatent, ReferenceEncoder, TextEncoder
from supertonic.models.duration_predictor import DurationPredictor
from supertonic.data.tokenizer import CharacterTokenizer


class TextEncoderWrapper(nn.Module):
    """Wrapper –¥–ª—è Text Encoder –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º–∏ inputs."""
    
    def __init__(self, text_to_latent: TextToLatent):
        super().__init__()
        self.text_encoder = text_to_latent.text_encoder
    
    def forward(
        self,
        text_ids: torch.Tensor,
        text_mask: torch.Tensor,
        lang_id: torch.Tensor
    ) -> torch.Tensor:
        return self.text_encoder(text_ids, text_mask, lang_id=lang_id)


class ReferenceEncoderWrapper(nn.Module):
    """Wrapper –¥–ª—è Reference Encoder."""
    
    def __init__(self, text_to_latent: TextToLatent):
        super().__init__()
        self.reference_encoder = text_to_latent.reference_encoder
    
    def forward(
        self,
        reference_latent: torch.Tensor,
        reference_mask: torch.Tensor
    ) -> torch.Tensor:
        return self.reference_encoder(reference_latent, reference_mask)


class VectorFieldWrapper(nn.Module):
    """Wrapper –¥–ª—è Vector Field Estimator."""
    
    def __init__(self, text_to_latent: TextToLatent):
        super().__init__()
        self.vector_field = text_to_latent.vector_field
    
    def forward(
        self,
        z_t: torch.Tensor,
        z_ref: torch.Tensor,
        text_encoding: torch.Tensor,
        reference_encoding: torch.Tensor,
        timestep: torch.Tensor,
        text_mask: torch.Tensor
    ) -> torch.Tensor:
        return self.vector_field(
            z_t=z_t,
            z_ref=z_ref,
            text_encoding=text_encoding,
            reference_encoding=reference_encoding,
            timestep=timestep,
            text_mask=text_mask
        )


class DurationPredictorWrapper(nn.Module):
    """Wrapper –¥–ª—è Duration Predictor."""
    
    def __init__(self, duration_predictor: DurationPredictor):
        super().__init__()
        self.predictor = duration_predictor
    
    def forward(
        self,
        text_ids: torch.Tensor,
        text_mask: torch.Tensor,
        reference_latent: torch.Tensor,
        reference_mask: torch.Tensor
    ) -> torch.Tensor:
        return self.predictor(
            text_ids=text_ids,
            text_mask=text_mask,
            reference_latent=reference_latent,
            reference_mask=reference_mask
        )


def export_text_encoder(
    text_to_latent: TextToLatent,
    output_path: Path,
    opset_version: int = 17
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î Text Encoder –≤ ONNX."""
    print("Exporting Text Encoder...")
    
    wrapper = TextEncoderWrapper(text_to_latent).eval()
    
    # Dummy inputs
    batch_size = 1
    text_len = 100
    
    text_ids = torch.randint(0, 100, (batch_size, text_len))
    text_mask = torch.ones(batch_size, text_len, dtype=torch.bool)
    lang_id = torch.tensor([0])
    
    # Export
    torch.onnx.export(
        wrapper,
        (text_ids, text_mask, lang_id),
        str(output_path),
        input_names=["text_ids", "text_mask", "lang_id"],
        output_names=["text_encoding"],
        dynamic_axes={
            "text_ids": {0: "batch", 1: "text_len"},
            "text_mask": {0: "batch", 1: "text_len"},
            "lang_id": {0: "batch"},
            "text_encoding": {0: "batch", 1: "text_len"}
        },
        opset_version=opset_version,
        do_constant_folding=True
    )
    
    print(f"  Saved to {output_path}")
    return output_path


def export_reference_encoder(
    text_to_latent: TextToLatent,
    output_path: Path,
    opset_version: int = 17
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î Reference Encoder –≤ ONNX."""
    print("Exporting Reference Encoder...")
    
    wrapper = ReferenceEncoderWrapper(text_to_latent).eval()
    
    # Dummy inputs
    batch_size = 1
    ref_len = 50
    latent_dim = 144
    
    reference_latent = torch.randn(batch_size, latent_dim, ref_len)
    reference_mask = torch.ones(batch_size, ref_len, dtype=torch.bool)
    
    # Export
    torch.onnx.export(
        wrapper,
        (reference_latent, reference_mask),
        str(output_path),
        input_names=["reference_latent", "reference_mask"],
        output_names=["reference_encoding"],
        dynamic_axes={
            "reference_latent": {0: "batch", 2: "ref_len"},
            "reference_mask": {0: "batch", 1: "ref_len"},
            "reference_encoding": {0: "batch"}
        },
        opset_version=opset_version,
        do_constant_folding=True
    )
    
    print(f"  Saved to {output_path}")
    return output_path


def export_vector_field(
    text_to_latent: TextToLatent,
    output_path: Path,
    opset_version: int = 17
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î Vector Field Estimator –≤ ONNX."""
    print("Exporting Vector Field Estimator...")
    
    wrapper = VectorFieldWrapper(text_to_latent).eval()
    
    # Dummy inputs
    batch_size = 1
    latent_dim = 144
    latent_len = 50
    text_len = 100
    hidden_dim = 128
    num_ref_vectors = 50
    
    z_t = torch.randn(batch_size, latent_dim, latent_len)
    z_ref = torch.randn(batch_size, latent_dim, latent_len)
    text_encoding = torch.randn(batch_size, text_len, hidden_dim)
    reference_encoding = torch.randn(batch_size, num_ref_vectors, hidden_dim)
    timestep = torch.rand(batch_size)
    text_mask = torch.ones(batch_size, text_len, dtype=torch.bool)
    
    # Export
    torch.onnx.export(
        wrapper,
        (z_t, z_ref, text_encoding, reference_encoding, timestep, text_mask),
        str(output_path),
        input_names=["z_t", "z_ref", "text_encoding", "reference_encoding", "timestep", "text_mask"],
        output_names=["velocity"],
        dynamic_axes={
            "z_t": {0: "batch", 2: "latent_len"},
            "z_ref": {0: "batch", 2: "latent_len"},
            "text_encoding": {0: "batch", 1: "text_len"},
            "reference_encoding": {0: "batch"},
            "timestep": {0: "batch"},
            "text_mask": {0: "batch", 1: "text_len"},
            "velocity": {0: "batch", 2: "latent_len"}
        },
        opset_version=opset_version,
        do_constant_folding=True
    )
    
    print(f"  Saved to {output_path}")
    return output_path


def export_latent_decoder(
    latent_decoder: LatentDecoder,
    output_path: Path,
    opset_version: int = 17
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î Latent Decoder (Vocoder) –≤ ONNX."""
    print("Exporting Latent Decoder...")
    
    latent_decoder.eval()
    
    # Dummy input
    batch_size = 1
    latent_dim = 24
    latent_len = 300
    
    latent = torch.randn(batch_size, latent_dim, latent_len)
    
    # Export
    torch.onnx.export(
        latent_decoder,
        (latent,),
        str(output_path),
        input_names=["latent"],
        output_names=["audio"],
        dynamic_axes={
            "latent": {0: "batch", 2: "latent_len"},
            "audio": {0: "batch", 1: "audio_len"}
        },
        opset_version=opset_version,
        do_constant_folding=True
    )
    
    print(f"  Saved to {output_path}")
    return output_path


def export_latent_encoder(
    latent_encoder: LatentEncoder,
    output_path: Path,
    opset_version: int = 17
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î Latent Encoder –≤ ONNX."""
    print("Exporting Latent Encoder...")
    
    latent_encoder.eval()
    
    # Dummy input
    batch_size = 1
    n_mels = 228
    mel_len = 200
    
    mel = torch.randn(batch_size, n_mels, mel_len)
    
    # Export
    torch.onnx.export(
        latent_encoder,
        (mel,),
        str(output_path),
        input_names=["mel"],
        output_names=["latent"],
        dynamic_axes={
            "mel": {0: "batch", 2: "mel_len"},
            "latent": {0: "batch", 2: "latent_len"}
        },
        opset_version=opset_version,
        do_constant_folding=True
    )
    
    print(f"  Saved to {output_path}")
    return output_path


def export_duration_predictor(
    duration_predictor: DurationPredictor,
    output_path: Path,
    opset_version: int = 17
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î Duration Predictor –≤ ONNX."""
    print("Exporting Duration Predictor...")
    
    wrapper = DurationPredictorWrapper(duration_predictor).eval()
    
    # Dummy inputs
    batch_size = 1
    text_len = 100
    ref_len = 50
    latent_dim = 144
    
    text_ids = torch.randint(0, 100, (batch_size, text_len))
    text_mask = torch.ones(batch_size, text_len, dtype=torch.bool)
    reference_latent = torch.randn(batch_size, latent_dim, ref_len)
    reference_mask = torch.ones(batch_size, ref_len, dtype=torch.bool)
    
    # Export
    torch.onnx.export(
        wrapper,
        (text_ids, text_mask, reference_latent, reference_mask),
        str(output_path),
        input_names=["text_ids", "text_mask", "reference_latent", "reference_mask"],
        output_names=["duration"],
        dynamic_axes={
            "text_ids": {0: "batch", 1: "text_len"},
            "text_mask": {0: "batch", 1: "text_len"},
            "reference_latent": {0: "batch", 2: "ref_len"},
            "reference_mask": {0: "batch", 1: "ref_len"},
            "duration": {0: "batch"}
        },
        opset_version=opset_version,
        do_constant_folding=True
    )
    
    print(f"  Saved to {output_path}")
    return output_path


def verify_onnx_model(onnx_path: Path, test_inputs: Dict[str, torch.Tensor]) -> bool:
    """–í–µ—Ä–∏—Ñ—ñ–∫—É—î ONNX –º–æ–¥–µ–ª—å."""
    print(f"Verifying {onnx_path.name}...")
    
    try:
        # Load and check model
        model = onnx.load(str(onnx_path))
        onnx.checker.check_model(model)
        
        # Run inference
        session = ort.InferenceSession(str(onnx_path))
        
        # Convert inputs to numpy
        inputs = {k: v.numpy() for k, v in test_inputs.items()}
        
        # Run
        outputs = session.run(None, inputs)
        
        print(f"  ‚úì Verification passed")
        print(f"  Output shapes: {[o.shape for o in outputs]}")
        return True
        
    except Exception as e:
        print(f"  ‚úó Verification failed: {e}")
        return False


def export_all(
    checkpoint_dir: Path,
    output_dir: Path,
    config_path: Path,
    opset_version: int = 17,
    verify: bool = True
):
    """–ï–∫—Å–ø–æ—Ä—Ç—É—î –≤—Å—ñ –º–æ–¥—É–ª—ñ."""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load config
    config = OmegaConf.load(config_path)
    
    # Create tokenizer for vocab size
    tokenizer = CharacterTokenizer(languages=config.languages.supported)
    
    device = torch.device("cpu")  # Export on CPU
    
    # Load models
    print("Loading models...")
    
    latent_encoder = LatentEncoder(
        input_dim=config.autoencoder.encoder.input_dim,
        hidden_dim=config.autoencoder.encoder.hidden_dim,
        output_dim=config.autoencoder.encoder.output_dim,
        num_blocks=config.autoencoder.encoder.num_blocks
    )
    
    latent_decoder = LatentDecoder(
        input_dim=config.autoencoder.decoder.input_dim,
        hidden_dim=config.autoencoder.decoder.hidden_dim,
        num_blocks=config.autoencoder.decoder.num_blocks,
        dilations=config.autoencoder.decoder.dilations,
        causal=config.autoencoder.decoder.causal
    )
    
    text_to_latent = TextToLatent(
        vocab_size=tokenizer.vocab_size,
        text_embed_dim=config.text_to_latent.text_encoder.embed_dim,
        text_hidden_dim=config.text_to_latent.text_encoder.hidden_dim,
        ref_input_dim=config.text_to_latent.reference_encoder.input_dim,
        ref_hidden_dim=config.text_to_latent.reference_encoder.hidden_dim,
        vf_hidden_dim=config.text_to_latent.vector_field.hidden_dim,
        gamma=config.larope.gamma
    )
    
    duration_predictor = DurationPredictor(
        vocab_size=tokenizer.vocab_size,
        hidden_dim=config.duration_predictor.hidden_dim,
        num_convnext_blocks=config.duration_predictor.num_convnext_blocks
    )
    
    # Load checkpoints
    ae_ckpt = checkpoint_dir / "autoencoder" / "checkpoint_final.pt"
    tts_ckpt = checkpoint_dir / "tts" / "checkpoint_final.pt"
    dur_ckpt = checkpoint_dir / "duration" / "checkpoint_final.pt"
    
    if ae_ckpt.exists():
        ckpt = torch.load(ae_ckpt, map_location=device)
        latent_encoder.load_state_dict(ckpt["encoder"])
        latent_decoder.load_state_dict(ckpt["decoder"])
    
    if tts_ckpt.exists():
        ckpt = torch.load(tts_ckpt, map_location=device)
        text_to_latent.load_state_dict(ckpt["model"])
    
    if dur_ckpt.exists():
        ckpt = torch.load(dur_ckpt, map_location=device)
        duration_predictor.load_state_dict(ckpt["model"])
    
    # Export all
    print("\n=== Exporting ONNX Models ===\n")
    
    exported = []
    
    # Latent Encoder
    path = export_latent_encoder(latent_encoder, output_dir / "latent_encoder.onnx", opset_version)
    exported.append(path)
    
    # Latent Decoder
    path = export_latent_decoder(latent_decoder, output_dir / "latent_decoder.onnx", opset_version)
    exported.append(path)
    
    # Text Encoder
    path = export_text_encoder(text_to_latent, output_dir / "text_encoder.onnx", opset_version)
    exported.append(path)
    
    # Reference Encoder
    path = export_reference_encoder(text_to_latent, output_dir / "reference_encoder.onnx", opset_version)
    exported.append(path)
    
    # Vector Field
    path = export_vector_field(text_to_latent, output_dir / "vector_field.onnx", opset_version)
    exported.append(path)
    
    # Duration Predictor
    path = export_duration_predictor(duration_predictor, output_dir / "duration_predictor.onnx", opset_version)
    exported.append(path)
    
    print(f"\n=== Export Complete ===")
    print(f"Exported {len(exported)} models to {output_dir}")
    
    # Calculate total size
    total_size = sum(p.stat().st_size for p in exported)
    print(f"Total size: {total_size / 1024 / 1024:.1f} MB")
    
    return exported


def main():
    """CLI –¥–ª—è ONNX export."""
    parser = argparse.ArgumentParser(description="Export Supertonic v2 to ONNX")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints")
    parser.add_argument("--output-dir", type=str, default="onnx_models")
    parser.add_argument("--config", type=str, default="config/default.yaml")
    parser.add_argument("--opset", type=int, default=17)
    parser.add_argument("--no-verify", action="store_true")
    
    args = parser.parse_args()
    
    export_all(
        checkpoint_dir=Path(args.checkpoint_dir),
        output_dir=Path(args.output_dir),
        config_path=Path(args.config),
        opset_version=args.opset,
        verify=not args.no_verify
    )


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: inference.py
–†–û–ó–ú–Ü–†: 17.28 KB
==================================================

"""
Inference Pipeline –¥–ª—è Supertonic v2 TTS

–ü–æ–≤–Ω–∏–π inference pipeline:
1. Text tokenization
2. Duration prediction
3. Latent generation (Flow-matching –∑ ODE solver)
4. Waveform decoding

–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º:
- NFE: 32 (optimal quality/speed)
- CFG scale: 3.0
- Sample rate: 44.1kHz

Usage:
    python inference.py --text "Hello world" --reference audio.wav --output output.wav
    
    # –∞–±–æ –∑ Python:
    tts = SupertonicTTS.from_pretrained("checkpoints/")
    audio = tts.synthesize("Hello world", reference_audio)
    tts.save_audio(audio, "output.wav")

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional, Union, Tuple
import time

import torch
import torch.nn as nn
import torchaudio
import numpy as np

from omegaconf import OmegaConf

sys.path.insert(0, str(Path(__file__).parent))

from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder
from supertonic.models.text_to_latent import TextToLatent
from supertonic.models.duration_predictor import DurationPredictor
from supertonic.losses.flow_matching_loss import ODESolver, compress_latents, decompress_latents
from supertonic.data.preprocessing import AudioProcessor, load_audio
from supertonic.data.tokenizer import CharacterTokenizer, detect_language_simple, LANGUAGE_CODES


class SupertonicTTS(nn.Module):
    """
    Supertonic v2 TTS - –ü–æ–≤–Ω–∏–π inference pipeline.
    
    –ì–µ–Ω–µ—Ä—É—î 44.1kHz high-fidelity speech –∑ —Ç–µ–∫—Å—Ç—É.
    
    Args:
        latent_encoder: Pretrained latent encoder
        latent_decoder: Pretrained latent decoder  
        text_to_latent: Text-to-latent flow-matching model
        duration_predictor: Duration predictor
        tokenizer: Character tokenizer
        audio_processor: Audio processor
        config: Model configuration
    """
    
    def __init__(
        self,
        latent_encoder: LatentEncoder,
        latent_decoder: LatentDecoder,
        text_to_latent: TextToLatent,
        duration_predictor: DurationPredictor,
        tokenizer: CharacterTokenizer,
        audio_processor: AudioProcessor,
        config: dict
    ):
        super().__init__()
        
        self.latent_encoder = latent_encoder
        self.latent_decoder = latent_decoder
        self.text_to_latent = text_to_latent
        self.duration_predictor = duration_predictor
        self.tokenizer = tokenizer
        self.audio_processor = audio_processor
        self.config = config
        
        # ODE solver –¥–ª—è flow-matching
        self.ode_solver = ODESolver(
            nfe=config.get("nfe", 32),
            cfg_scale=config.get("cfg_scale", 3.0)
        )
        
        # Default parameters
        self.sample_rate = config.get("sample_rate", 44100)
        self.compression_factor = config.get("compression_factor", 6)
    
    @classmethod
    def from_pretrained(
        cls,
        checkpoint_dir: Union[str, Path],
        config_path: Optional[Union[str, Path]] = None,
        device: Optional[torch.device] = None
    ) -> "SupertonicTTS":
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î pretrained –º–æ–¥–µ–ª—ñ.
        
        Args:
            checkpoint_dir: Directory –∑ checkpoints
            config_path: Path –¥–æ config file
            device: Target device
            
        Returns:
            SupertonicTTS instance
        """
        checkpoint_dir = Path(checkpoint_dir)
        device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Load config
        if config_path is None:
            config_path = checkpoint_dir / "config.yaml"
        
        config = OmegaConf.load(config_path)
        
        # Create tokenizer
        tokenizer = CharacterTokenizer(languages=config.languages.supported)
        
        # Create audio processor
        audio_processor = AudioProcessor(
            sample_rate=config.audio.sample_rate,
            n_fft=config.audio.n_fft,
            hop_length=config.audio.hop_length,
            n_mels=config.audio.n_mels
        )
        
        # Create models
        latent_encoder = LatentEncoder(
            input_dim=config.autoencoder.encoder.input_dim,
            hidden_dim=config.autoencoder.encoder.hidden_dim,
            output_dim=config.autoencoder.encoder.output_dim,
            num_blocks=config.autoencoder.encoder.num_blocks
        )
        
        latent_decoder = LatentDecoder(
            input_dim=config.autoencoder.decoder.input_dim,
            hidden_dim=config.autoencoder.decoder.hidden_dim,
            num_blocks=config.autoencoder.decoder.num_blocks,
            dilations=config.autoencoder.decoder.dilations,
            causal=config.autoencoder.decoder.causal
        )
        
        text_to_latent = TextToLatent(
            vocab_size=tokenizer.vocab_size,
            text_embed_dim=config.text_to_latent.text_encoder.embed_dim,
            text_hidden_dim=config.text_to_latent.text_encoder.hidden_dim,
            ref_input_dim=config.text_to_latent.reference_encoder.input_dim,
            ref_hidden_dim=config.text_to_latent.reference_encoder.hidden_dim,
            vf_hidden_dim=config.text_to_latent.vector_field.hidden_dim,
            gamma=config.larope.gamma
        )
        
        duration_predictor = DurationPredictor(
            vocab_size=tokenizer.vocab_size,
            hidden_dim=config.duration_predictor.hidden_dim,
            num_convnext_blocks=config.duration_predictor.num_convnext_blocks
        )
        
        # Load checkpoints
        ae_ckpt_path = checkpoint_dir / "autoencoder" / "checkpoint_final.pt"
        tts_ckpt_path = checkpoint_dir / "tts" / "checkpoint_final.pt"
        dur_ckpt_path = checkpoint_dir / "duration" / "checkpoint_final.pt"
        
        if ae_ckpt_path.exists():
            ae_ckpt = torch.load(ae_ckpt_path, map_location=device)
            latent_encoder.load_state_dict(ae_ckpt["encoder"])
            latent_decoder.load_state_dict(ae_ckpt["decoder"])
            print(f"Loaded autoencoder from {ae_ckpt_path}")
        
        if tts_ckpt_path.exists():
            tts_ckpt = torch.load(tts_ckpt_path, map_location=device)
            text_to_latent.load_state_dict(tts_ckpt["model"])
            print(f"Loaded TTS model from {tts_ckpt_path}")
        
        if dur_ckpt_path.exists():
            dur_ckpt = torch.load(dur_ckpt_path, map_location=device)
            duration_predictor.load_state_dict(dur_ckpt["model"])
            print(f"Loaded duration predictor from {dur_ckpt_path}")
        
        # Move to device
        latent_encoder = latent_encoder.to(device).eval()
        latent_decoder = latent_decoder.to(device).eval()
        text_to_latent = text_to_latent.to(device).eval()
        duration_predictor = duration_predictor.to(device).eval()
        
        # Config dict for inference
        inference_config = {
            "sample_rate": config.audio.sample_rate,
            "hop_length": config.audio.hop_length,
            "compression_factor": config.latent.temporal_compression,
            "nfe": config.flow_matching.nfe,
            "cfg_scale": config.flow_matching.cfg_scale
        }
        
        return cls(
            latent_encoder=latent_encoder,
            latent_decoder=latent_decoder,
            text_to_latent=text_to_latent,
            duration_predictor=duration_predictor,
            tokenizer=tokenizer,
            audio_processor=audio_processor,
            config=inference_config
        )
    
    @torch.no_grad()
    def synthesize(
        self,
        text: str,
        reference_audio: Union[str, Path, torch.Tensor],
        language: Optional[str] = None,
        duration_scale: float = 1.0,
        nfe: Optional[int] = None,
        cfg_scale: Optional[float] = None,
        return_latent: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        –°–∏–Ω—Ç–µ–∑—É—î speech –∑ —Ç–µ–∫—Å—Ç—É.
        
        Args:
            text: Input text
            reference_audio: Reference audio –¥–ª—è speaker/style conditioning
            language: Language code (auto-detected if None)
            duration_scale: Scale factor for duration (>1 = slower)
            nfe: Number of function evaluations (override default)
            cfg_scale: CFG scale (override default)
            return_latent: Also return generated latents
            
        Returns:
            audio: Generated waveform [T]
            latent: (optional) Generated latent [24, T_latent]
        """
        device = next(self.parameters()).device
        
        # 1. Process reference audio
        if isinstance(reference_audio, (str, Path)):
            reference_audio = self.audio_processor.load(reference_audio)
        
        if reference_audio.dim() == 1:
            reference_audio = reference_audio.unsqueeze(0)  # [1, T]
        
        reference_audio = reference_audio.to(device)
        reference_mel = self.audio_processor.compute_mel(reference_audio)
        
        if reference_mel.dim() == 2:
            reference_mel = reference_mel.unsqueeze(0)  # [1, C, T]
        
        # 2. Tokenize text
        if language is None:
            language = detect_language_simple(text)
        
        lang_id = LANGUAGE_CODES.get(language, 0)
        text_ids = self.tokenizer.encode(text).unsqueeze(0).to(device)  # [1, L]
        text_mask = torch.ones(1, text_ids.size(1), dtype=torch.bool, device=device)
        lang_ids = torch.tensor([lang_id], device=device)
        
        # 3. Encode reference to latent
        ref_latent = self.latent_encoder(reference_mel)  # [1, 24, T]
        ref_compressed = compress_latents(ref_latent, self.compression_factor)  # [1, 144, T/6]
        ref_mask = torch.ones(1, ref_compressed.size(-1), dtype=torch.bool, device=device)
        
        # 4. Predict duration
        predicted_duration = self.duration_predictor(
            text_ids=text_ids,
            text_mask=text_mask,
            reference_latent=ref_compressed,
            reference_mask=ref_mask
        )  # [1]
        
        # Apply duration scale
        predicted_duration = predicted_duration * duration_scale
        num_frames = int(predicted_duration.item())
        num_frames = max(num_frames, 1)
        
        # 5. Encode text and reference
        text_encoding = self.text_to_latent.encode_text(
            text_ids, text_mask, lang_id=lang_ids
        )
        reference_encoding = self.text_to_latent.encode_reference(
            ref_compressed, ref_mask
        )
        
        # 6. Generate latents via ODE solver
        if nfe is not None:
            self.ode_solver.nfe = nfe
        if cfg_scale is not None:
            self.ode_solver.cfg_scale = cfg_scale
        
        z_shape = (1, 144, num_frames)
        
        generated_compressed = self.ode_solver.solve(
            model=self.text_to_latent.vector_field,
            z_shape=z_shape,
            text_encoding=text_encoding,
            reference_encoding=reference_encoding,
            z_ref=ref_compressed[:, :, :num_frames] if ref_compressed.size(-1) >= num_frames else None,
            text_mask=text_mask,
            device=device
        )
        
        # 7. Decompress latents
        generated_latent = decompress_latents(generated_compressed, self.compression_factor)
        
        # 8. Decode to waveform
        audio = self.latent_decoder(generated_latent)  # [1, T_audio]
        audio = audio.squeeze(0)  # [T_audio]
        
        if return_latent:
            return audio, generated_latent.squeeze(0)
        return audio
    
    def save_audio(
        self,
        audio: torch.Tensor,
        path: Union[str, Path],
        normalize: bool = True
    ):
        """
        –ó–±–µ—Ä—ñ–≥–∞—î –∞—É–¥—ñ–æ –≤ —Ñ–∞–π–ª.
        
        Args:
            audio: Audio tensor [T]
            path: Output path (.wav)
            normalize: Normalize audio before saving
        """
        if normalize:
            audio = audio / audio.abs().max().clamp(min=1e-8)
        
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)
        
        torchaudio.save(
            str(path),
            audio.cpu(),
            self.sample_rate
        )
    
    @torch.no_grad()
    def synthesize_batch(
        self,
        texts: list[str],
        reference_audios: list[Union[str, Path, torch.Tensor]],
        **kwargs
    ) -> list[torch.Tensor]:
        """
        Batch synthesis (sequential, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏).
        
        Args:
            texts: List of texts
            reference_audios: List of reference audios
            **kwargs: Additional arguments for synthesize()
            
        Returns:
            List of generated audio tensors
        """
        results = []
        
        for text, ref_audio in zip(texts, reference_audios):
            audio = self.synthesize(text, ref_audio, **kwargs)
            results.append(audio)
        
        return results


def benchmark(
    tts: SupertonicTTS,
    text: str,
    reference_audio: torch.Tensor,
    num_runs: int = 10,
    warmup: int = 3
) -> dict:
    """
    Benchmark inference speed.
    
    Args:
        tts: TTS model
        text: Test text
        reference_audio: Reference audio
        num_runs: Number of benchmark runs
        warmup: Warmup runs
        
    Returns:
        Dict with timing statistics
    """
    device = next(tts.parameters()).device
    
    # Warmup
    for _ in range(warmup):
        _ = tts.synthesize(text, reference_audio)
    
    torch.cuda.synchronize() if device.type == "cuda" else None
    
    # Benchmark
    times = []
    audio_lengths = []
    
    for _ in range(num_runs):
        start = time.perf_counter()
        audio = tts.synthesize(text, reference_audio)
        torch.cuda.synchronize() if device.type == "cuda" else None
        end = time.perf_counter()
        
        times.append(end - start)
        audio_lengths.append(len(audio) / tts.sample_rate)
    
    avg_time = np.mean(times)
    std_time = np.std(times)
    avg_audio_len = np.mean(audio_lengths)
    rtf = avg_time / avg_audio_len  # Real-Time Factor
    
    return {
        "avg_inference_time": avg_time,
        "std_inference_time": std_time,
        "avg_audio_length": avg_audio_len,
        "rtf": rtf,
        "speedup": 1.0 / rtf,
        "num_runs": num_runs
    }


def main():
    """CLI –¥–ª—è inference."""
    parser = argparse.ArgumentParser(description="Supertonic v2 TTS Inference")
    parser.add_argument("--text", type=str, required=True, help="Text to synthesize")
    parser.add_argument("--reference", type=str, required=True, help="Reference audio path")
    parser.add_argument("--output", type=str, default="output.wav", help="Output audio path")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpoint directory")
    parser.add_argument("--config", type=str, default="config/default.yaml", help="Config file")
    parser.add_argument("--language", type=str, default=None, help="Language code (auto-detected)")
    parser.add_argument("--duration-scale", type=float, default=1.0, help="Duration scale factor")
    parser.add_argument("--nfe", type=int, default=32, help="Number of function evaluations")
    parser.add_argument("--cfg-scale", type=float, default=3.0, help="CFG scale")
    parser.add_argument("--benchmark", action="store_true", help="Run benchmark")
    parser.add_argument("--device", type=str, default="cuda", help="Device (cuda/cpu)")
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    print(f"Loading model from {args.checkpoint_dir}...")
    tts = SupertonicTTS.from_pretrained(
        args.checkpoint_dir,
        config_path=args.config,
        device=device
    )
    
    print(f"Synthesizing: '{args.text}'")
    print(f"Reference: {args.reference}")
    
    if args.benchmark:
        reference_audio, _ = load_audio(args.reference)
        results = benchmark(tts, args.text, reference_audio)
        
        print(f"\n=== Benchmark Results ===")
        print(f"Average inference time: {results['avg_inference_time']*1000:.2f} ms")
        print(f"Average audio length: {results['avg_audio_length']:.2f} s")
        print(f"Real-Time Factor (RTF): {results['rtf']:.4f}")
        print(f"Speed: {results['speedup']:.1f}√ó real-time")
    
    start = time.perf_counter()
    
    audio = tts.synthesize(
        text=args.text,
        reference_audio=args.reference,
        language=args.language,
        duration_scale=args.duration_scale,
        nfe=args.nfe,
        cfg_scale=args.cfg_scale
    )
    
    inference_time = time.perf_counter() - start
    audio_duration = len(audio) / tts.sample_rate
    
    tts.save_audio(audio, args.output)
    
    print(f"\nGenerated {audio_duration:.2f}s audio in {inference_time*1000:.0f}ms")
    print(f"Speed: {audio_duration/inference_time:.1f}√ó real-time")
    print(f"Saved to: {args.output}")


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: README.md
–†–û–ó–ú–Ü–†: 9.19 KB
==================================================

# üá∫üá¶ Supertonic v2 TTS - Ukrainian

–ü–æ–≤–Ω–∞ —Ä–µ—ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è **Supertonic v2 TTS** –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏.

> **Paper**: [Supertonic: Lightweight Text-to-Speech for Super-Diverse Settings](https://arxiv.org/abs/2509.11084)

## üéØ Features

- **66M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** (–∫–æ–º–ø–∞–∫—Ç–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞)
- **44.1kHz** –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω–∏–π –∞—É–¥—ñ–æ –≤–∏—Ö—ñ–¥
- **–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞** –∑ –Ω—É–ª—è
- **Character-level** —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è (–±–µ–∑ G2P)
- **Flow-matching** –¥–ª—è —è–∫—ñ—Å–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
- **ONNX export** –¥–ª—è production

## üìä –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞

| Module | Parameters | Description |
|--------|-----------|-------------|
| Speech Autoencoder | ~47M | Vocos-based encoder/decoder –∑ ISTFT |
| Text-to-Latent | ~19M | Flow-matching –∑ LARoPE (Œ≥=10) |
| Duration Predictor | ~0.5M | –®–≤–∏–¥–∫–µ L1 —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è |

## üöÄ Quick Start

### Vast.ai Setup

```bash
# 1. –ö–ª–æ–Ω—É–π—Ç–µ —Ä–µ–ø–æ
git clone https://github.com/your-username/supertonic_v2_tts_uk.git
cd supertonic_v2_tts_uk

# 2. –ó–∞–ø—É—Å—Ç—ñ—Ç—å setup —Å–∫—Ä–∏–ø—Ç
chmod +x scripts/setup_vast.sh
./scripts/setup_vast.sh --minimal
```

### –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

```bash
# 1. Autoencoder (—Å–ø–æ—á–∞—Ç–∫—É) - ~7-8 –¥–Ω—ñ–≤ –Ω–∞ 1√ó5090
python train_autoencoder.py --config config/default.yaml

# 2. Text-to-Latent - ~4-5 –¥–Ω—ñ–≤ –Ω–∞ 1√ó5090
python train_text_to_latent.py --config config/default.yaml \
    --autoencoder-checkpoint checkpoints/autoencoder/checkpoint_final.pt

# 3. Duration Predictor - ~20 —Ö–≤–∏–ª–∏–Ω
python train_duration_predictor.py --config config/default.yaml
```

### Inference

```bash
python inference.py \
    --text "–ü—Ä–∏–≤—ñ—Ç, —è–∫ —Å–ø—Ä–∞–≤–∏?" \
    --reference samples/reference.wav \
    --output output.wav
```

## üìö –î–∞—Ç–∞—Å–µ—Ç–∏

### –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞

| –î–∞—Ç–∞—Å–µ—Ç | –ì–æ–¥–∏–Ω | –°–ø—ñ–∫–µ—Ä—ñ–≤ | –ü–æ—Å–∏–ª–∞–Ω–Ω—è |
|---------|-------|----------|-----------|
| **OpenTTS LADA** | ~5 | 1 (female) | [HuggingFace](https://huggingface.co/datasets/speech-uk/opentts-lada) ‚úÖ |
| **OpenTTS TETIANA** | ~5 | 1 (female) | [HuggingFace](https://huggingface.co/datasets/speech-uk/opentts-tetiana) ‚úÖ |
| **OpenTTS KATERYNA** | ~5 | 1 (female) | [HuggingFace](https://huggingface.co/datasets/speech-uk/opentts-kateryna) ‚úÖ |
| **OpenTTS MYKYTA** | ~5 | 1 (male) | [HuggingFace](https://huggingface.co/datasets/speech-uk/opentts-mykyta) ‚úÖ |
| **OpenTTS OLEKSA** | ~5 | 1 (male) | [HuggingFace](https://huggingface.co/datasets/speech-uk/opentts-oleksa) ‚úÖ |
| **Ukrainian Podcasts** | ~100+ | Many | [HuggingFace](https://huggingface.co/datasets/taras-sereda/uk-pods) ‚úÖ |
| **Common Voice UK** | ~80 | 1000+ | [Mozilla](https://commonvoice.mozilla.org/uk/datasets) |
| **Voice of America** | ~390 | Many | [HuggingFace](https://huggingface.co/datasets/speech-uk/voice-of-america) ‚úÖ |
| **Broadcast Speech** | ~300 | Many | [HuggingFace](https://huggingface.co/datasets/Yehor/broadcast-speech-uk) ‚úÖ |
| **Compiled Dataset** | ~1200 | Many | [NextCloud](https://nx16725.your-storageshare.de/s/cAbcBeXtdz7znDN) / [Torrent](https://academictorrents.com/details/fcf8bb60c59e9eb583df003d54ed61776650beb8) |

> ‚ö†Ô∏è **M-AILABS** (caito.de) –Ω–∞—Ä–∞–∑—ñ **–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π**. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ OpenTTS voices –∑–∞–º—ñ—Å—Ç—å –Ω—å–æ–≥–æ.

### –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ (–¥–ª—è pretrain)

| –î–∞—Ç–∞—Å–µ—Ç | –ì–æ–¥–∏–Ω | –ü–æ—Å–∏–ª–∞–Ω–Ω—è |
|---------|-------|-----------|
| LJSpeech | 24 | [Link](https://keithito.com/LJ-Speech-Dataset/) |
| LibriTTS-R | 585 | [Link](https://www.openslr.org/141/) |

### –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

```bash
# –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä (~50GB)
python scripts/download_datasets.py --minimal

# –ü–æ–≤–Ω–∏–π –Ω–∞–±—ñ—Ä (~500GB)
python scripts/download_datasets.py --full
```

## üñ•Ô∏è Vast.ai Configuration

### üèÜ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ GPU (—Ü—ñ–Ω–∞/—à–≤–∏–¥–∫—ñ—Å—Ç—å)

| GPU | –¶—ñ–Ω–∞/–≥–æ–¥ | –ß–∞—Å | –í–∞—Ä—Ç—ñ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è |
|-----|----------|-----|----------|--------------|
| **A100 40GB (Italy)** | $0.152 | ~7 –¥–Ω—ñ–≤ | **~$26** | üí∞ –ù–∞–π–¥–µ—à–µ–≤—à–µ |
| 2√ó A100 40GB (Italy) | $0.299 | ~4 –¥–Ω—ñ | ~$29 | –®–≤–∏–¥–∫–æ + –¥–µ—à–µ–≤–æ |
| **H100 SXM (India)** | $0.746 | ~2-3 –¥–Ω—ñ | **~$35-45** | üöÄ –ù–∞–π—à–≤–∏–¥—à–µ |
| RTX 4090 (Portugal) | $0.155 | ~14 –¥–Ω—ñ–≤ | ~$52 | Backup |
| RTX PRO 6000 Blackwell | $0.413 | ~5-6 –¥–Ω—ñ–≤ | ~$55 | 96GB VRAM |

### üöÄ H100 SXM - –ù–∞–π—à–≤–∏–¥—à–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç

```bash
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è H100 80GB
./scripts/train_h100.sh
# –∞–±–æ
python train_autoencoder.py --config config/h100_optimized.yaml
```

**–ü–µ—Ä–µ–≤–∞–≥–∏ H100:**
- Transformer Engine (FP8) ‚Äî 2√ó speedup
- 3,350 GB/s memory bandwidth
- 80GB VRAM ‚Üí batch_size=48-128

### –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π Template

- **PyTorch (Vast)** –∑ Jupyter
- CUDA 12.x

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É

```
supertonic_v2_tts_uk/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ default.yaml           # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
‚îú‚îÄ‚îÄ supertonic/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attention.py       # Multi-head attention –∑ RoPE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ convnext.py        # ConvNeXt blocks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ larope.py          # Length-Aware RoPE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ speech_autoencoder.py  # Encoder/Decoder/Discriminators
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_to_latent.py  # Text‚ÜíLatent flow-matching
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ duration_predictor.py
‚îÇ   ‚îú‚îÄ‚îÄ losses/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_loss.py    # GAN + Mel + FM loss
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flow_matching_loss.py  # CFM loss + ODE solver
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ duration_loss.py
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îú‚îÄ‚îÄ preprocessing.py   # Audio processing
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer.py       # Multilingual tokenizer
‚îÇ       ‚îú‚îÄ‚îÄ dataset.py         # Dataset classes
‚îÇ       ‚îî‚îÄ‚îÄ collate.py         # Batch collation
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup_vast.sh          # Vast.ai setup
‚îÇ   ‚îú‚îÄ‚îÄ download_datasets.py   # Dataset downloader
‚îÇ   ‚îî‚îÄ‚îÄ prepare_manifest.py    # Manifest preparation
‚îú‚îÄ‚îÄ train_autoencoder.py       # Autoencoder training
‚îú‚îÄ‚îÄ train_text_to_latent.py    # TTS training
‚îú‚îÄ‚îÄ train_duration_predictor.py
‚îú‚îÄ‚îÄ inference.py               # Synthesis pipeline
‚îú‚îÄ‚îÄ export_onnx.py             # ONNX export
‚îî‚îÄ‚îÄ requirements.txt
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

–û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ `config/default.yaml`:

```yaml
# Audio
audio:
  sample_rate: 44100
  n_fft: 2048
  hop_length: 512
  n_mels: 228

# Latent space
latent:
  dim: 24
  temporal_compression: 6  # Kc

# Flow matching
flow_matching:
  sigma_min: 1.0e-8
  p_uncond: 0.05           # CFG probability
  cfg_scale: 3.0           # Inference CFG scale
  nfe: 32                  # ODE steps

# LARoPE
larope:
  gamma: 10                # Critical for alignment!
```

## üìà –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

### –ï—Ç–∞–ø 1: Autoencoder

```bash
python train_autoencoder.py \
    --config config/default.yaml \
    --data-dir data/raw \
    --batch-size 16 \
    --epochs 50
```

**Loss weights**: Œª_recon=45, Œª_adv=1, Œª_fm=0.1

### –ï—Ç–∞–ø 2: Text-to-Latent

```bash
python train_text_to_latent.py \
    --config config/default.yaml \
    --autoencoder-checkpoint checkpoints/autoencoder/checkpoint_final.pt \
    --batch-size 64 \
    --expansion-factor 4 \
    --iterations 700000
```

**Context-Sharing**: B=64, Ke=4 ‚Üí effective batch = 256

### –ï—Ç–∞–ø 3: Duration Predictor

```bash
python train_duration_predictor.py \
    --config config/default.yaml \
    --iterations 3000
```

## üîß ONNX Export

```bash
python export_onnx.py \
    --checkpoint-dir checkpoints \
    --output-dir onnx_models \
    --opset 17
```

Outputs:
- `latent_encoder.onnx`
- `latent_decoder.onnx`
- `text_encoder.onnx`
- `reference_encoder.onnx`
- `vector_field.onnx`
- `duration_predictor.onnx`

Total: ~260MB

## üìä Benchmarks

Target metrics (based on paper):

| Metric | Target |
|--------|--------|
| Word Error Rate (WER) | <3% |
| Speaker Similarity | >0.85 |
| MOS | >4.0 |
| RTF (1√ó5090) | <0.1 |

## üîó Resources

- [Supertonic v2 Paper](https://arxiv.org/abs/2509.11084)
- [Ukrainian TTS Resources](https://github.com/egorsmkv/speech-recognition-uk)
- [HuggingFace speech-uk](https://huggingface.co/speech-uk)
- [Discord: Ukrainian Data Science](https://bit.ly/discord-uds)

## üìù Citation

```bibtex
@article{supertonic2025,
  title={Supertonic: Lightweight Text-to-Speech for Super-Diverse Settings},
  author={...},
  journal={arXiv preprint arXiv:2509.11084},
  year={2025}
}
```

## üìú License

MIT License

## üôè Acknowledgements

- [egorsmkv/speech-recognition-uk](https://github.com/egorsmkv/speech-recognition-uk) - Ukrainian speech resources
- [Yehor/opentts-uk](https://huggingface.co/datasets/Yehor/opentts-uk) - OpenTTS voices
- [Mozilla Common Voice](https://commonvoice.mozilla.org/) - Ukrainian dataset
- [speech-uk](https://huggingface.co/speech-uk) - HuggingFace organization



==================================================
–§–ê–ô–õ: scripts/analyze_checkpoint.py
–†–û–ó–ú–Ü–†: 29.98 KB
==================================================

#!/usr/bin/env python3
"""
üéµ SUPERTONIC V2 - ADVANCED CHECKPOINT ANALYZER

Comprehensive analysis with TTS-relevant metrics:
- Mel-based metrics (most important for TTS)
- Multi-resolution spectral analysis
- Perceptual quality indicators
- Amplitude/dynamics analysis
- Batch comparison across checkpoints

Usage:
    python scripts/analyze_checkpoint.py --checkpoint checkpoints/autoencoder/checkpoint_20000.pt
    python scripts/analyze_checkpoint.py --checkpoint checkpoints/autoencoder/checkpoint_20000.pt --audio path/to/test.wav
    python scripts/analyze_checkpoint.py --compare checkpoints/autoencoder/  # Compare all checkpoints
"""

import sys
import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn.functional as F
import torchaudio
import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder
from supertonic.data.preprocessing import AudioProcessor


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# COLORS AND FORMATTING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    RESET = '\033[0m'


def colored(text: str, color: str) -> str:
    return f"{color}{text}{Colors.RESET}"


def header(text: str) -> str:
    line = "‚ïê" * 70
    return f"\n{Colors.CYAN}{line}\n{Colors.BOLD}  {text}\n{Colors.CYAN}{line}{Colors.RESET}"


def subheader(text: str) -> str:
    return f"\n{Colors.YELLOW}‚ñ∂ {text}{Colors.RESET}"


def metric_bar(value: float, max_val: float, width: int = 30, invert: bool = False) -> str:
    """Create a visual progress bar for metrics."""
    ratio = min(value / max_val, 1.0)
    if invert:
        ratio = 1 - ratio
    filled = int(ratio * width)
    bar = "‚ñà" * filled + "‚ñë" * (width - filled)
    
    # Color based on quality
    if invert:
        color = Colors.GREEN if ratio > 0.7 else Colors.YELLOW if ratio > 0.4 else Colors.RED
    else:
        color = Colors.GREEN if ratio < 0.3 else Colors.YELLOW if ratio < 0.6 else Colors.RED
    
    return f"{color}{bar}{Colors.RESET}"


def quality_emoji(score: float, thresholds: Tuple[float, float] = (0.3, 0.6)) -> str:
    """Return emoji based on quality score (lower is better)."""
    if score < thresholds[0]:
        return "üü¢"
    elif score < thresholds[1]:
        return "üü°"
    else:
        return "üî¥"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DATA CLASSES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class AudioMetrics:
    """Comprehensive audio quality metrics."""
    # Mel-based (most important for TTS)
    mel_l1: float
    mel_mse: float
    mel_cosine_sim: float
    
    # Multi-resolution mel
    mel_l1_256: float   # Fine detail
    mel_l1_512: float   # Medium
    mel_l1_1024: float  # Coarse structure
    
    # Spectral
    spectral_convergence: float
    log_spectral_distance: float
    
    # Amplitude/Dynamics
    amplitude_ratio: float
    rms_ratio: float
    peak_ratio: float
    dynamic_range_diff: float
    
    # Waveform (less important)
    l1_loss: float
    mse_loss: float
    
    # Overall score (composite)
    overall_score: float


@dataclass
class CheckpointInfo:
    """Checkpoint metadata."""
    path: str
    step: int
    n_mels: int
    hop_length: int
    sample_rate: int


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MODEL LOADING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def strip_ddp_prefix(state_dict: dict) -> dict:
    """Remove 'module.' prefix from DDP state dict."""
    return {k.replace("module.", ""): v for k, v in state_dict.items()}


def load_checkpoint(checkpoint_path: str, device: str = "cuda") -> Tuple[LatentEncoder, LatentDecoder, AudioProcessor, CheckpointInfo]:
    """Load checkpoint and create models."""
    checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
    
    encoder_state = strip_ddp_prefix(checkpoint["encoder"])
    decoder_state = strip_ddp_prefix(checkpoint["decoder"])
    
    # Detect params
    n_mels = encoder_state["input_conv.weight"].shape[1]
    
    if "head.fc.weight" in decoder_state:
        hop_length = decoder_state["head.fc.weight"].shape[0]
    else:
        hop_length = 256
    
    n_fft = 1024 if hop_length == 256 else 2048
    sample_rate = 22050 if hop_length == 256 else 44100
    
    # Extract step from filename
    step = 0
    try:
        step = int(Path(checkpoint_path).stem.split("_")[-1])
    except:
        pass
    
    # Create models
    encoder = LatentEncoder(
        input_dim=n_mels,
        hidden_dim=512,
        output_dim=24,
        num_blocks=10,
        kernel_size=7,
    ).to(device)
    
    decoder = LatentDecoder(
        input_dim=24,
        hidden_dim=512,
        num_blocks=10,
        kernel_size=7,
        dilations=[1, 2, 4, 1, 2, 4, 1, 1, 1, 1],
        n_fft=n_fft,
        hop_length=hop_length,
        causal=True,
    ).to(device)
    
    encoder.load_state_dict(encoder_state)
    decoder.load_state_dict(decoder_state)
    encoder.eval()
    decoder.eval()
    
    audio_processor = AudioProcessor(
        sample_rate=sample_rate,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels
    )
    
    info = CheckpointInfo(
        path=checkpoint_path,
        step=step,
        n_mels=n_mels,
        hop_length=hop_length,
        sample_rate=sample_rate
    )
    
    return encoder, decoder, audio_processor, info


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# METRIC COMPUTATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def compute_mel_spectrogram(audio: torch.Tensor, n_fft: int, hop_length: int, 
                            n_mels: int, sample_rate: int) -> torch.Tensor:
    """Compute mel spectrogram."""
    transform = torchaudio.transforms.MelSpectrogram(
        sample_rate=sample_rate,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels,
        norm="slaney",
        mel_scale="slaney"
    )
    mel = transform(audio)
    return torch.log(mel.clamp(min=1e-5))


def compute_stft(audio: torch.Tensor, n_fft: int, hop_length: int) -> torch.Tensor:
    """Compute STFT magnitude."""
    window = torch.hann_window(n_fft, device=audio.device)
    stft = torch.stft(audio, n_fft=n_fft, hop_length=hop_length, 
                      window=window, return_complex=True)
    return stft.abs()


def spectral_convergence(pred_mag: torch.Tensor, target_mag: torch.Tensor) -> float:
    """Spectral convergence loss (lower is better)."""
    return (torch.norm(target_mag - pred_mag, p="fro") / torch.norm(target_mag, p="fro")).item()


def log_spectral_distance(pred_mag: torch.Tensor, target_mag: torch.Tensor) -> float:
    """Log spectral distance (lower is better)."""
    pred_log = torch.log(pred_mag.clamp(min=1e-5))
    target_log = torch.log(target_mag.clamp(min=1e-5))
    return torch.mean((pred_log - target_log).pow(2)).sqrt().item()


def compute_metrics(original: torch.Tensor, reconstructed: torch.Tensor, 
                   sample_rate: int = 22050) -> AudioMetrics:
    """Compute comprehensive audio quality metrics."""
    
    # Ensure same length
    min_len = min(len(original), len(reconstructed))
    orig = original[:min_len]
    recon = reconstructed[:min_len]
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Mel-based metrics (MOST IMPORTANT)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Standard mel (n_fft=1024)
    mel_orig = compute_mel_spectrogram(orig, 1024, 256, 80, sample_rate)
    mel_recon = compute_mel_spectrogram(recon, 1024, 256, 80, sample_rate)
    
    mel_l1 = F.l1_loss(mel_recon, mel_orig).item()
    mel_mse = F.mse_loss(mel_recon, mel_orig).item()
    
    # Cosine similarity (1.0 = identical, 0.0 = orthogonal)
    mel_cosine = F.cosine_similarity(
        mel_orig.flatten().unsqueeze(0), 
        mel_recon.flatten().unsqueeze(0)
    ).item()
    
    # Multi-resolution mel
    mel_orig_256 = compute_mel_spectrogram(orig, 256, 64, 80, sample_rate)
    mel_recon_256 = compute_mel_spectrogram(recon, 256, 64, 80, sample_rate)
    mel_l1_256 = F.l1_loss(mel_recon_256, mel_orig_256).item()
    
    mel_orig_512 = compute_mel_spectrogram(orig, 512, 128, 80, sample_rate)
    mel_recon_512 = compute_mel_spectrogram(recon, 512, 128, 80, sample_rate)
    mel_l1_512 = F.l1_loss(mel_recon_512, mel_orig_512).item()
    
    mel_orig_1024 = compute_mel_spectrogram(orig, 1024, 256, 80, sample_rate)
    mel_recon_1024 = compute_mel_spectrogram(recon, 1024, 256, 80, sample_rate)
    mel_l1_1024 = F.l1_loss(mel_recon_1024, mel_orig_1024).item()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Spectral metrics
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    stft_orig = compute_stft(orig, 1024, 256)
    stft_recon = compute_stft(recon, 1024, 256)
    
    sc = spectral_convergence(stft_recon, stft_orig)
    lsd = log_spectral_distance(stft_recon, stft_orig)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Amplitude/Dynamics
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    orig_abs = orig.abs()
    recon_abs = recon.abs()
    
    amplitude_ratio = (recon_abs.mean() / (orig_abs.mean() + 1e-8)).item()
    rms_ratio = (recon.pow(2).mean().sqrt() / (orig.pow(2).mean().sqrt() + 1e-8)).item()
    peak_ratio = (recon_abs.max() / (orig_abs.max() + 1e-8)).item()
    
    # Dynamic range (dB)
    orig_dr = 20 * np.log10((orig_abs.max() / (orig_abs[orig_abs > 0].min() + 1e-8)).item() + 1e-8)
    recon_dr = 20 * np.log10((recon_abs.max() / (recon_abs[recon_abs > 0].min() + 1e-8)).item() + 1e-8)
    dynamic_range_diff = abs(orig_dr - recon_dr)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Waveform metrics (less important for TTS)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    l1_loss = F.l1_loss(recon, orig).item()
    mse_loss = F.mse_loss(recon, orig).item()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Overall score (weighted composite)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Lower is better, normalized roughly to 0-1 range
    overall = (
        mel_l1 * 0.4 +                    # 40% - most important
        sc * 0.2 +                         # 20% - spectral shape
        (1 - mel_cosine) * 0.2 +          # 20% - similarity
        abs(1 - amplitude_ratio) * 0.1 +  # 10% - loudness
        lsd * 0.1                          # 10% - log spectral
    )
    
    return AudioMetrics(
        mel_l1=mel_l1,
        mel_mse=mel_mse,
        mel_cosine_sim=mel_cosine,
        mel_l1_256=mel_l1_256,
        mel_l1_512=mel_l1_512,
        mel_l1_1024=mel_l1_1024,
        spectral_convergence=sc,
        log_spectral_distance=lsd,
        amplitude_ratio=amplitude_ratio,
        rms_ratio=rms_ratio,
        peak_ratio=peak_ratio,
        dynamic_range_diff=dynamic_range_diff,
        l1_loss=l1_loss,
        mse_loss=mse_loss,
        overall_score=overall
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ANALYSIS FUNCTIONS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def reconstruct_audio(encoder: LatentEncoder, decoder: LatentDecoder,
                     audio_processor: AudioProcessor, audio: torch.Tensor,
                     device: str = "cuda") -> torch.Tensor:
    """Reconstruct audio through encoder-decoder."""
    with torch.no_grad():
        mel = audio_processor.compute_mel(audio).unsqueeze(0).to(device)
        latent = encoder(mel)
        reconstructed = decoder(latent)
    return reconstructed.squeeze(0).cpu()


def analyze_single(checkpoint_path: str, audio_path: str, output_dir: str = "analysis_output",
                  device: str = "cuda") -> AudioMetrics:
    """Analyze single audio file with checkpoint."""
    
    print(header("SUPERTONIC V2 - CHECKPOINT ANALYZER"))
    print(f"\n  {Colors.DIM}Checkpoint:{Colors.RESET} {checkpoint_path}")
    print(f"  {Colors.DIM}Audio:{Colors.RESET} {audio_path}")
    
    # Load checkpoint
    print(subheader("Loading checkpoint..."))
    encoder, decoder, audio_processor, info = load_checkpoint(checkpoint_path, device)
    print(f"  ‚úì Step: {Colors.BOLD}{info.step:,}{Colors.RESET}")
    print(f"  ‚úì Config: {info.n_mels} mels, hop={info.hop_length}, sr={info.sample_rate}")
    
    # Load audio
    print(subheader("Loading audio..."))
    audio, sr = torchaudio.load(audio_path)
    if sr != info.sample_rate:
        audio = torchaudio.functional.resample(audio, sr, info.sample_rate)
    audio = audio.mean(dim=0)  # Mono
    
    duration = len(audio) / info.sample_rate
    print(f"  ‚úì Duration: {duration:.2f}s ({len(audio):,} samples)")
    
    # Reconstruct
    print(subheader("Reconstructing..."))
    reconstructed = reconstruct_audio(encoder, decoder, audio_processor, audio, device)
    
    # Compute metrics
    print(subheader("Computing metrics..."))
    metrics = compute_metrics(audio, reconstructed, info.sample_rate)
    
    # Display results
    print(header("RESULTS"))
    
    # Mel metrics (most important)
    print(f"\n  {Colors.BOLD}üìä MEL METRICS (Primary){Colors.RESET}")
    print(f"  ‚îú‚îÄ Mel L1:        {metrics.mel_l1:.4f}  {metric_bar(metrics.mel_l1, 1.0)} {quality_emoji(metrics.mel_l1)}")
    print(f"  ‚îú‚îÄ Mel MSE:       {metrics.mel_mse:.4f}  {metric_bar(metrics.mel_mse, 0.5)}")
    print(f"  ‚îî‚îÄ Mel Cosine:    {metrics.mel_cosine_sim:.4f}  {metric_bar(1-metrics.mel_cosine_sim, 1.0, invert=True)} {'üü¢' if metrics.mel_cosine_sim > 0.9 else 'üü°' if metrics.mel_cosine_sim > 0.7 else 'üî¥'}")
    
    # Multi-resolution
    print(f"\n  {Colors.BOLD}üîç MULTI-RESOLUTION MEL{Colors.RESET}")
    print(f"  ‚îú‚îÄ Fine (256):    {metrics.mel_l1_256:.4f}  {metric_bar(metrics.mel_l1_256, 1.0)}")
    print(f"  ‚îú‚îÄ Medium (512):  {metrics.mel_l1_512:.4f}  {metric_bar(metrics.mel_l1_512, 1.0)}")
    print(f"  ‚îî‚îÄ Coarse (1024): {metrics.mel_l1_1024:.4f}  {metric_bar(metrics.mel_l1_1024, 1.0)}")
    
    # Spectral
    print(f"\n  {Colors.BOLD}üìà SPECTRAL METRICS{Colors.RESET}")
    print(f"  ‚îú‚îÄ Spectral Conv: {metrics.spectral_convergence:.4f}  {metric_bar(metrics.spectral_convergence, 1.0)}")
    print(f"  ‚îî‚îÄ Log Spectral:  {metrics.log_spectral_distance:.4f}  {metric_bar(metrics.log_spectral_distance, 2.0)}")
    
    # Amplitude
    print(f"\n  {Colors.BOLD}üîä AMPLITUDE/DYNAMICS{Colors.RESET}")
    amp_quality = 'üü¢' if 0.8 < metrics.amplitude_ratio < 1.2 else 'üü°' if 0.6 < metrics.amplitude_ratio < 1.4 else 'üî¥'
    print(f"  ‚îú‚îÄ Amplitude:     {metrics.amplitude_ratio:.2%}  {amp_quality}")
    print(f"  ‚îú‚îÄ RMS Ratio:     {metrics.rms_ratio:.2%}")
    print(f"  ‚îú‚îÄ Peak Ratio:    {metrics.peak_ratio:.2%}")
    print(f"  ‚îî‚îÄ DR Diff:       {metrics.dynamic_range_diff:.1f} dB")
    
    # Overall
    print(f"\n  {Colors.BOLD}{'‚îÄ'*50}{Colors.RESET}")
    overall_emoji = 'üü¢' if metrics.overall_score < 0.3 else 'üü°' if metrics.overall_score < 0.5 else 'üî¥'
    print(f"  {Colors.BOLD}OVERALL SCORE: {metrics.overall_score:.4f}{Colors.RESET}  {overall_emoji}")
    print(f"  {Colors.DIM}(Lower is better, weighted: 40% Mel, 20% Spectral, 20% Cosine, 10% Amp, 10% LSD){Colors.RESET}")
    
    # Save outputs
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    step_str = f"step{info.step}"
    audio_name = Path(audio_path).stem
    
    # Save audio files
    orig_path = output_path / f"{audio_name}_{step_str}_original.wav"
    recon_path = output_path / f"{audio_name}_{step_str}_reconstructed.wav"
    
    torchaudio.save(str(orig_path), audio.unsqueeze(0), info.sample_rate)
    torchaudio.save(str(recon_path), reconstructed.unsqueeze(0), info.sample_rate)
    
    # Save metrics JSON
    metrics_path = output_path / f"{audio_name}_{step_str}_metrics.json"
    with open(metrics_path, "w") as f:
        json.dump(asdict(metrics), f, indent=2)
    
    print(subheader("Saved outputs"))
    print(f"  ‚îú‚îÄ {orig_path}")
    print(f"  ‚îú‚îÄ {recon_path}")
    print(f"  ‚îî‚îÄ {metrics_path}")
    
    return metrics


def compare_checkpoints(checkpoint_dir: str, audio_path: str, device: str = "cuda"):
    """Compare multiple checkpoints on same audio."""
    
    print(header("CHECKPOINT COMPARISON"))
    
    checkpoint_dir = Path(checkpoint_dir)
    checkpoints = sorted(checkpoint_dir.glob("*.pt"), 
                        key=lambda x: int(x.stem.split("_")[-1]) if x.stem.split("_")[-1].isdigit() else 0)
    
    if not checkpoints:
        print(f"  {Colors.RED}No checkpoints found in {checkpoint_dir}{Colors.RESET}")
        return
    
    print(f"\n  Found {len(checkpoints)} checkpoints")
    print(f"  Audio: {audio_path}")
    
    # Load audio once
    audio, sr = torchaudio.load(audio_path)
    
    results = []
    
    print(f"\n  {Colors.BOLD}{'Step':>8} {'Mel L1':>10} {'Cosine':>10} {'Amp':>10} {'Overall':>10}{Colors.RESET}")
    print(f"  {'-'*50}")
    
    for cp in checkpoints:
        try:
            encoder, decoder, audio_processor, info = load_checkpoint(str(cp), device)
            
            # Resample if needed
            audio_proc = audio.clone()
            if sr != info.sample_rate:
                audio_proc = torchaudio.functional.resample(audio_proc, sr, info.sample_rate)
            audio_proc = audio_proc.mean(dim=0)
            
            reconstructed = reconstruct_audio(encoder, decoder, audio_processor, audio_proc, device)
            metrics = compute_metrics(audio_proc, reconstructed, info.sample_rate)
            
            results.append({
                "step": info.step,
                "metrics": asdict(metrics)
            })
            
            # Color coding
            mel_color = Colors.GREEN if metrics.mel_l1 < 0.4 else Colors.YELLOW if metrics.mel_l1 < 0.6 else Colors.RED
            cos_color = Colors.GREEN if metrics.mel_cosine_sim > 0.9 else Colors.YELLOW if metrics.mel_cosine_sim > 0.7 else Colors.RED
            amp_color = Colors.GREEN if 0.8 < metrics.amplitude_ratio < 1.2 else Colors.YELLOW
            overall_color = Colors.GREEN if metrics.overall_score < 0.3 else Colors.YELLOW if metrics.overall_score < 0.5 else Colors.RED
            
            print(f"  {info.step:>8,} {mel_color}{metrics.mel_l1:>10.4f}{Colors.RESET} "
                  f"{cos_color}{metrics.mel_cosine_sim:>10.4f}{Colors.RESET} "
                  f"{amp_color}{metrics.amplitude_ratio:>9.0%}{Colors.RESET} "
                  f"{overall_color}{metrics.overall_score:>10.4f}{Colors.RESET}")
            
            # Free memory
            del encoder, decoder
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"  {info.step:>8} {Colors.RED}Error: {e}{Colors.RESET}")
    
    # Show improvement
    if len(results) >= 2:
        first = results[0]["metrics"]
        last = results[-1]["metrics"]
        
        mel_improvement = (first["mel_l1"] - last["mel_l1"]) / first["mel_l1"] * 100
        cos_improvement = (last["mel_cosine_sim"] - first["mel_cosine_sim"]) / (1 - first["mel_cosine_sim"] + 1e-8) * 100
        amp_improvement = abs(1 - last["amplitude_ratio"]) < abs(1 - first["amplitude_ratio"])
        
        print(f"\n  {Colors.BOLD}üìà IMPROVEMENT (first ‚Üí last){Colors.RESET}")
        print(f"  ‚îú‚îÄ Mel L1:    {'+' if mel_improvement > 0 else ''}{mel_improvement:.1f}%")
        print(f"  ‚îú‚îÄ Cosine:    {'+' if cos_improvement > 0 else ''}{cos_improvement:.1f}%")
        print(f"  ‚îî‚îÄ Amplitude: {'‚úì Improved' if amp_improvement else '‚úó Worsened'}")
    
    # Save comparison
    output_path = Path("analysis_output") / "comparison.json"
    output_path.parent.mkdir(exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\n  üíæ Saved: {output_path}")


def list_test_audio(data_dir: str = "data", limit: int = 20) -> List[str]:
    """List available test audio files."""
    data_path = Path(data_dir)
    audio_files = []
    
    for pattern in ["**/*.wav", "**/*.flac", "**/*.mp3"]:
        for f in data_path.glob(pattern):
            audio_files.append(str(f))
            if len(audio_files) >= limit:
                break
        if len(audio_files) >= limit:
            break
    
    return sorted(audio_files)


def interactive_menu(device: str = "cuda"):
    """Interactive mode."""
    
    print(header("SUPERTONIC V2 - CHECKPOINT ANALYZER"))
    print(f"\n  Device: {Colors.CYAN}{device}{Colors.RESET}")
    
    checkpoint_dir = Path("checkpoints/autoencoder")
    current_checkpoint = None
    
    while True:
        print(f"\n{'‚îÄ'*50}")
        print(f"  {Colors.BOLD}MENU{Colors.RESET}")
        print(f"  1. List checkpoints")
        print(f"  2. Analyze checkpoint + audio")
        print(f"  3. Compare all checkpoints")
        print(f"  4. List test audio files")
        print(f"  0. Exit")
        print(f"{'‚îÄ'*50}")
        
        choice = input("  Choice: ").strip()
        
        if choice == "0":
            print(f"\n  {Colors.CYAN}Goodbye! üëã{Colors.RESET}\n")
            break
            
        elif choice == "1":
            checkpoints = sorted(checkpoint_dir.glob("*.pt"),
                               key=lambda x: int(x.stem.split("_")[-1]) if x.stem.split("_")[-1].isdigit() else 0)
            print(f"\n  {Colors.BOLD}Found {len(checkpoints)} checkpoints:{Colors.RESET}")
            for i, cp in enumerate(checkpoints):
                step = cp.stem.split("_")[-1]
                print(f"    [{i:>2}] {cp.name}")
                
        elif choice == "2":
            # Select checkpoint
            checkpoints = sorted(checkpoint_dir.glob("*.pt"),
                               key=lambda x: int(x.stem.split("_")[-1]) if x.stem.split("_")[-1].isdigit() else 0)
            print(f"\n  {Colors.BOLD}Checkpoints:{Colors.RESET}")
            for i, cp in enumerate(checkpoints):
                print(f"    [{i:>2}] {cp.name}")
            
            idx = input("  Checkpoint index (or path): ").strip()
            try:
                if idx.isdigit():
                    cp_path = str(checkpoints[int(idx)])
                else:
                    cp_path = idx
            except:
                print(f"  {Colors.RED}Invalid selection{Colors.RESET}")
                continue
            
            # Select audio
            audio_files = list_test_audio()
            print(f"\n  {Colors.BOLD}Sample audio files:{Colors.RESET}")
            for i, af in enumerate(audio_files[:10]):
                print(f"    [{i:>2}] {af}")
            
            audio_idx = input("  Audio index (or path): ").strip()
            try:
                if audio_idx.isdigit():
                    audio_path = audio_files[int(audio_idx)]
                else:
                    audio_path = audio_idx
            except:
                print(f"  {Colors.RED}Invalid selection{Colors.RESET}")
                continue
            
            # Analyze
            analyze_single(cp_path, audio_path, device=device)
            
        elif choice == "3":
            audio_files = list_test_audio()
            print(f"\n  {Colors.BOLD}Sample audio files:{Colors.RESET}")
            for i, af in enumerate(audio_files[:10]):
                print(f"    [{i:>2}] {af}")
            
            audio_idx = input("  Audio index (or path): ").strip()
            try:
                if audio_idx.isdigit():
                    audio_path = audio_files[int(audio_idx)]
                else:
                    audio_path = audio_idx
            except:
                print(f"  {Colors.RED}Invalid selection{Colors.RESET}")
                continue
            
            compare_checkpoints(str(checkpoint_dir), audio_path, device)
            
        elif choice == "4":
            audio_files = list_test_audio(limit=50)
            print(f"\n  {Colors.BOLD}Found {len(audio_files)} audio files:{Colors.RESET}")
            for i, af in enumerate(audio_files):
                print(f"    [{i:>2}] {af}")
        
        else:
            print(f"  {Colors.RED}Unknown option{Colors.RESET}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    parser = argparse.ArgumentParser(description="Analyze TTS autoencoder checkpoints")
    parser.add_argument("--checkpoint", "-c", type=str, help="Path to checkpoint")
    parser.add_argument("--audio", "-a", type=str, help="Path to test audio")
    parser.add_argument("--compare", type=str, help="Directory of checkpoints to compare")
    parser.add_argument("--output", "-o", type=str, default="analysis_output", help="Output directory")
    parser.add_argument("--device", "-d", type=str, default="cuda", help="Device (cuda/cpu)")
    parser.add_argument("--interactive", "-i", action="store_true", help="Interactive mode")
    
    args = parser.parse_args()
    
    device = args.device if torch.cuda.is_available() else "cpu"
    
    if args.interactive or (not args.checkpoint and not args.compare):
        interactive_menu(device)
    elif args.compare:
        if not args.audio:
            print("Error: --audio required for comparison")
            return
        compare_checkpoints(args.compare, args.audio, device)
    elif args.checkpoint:
        if not args.audio:
            print("Error: --audio required")
            return
        analyze_single(args.checkpoint, args.audio, args.output, device)


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: scripts/compare_checkpoints.py
–†–û–ó–ú–Ü–†: 7.82 KB
==================================================

#!/usr/bin/env python3
"""
–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–≤–æ—Ö checkpoint'—ñ–≤ –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä–∞.
–ì–µ–Ω–µ—Ä—É—î audio –¥–ª—è –æ–¥–Ω–∏—Ö —ñ —Ç–∏—Ö —Å–∞–º–∏—Ö —Ñ–∞–π–ª—ñ–≤ —ñ –ø–æ–∫–∞–∑—É—î –º–µ—Ç—Ä–∏–∫–∏.
"""

import torch
import torchaudio
import json
from pathlib import Path
import sys
import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.speech_autoencoder import SpeechAutoencoder

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model(checkpoint_path):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å –∑ checkpoint."""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    config = checkpoint.get("config", {})
    audio_cfg = config.get("audio", {})
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –∑ –¥–µ—Ñ–æ–ª—Ç–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (latent_dim=24)
    model = SpeechAutoencoder(
        sample_rate=audio_cfg.get("sample_rate", 44100),
        n_fft=audio_cfg.get("n_fft", 2048),
        hop_length=audio_cfg.get("hop_length", 512),
        n_mels=228,
        latent_dim=24,
        hidden_dim=512,
    ).to(device)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∞–≥–∏
    model.encoder.load_state_dict(checkpoint["encoder"])
    model.decoder.load_state_dict(checkpoint["decoder"])
    model.eval()
    
    iteration = checkpoint.get("iteration", "?")
    return model, iteration


def analyze_frequency(audio, sr=44100):
    """–ê–Ω–∞–ª—ñ–∑—É—î —á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ø–µ–∫—Ç—Ä."""
    spec_transform = torchaudio.transforms.Spectrogram(n_fft=2048, hop_length=512, power=2)
    spec = spec_transform(audio).squeeze().numpy()
    spec_db = 10 * np.log10(spec + 1e-10)
    
    freq_bins = np.fft.rfftfreq(2048, 1/sr)
    
    results = {}
    ranges = [
        ("low_0_500", 0, 500),
        ("mid_500_2000", 500, 2000),
        ("high_2000_5000", 2000, 5000),
        ("vhigh_5000_10000", 5000, 10000),
    ]
    
    for name, low, high in ranges:
        mask = (freq_bins >= low) & (freq_bins < high)
        results[name] = spec_db[mask].mean()
    
    return results


def compare_audio(original, recon1, recon2, sr=44100):
    """–ü–æ—Ä—ñ–≤–Ω—é—î –¥–≤–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–π–æ–≤–∞–Ω–∏—Ö –∞—É–¥—ñ–æ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª–æ–º."""
    min_len = min(original.shape[-1], recon1.shape[-1], recon2.shape[-1])
    original = original[..., :min_len]
    recon1 = recon1[..., :min_len]
    recon2 = recon2[..., :min_len]
    
    # L1 loss
    l1_1 = torch.nn.functional.l1_loss(recon1, original).item()
    l1_2 = torch.nn.functional.l1_loss(recon2, original).item()
    
    # Frequency analysis
    orig_freq = analyze_frequency(original.cpu())
    recon1_freq = analyze_frequency(recon1.cpu())
    recon2_freq = analyze_frequency(recon2.cpu())
    
    freq_diff1 = {k: recon1_freq[k] - orig_freq[k] for k in orig_freq}
    freq_diff2 = {k: recon2_freq[k] - orig_freq[k] for k in orig_freq}
    
    return {
        "l1_1": l1_1,
        "l1_2": l1_2,
        "freq_diff1": freq_diff1,
        "freq_diff2": freq_diff2,
    }


def main():
    # Checkpoint paths
    ckpt1_path = "checkpoints/autoencoder/checkpoint_80000.pt"
    ckpt2_path = "checkpoints/autoencoder/checkpoint_95000.pt"
    
    print("="*70)
    print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø CHECKPOINT'–Ü–í")
    print("="*70)
    
    # Load models
    print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è checkpoint 1: {ckpt1_path}")
    model1, iter1 = load_model(ckpt1_path)
    print(f"  ‚Üí Iteration: {iter1}")
    
    print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è checkpoint 2: {ckpt2_path}")
    model2, iter2 = load_model(ckpt2_path)
    print(f"  ‚Üí Iteration: {iter2}")
    
    # Test files
    manifest = Path("data/manifests/val.json")
    with open(manifest) as f:
        samples = json.load(f)[:5]
    
    output_dir = Path("test_outputs/compare_checkpoints")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    print("="*70)
    
    all_results = []
    
    for i, sample in enumerate(samples):
        audio_path = Path(sample["audio_path"])
        if not audio_path.exists():
            continue
        
        print(f"\n[{i+1}] {audio_path.name}")
        
        # Load audio
        audio, sr = torchaudio.load(str(audio_path))
        if audio.dim() == 2:
            audio = audio.mean(dim=0)
        if sr != 44100:
            audio = torchaudio.functional.resample(audio, sr, 44100)
        audio = audio[:44100*10].unsqueeze(0).to(device)
        
        # Reconstruct with both models
        with torch.no_grad():
            latent1 = model1.encode(audio)
            recon1 = model1.decode(latent1)
            
            latent2 = model2.encode(audio)
            recon2 = model2.decode(latent2)
        
        # Compare
        results = compare_audio(audio, recon1, recon2)
        all_results.append(results)
        
        print(f"    Audio L1 loss:")
        print(f"      Checkpoint {iter1}: {results['l1_1']:.4f}")
        print(f"      Checkpoint {iter2}: {results['l1_2']:.4f}")
        improvement = (results['l1_1'] - results['l1_2']) / results['l1_1'] * 100
        print(f"      {'üìà –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è' if improvement > 0 else 'üìâ –ü–æ–≥—ñ—Ä—à–µ–Ω–Ω—è'}: {abs(improvement):.1f}%")
        
        print(f"    –ß–∞—Å—Ç–æ—Ç–Ω–∏–π –±–∞–ª–∞–Ω—Å (—Ä—ñ–∑–Ω–∏—Ü—è –≤—ñ–¥ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É, dB):")
        print(f"      {'–î—ñ–∞–ø–∞–∑–æ–Ω':20} | {f'Ckpt {iter1}':>10} | {f'Ckpt {iter2}':>10} | –ö—Ä–∞—â–µ?")
        print(f"      {'-'*20}-+-{'-'*10}-+-{'-'*10}-+-------")
        
        for key in results['freq_diff1']:
            d1 = results['freq_diff1'][key]
            d2 = results['freq_diff2'][key]
            better = "‚úÖ" if abs(d2) < abs(d1) else "‚ùå" if abs(d2) > abs(d1) else "="
            name = key.replace("_", " ").replace("low", "–ù–∏–∑—å–∫—ñ").replace("mid", "–°–µ—Ä–µ–¥–Ω—ñ").replace("high", "–í–∏—Å–æ–∫—ñ").replace("vhigh", "–î—É–∂–µ –≤–∏—Å.")
            print(f"      {name:20} | {d1:>+10.2f} | {d2:>+10.2f} | {better}")
        
        # Save audio files
        min_len = min(audio.shape[-1], recon1.shape[-1], recon2.shape[-1])
        torchaudio.save(str(output_dir / f"{i+1}_original.wav"), audio[..., :min_len].cpu(), 44100)
        torchaudio.save(str(output_dir / f"{i+1}_ckpt{iter1}.wav"), recon1[..., :min_len].cpu(), 44100)
        torchaudio.save(str(output_dir / f"{i+1}_ckpt{iter2}.wav"), recon2[..., :min_len].cpu(), 44100)
    
    # Summary
    print("\n" + "="*70)
    print("–ó–ê–ì–ê–õ–¨–ù–ò–ô –ü–Ü–î–°–£–ú–û–ö")
    print("="*70)
    
    avg_l1_1 = np.mean([r['l1_1'] for r in all_results])
    avg_l1_2 = np.mean([r['l1_2'] for r in all_results])
    
    print(f"\n–°–µ—Ä–µ–¥–Ω—ñ–π Audio L1 loss:")
    print(f"  Checkpoint {iter1}: {avg_l1_1:.4f}")
    print(f"  Checkpoint {iter2}: {avg_l1_2:.4f}")
    
    improvement = (avg_l1_1 - avg_l1_2) / avg_l1_1 * 100
    if improvement > 0:
        print(f"  üìà –ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvement:.1f}%")
    else:
        print(f"  üìâ –ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è: {abs(improvement):.1f}%")
    
    # Average frequency differences
    print(f"\n–°–µ—Ä–µ–¥–Ω—è —á–∞—Å—Ç–æ—Ç–Ω–∞ —Ä—ñ–∑–Ω–∏—Ü—è –≤—ñ–¥ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É:")
    for key in all_results[0]['freq_diff1']:
        avg_d1 = np.mean([r['freq_diff1'][key] for r in all_results])
        avg_d2 = np.mean([r['freq_diff2'][key] for r in all_results])
        better = "‚úÖ" if abs(avg_d2) < abs(avg_d1) else "‚ùå"
        name = key.replace("_", " ")
        print(f"  {name:20}: {avg_d1:+.2f} ‚Üí {avg_d2:+.2f} dB  {better}")
    
    print(f"\n‚úì –ê—É–¥—ñ–æ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {output_dir}/")
    print("\n–ü–û–°–õ–£–•–ê–ô —Ñ–∞–π–ª–∏ —â–æ–± –æ—Ü—ñ–Ω–∏—Ç–∏ —Ä—ñ–∑–Ω–∏—Ü—é!")


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: scripts/debug_tts_quality.py
–†–û–ó–ú–Ü–†: 11.94 KB
==================================================

#!/usr/bin/env python3
"""Debug script to compare GT vs Generated latents from Stage 2 TTS."""

import torch
import torchaudio
import json
import sys
import os
import glob

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def find_tts_checkpoint():
    """Find the latest TTS checkpoint in various possible locations."""
    search_paths = [
        'outputs/text_to_latent/checkpoints/*.pt',  # Found here!
        'checkpoints/tts/*.pt',
        'checkpoints/text_to_latent/*.pt',
        'checkpoints/stage2/*.pt',
        'outputs/*.pt',
        'outputs/tts/*.pt',
        'outputs/checkpoints/*.pt',
        '*.pt',
    ]
    
    all_checkpoints = []
    for pattern in search_paths:
        all_checkpoints.extend(glob.glob(pattern))
    
    # Filter for TTS-related checkpoints
    tts_ckpts = [c for c in all_checkpoints if 'autoencoder' not in c.lower()]
    
    if not tts_ckpts:
        # List all directories to help find
        print("No TTS checkpoints found. Searching all directories...")
        for root, dirs, files in os.walk('.'):
            pt_files = [f for f in files if f.endswith('.pt')]
            if pt_files and 'autoencoder' not in root:
                print(f"  {root}: {pt_files[:5]}...")
        return None
    
    # Sort by modification time, get latest
    tts_ckpts.sort(key=os.path.getmtime, reverse=True)
    return tts_ckpts[0]

def main():
    device = 'cuda'
    
    # ========== 1. Check checkpoint structure ==========
    print("=" * 60)
    print("STEP 1: Finding and checking checkpoints")
    print("=" * 60)
    
    enc_ckpt = torch.load('checkpoints/autoencoder/checkpoint_150000.pt', map_location=device)
    print(f"Autoencoder checkpoint keys: {list(enc_ckpt.keys())}")
    
    # Find TTS checkpoint
    tts_path = find_tts_checkpoint()
    if tts_path is None:
        print("\nERROR: Cannot find TTS checkpoint!")
        print("Run: find . -name '*.pt' -type f | head -20")
        sys.exit(1)
    
    print(f"\nFound TTS checkpoint: {tts_path}")
    tts_ckpt = torch.load(tts_path, map_location=device)
    print(f"TTS checkpoint keys: {list(tts_ckpt.keys())}")
    
    # Determine the correct key for model weights
    # Autoencoder has 'encoder' and 'decoder' keys directly
    if 'encoder' in enc_ckpt:
        enc_state = enc_ckpt['encoder']
        dec_state = enc_ckpt['decoder']
    elif 'model_state_dict' in enc_ckpt:
        enc_state = {k.replace('encoder.', ''): v for k, v in enc_ckpt['model_state_dict'].items() if k.startswith('encoder.')}
        dec_state = {k.replace('decoder.', ''): v for k, v in enc_ckpt['model_state_dict'].items() if k.startswith('decoder.')}
    else:
        enc_state = enc_ckpt
        dec_state = enc_ckpt
    
    # TTS has 'model' key directly
    if 'model' in tts_ckpt:
        tts_state = tts_ckpt['model']
    elif 'model_state_dict' in tts_ckpt:
        tts_state = tts_ckpt['model_state_dict']
    else:
        tts_state = tts_ckpt
    
    # Remove 'module.' prefix from DDP checkpoints
    def strip_module_prefix(state_dict):
        return {k.replace('module.', ''): v for k, v in state_dict.items()}
    
    enc_state = strip_module_prefix(enc_state)
    dec_state = strip_module_prefix(dec_state)
    tts_state = strip_module_prefix(tts_state)
    
    print(f"\nEncoder state dict - first 5 keys: {list(enc_state.keys())[:5]}")
    print(f"TTS state dict - first 5 keys: {list(tts_state.keys())[:5]}")
    
    # ========== 2. Load models ==========
    print("\n" + "=" * 60)
    print("STEP 2: Loading models")
    print("=" * 60)
    
    from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder
    from supertonic.models.text_to_latent import TextToLatent
    from supertonic.data.tokenizer import CharacterTokenizer
    
    # Encoder
    enc = LatentEncoder(
        input_dim=100, hidden_dim=512, output_dim=24,
        num_blocks=10, kernel_size=7, intermediate_mult=4
    )
    enc.load_state_dict(enc_state)
    enc.to(device).eval()
    print("‚úì Encoder loaded")
    
    # Decoder
    dec = LatentDecoder(
        input_dim=24, hidden_dim=512, num_blocks=10, kernel_size=7,
        intermediate_mult=4, dilations=[1,2,4,1,2,4,1,1,1,1],
        n_fft=1024, hop_length=256, causal=True,
        use_hifigan=True, upsample_rates=[8,8,2,2],
        upsample_kernel_sizes=[16,16,4,4], upsample_initial_channel=512,
        resblock_kernel_sizes=[3,7,11],
        resblock_dilation_sizes=[[1,3,5],[1,3,5],[1,3,5]]
    )
    dec.load_state_dict(dec_state)
    dec.to(device).eval()
    print("‚úì Decoder loaded")
    
    # TTS - get vocab size from checkpoint
    vocab_key = 'text_encoder.token_embedding.weight'
    if vocab_key in tts_state:
        vocab_size = tts_state[vocab_key].shape[0]
    else:
        vocab_size = 104  # fallback
    print(f"Vocab size from checkpoint: {vocab_size}")
    
    tts = TextToLatent(
        latent_dim=144,
        vocab_size=vocab_size,
        text_embed_dim=128,
        text_hidden_dim=512,
        ref_hidden_dim=128,
        vf_hidden_dim=512,
        num_ref_vectors=50,
        sigma_min=1e-8,
        p_uncond=0.05,
        cfg_scale=3.0,
        gamma=10.0
    )
    tts.load_state_dict(tts_state)
    tts.to(device).eval()
    print("‚úì TTS loaded")
    
    # Tokenizer - MUST match training config!
    # Training used languages=["uk"] which gives vocab_size=104
    tokenizer = CharacterTokenizer(languages=["uk"])
    print(f"‚úì Tokenizer loaded, vocab size: {tokenizer.vocab_size}")
    
    # ========== 3. Load test sample ==========
    print("\n" + "=" * 60)
    print("STEP 3: Loading test sample")
    print("=" * 60)
    
    with open('data/manifests_stage2/val.json') as f:
        content = f.read()
    
    # Try to parse - might be array or line-delimited JSON
    try:
        samples = json.loads(content)
        if not isinstance(samples, list):
            samples = [samples]
    except json.JSONDecodeError:
        # Line-delimited JSON, skip empty lines
        samples = []
        for line in content.strip().split('\n'):
            line = line.strip()
            if line:
                try:
                    samples.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    print(f"Loaded {len(samples)} samples from manifest")
    
    # Find sample with text
    sample = None
    for s in samples:
        text = s.get('text', '')
        if len(text) > 10:
            sample = s
            if 'opentts' in s.get('audio_path', ''):
                break  # Prefer OpenTTS
    
    if sample is None:
        print("ERROR: No sample with text found!")
        sys.exit(1)
    
    print(f"Audio: {sample['audio_path']}")
    print(f"Text: {sample['text']}")
    
    # ========== 4. Process audio ==========
    print("\n" + "=" * 60)
    print("STEP 4: Processing audio")
    print("=" * 60)
    
    mel_transform = torchaudio.transforms.MelSpectrogram(
        sample_rate=22050, n_fft=1024, hop_length=256,
        win_length=1024, n_mels=100, f_min=20.0, f_max=11025.0, power=1.0
    ).to(device)
    
    # Handle relative paths - prepend data/ if needed
    audio_path = sample['audio_path']
    if not os.path.exists(audio_path):
        if os.path.exists(f"data/{audio_path}"):
            audio_path = f"data/{audio_path}"
        elif os.path.exists(audio_path.replace('audio/', 'data/audio/')):
            audio_path = audio_path.replace('audio/', 'data/audio/')
    
    print(f"Loading: {audio_path}")
    audio, sr = torchaudio.load(audio_path)
    if sr != 22050:
        audio = torchaudio.functional.resample(audio, sr, 22050)
    audio = audio.mean(0, keepdim=True).to(device)  # [1, samples]
    print(f"Audio shape: {audio.shape}, duration: {audio.shape[1]/22050:.2f}s")
    
    with torch.no_grad():
        mel = mel_transform(audio)  # [1, n_mels, T]
        print(f"Mel shape: {mel.shape}")
        
        # Encode to latent
        latent = enc(mel)  # [1, 24, T]
        print(f"Latent shape: {latent.shape}")
        
        # Compress 6x
        T = latent.shape[-1]
        T_pad = (6 - T % 6) % 6
        if T_pad > 0:
            latent_pad = torch.nn.functional.pad(latent, (0, T_pad))
        else:
            latent_pad = latent
        compressed = latent_pad.reshape(1, 24, -1, 6).permute(0, 1, 3, 2).reshape(1, 144, -1)
        print(f"Compressed GT shape: {compressed.shape}")
        
        # ========== 5. Compare latents ==========
        print("\n" + "=" * 60)
        print("STEP 5: Comparing GT vs Generated latents")
        print("=" * 60)
        
        print(f"GT compressed - mean: {compressed.mean():.4f}, std: {compressed.std():.4f}")
        print(f"               min: {compressed.min():.4f}, max: {compressed.max():.4f}")
        
        # Tokenize
        tokens = tokenizer.encode(sample['text'])
        print(f"\nTokens type: {type(tokens)}, value: {tokens[:20] if hasattr(tokens, '__len__') else tokens}...")
        
        # Convert to tensor properly
        if isinstance(tokens, torch.Tensor):
            tokens_t = tokens.unsqueeze(0).to(device) if tokens.dim() == 1 else tokens.to(device)
        else:
            tokens_t = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)
        
        print(f"Tokens tensor shape: {tokens_t.shape}")
        
        # Generate with TTS
        # num_frames should match compressed GT frames
        num_frames = compressed.shape[-1]
        gen_compressed = tts.generate(
            text=tokens_t,
            ref_latent=compressed,
            num_frames=num_frames,
            nfe=32,
            cfg_scale=3.0
        )
        print(f"Generated shape: {gen_compressed.shape}")
        
        print(f"\nGenerated compressed stats:")
        print(f"  mean: {gen_compressed.mean():.4f}")
        print(f"  std:  {gen_compressed.std():.4f}")
        print(f"  min:  {gen_compressed.min():.4f}")
        print(f"  max:  {gen_compressed.max():.4f}")
        
        # Difference
        diff = (compressed - gen_compressed).abs().mean()
        print(f"\n>>> L1 difference: {diff:.4f}")
        
        # Correlation
        gt_flat = compressed.flatten()
        gen_flat = gen_compressed.flatten()
        corr = torch.corrcoef(torch.stack([gt_flat, gen_flat]))[0, 1]
        print(f">>> Correlation: {corr:.4f}")
        
        # ========== 6. Decode and save ==========
        print("\n" + "=" * 60)
        print("STEP 6: Decoding and saving audio")
        print("=" * 60)
        
        # Decode GT
        gt_audio = dec(latent)
        torchaudio.save('debug_gt.wav', gt_audio.cpu(), 22050)
        print("‚úì Saved debug_gt.wav (ground truth reconstruction)")
        
        # Decode generated
        T_gen = gen_compressed.shape[-1]
        gen_latent = gen_compressed.reshape(1, 24, 6, T_gen).permute(0, 1, 3, 2).reshape(1, 24, T_gen * 6)
        gen_audio = dec(gen_latent)
        torchaudio.save('debug_gen.wav', gen_audio.cpu(), 22050)
        print("‚úì Saved debug_gen.wav (TTS generated)")
        
        print("\n" + "=" * 60)
        print("DONE! Compare the two files:")
        print("  - debug_gt.wav  = original audio through autoencoder")
        print("  - debug_gen.wav = TTS generated from text + reference")
        print("=" * 60)
        
        # Interpretation
        print("\n>>> INTERPRETATION:")
        if diff < 0.5 and corr > 0.8:
            print("Model is learning well! Output should be similar to GT.")
        elif diff < 1.0 and corr > 0.5:
            print("Model is learning, but needs more training.")
        elif corr > 0.3:
            print("Model is starting to learn, continue training.")
        else:
            print("Model output is random - check training loop!")

if __name__ == '__main__':
    main()



==================================================
–§–ê–ô–õ: scripts/diagnose_reconstruction.py
–†–û–ó–ú–Ü–†: 18.88 KB
==================================================

#!/usr/bin/env python3
"""
–î–ï–¢–ê–õ–¨–ù–ê –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –†–ï–ö–û–ù–°–¢–†–£–ö–¶–Ü–á AUTOENCODER

–ê–Ω–∞–ª—ñ–∑—É—î –∫–æ–∂–µ–Ω –µ—Ç–∞–ø pipeline:
1. –í—Ö—ñ–¥–Ω–µ –∞—É–¥—ñ–æ
2. Mel spectrogram
3. Encoder output (latent)
4. Decoder intermediate states
5. iSTFT magnitude/phase
6. –í–∏—Ö—ñ–¥–Ω–µ –∞—É–¥—ñ–æ (–¥–æ —ñ –ø—ñ—Å–ª—è tanh)

–ó–∞–ø—É—Å–∫:
    python scripts/diagnose_reconstruction.py --checkpoint checkpoints/autoencoder/checkpoint_2500.pt --audio data/opentts/audio/lada/lada_000001.wav
"""

import sys
import argparse
from pathlib import Path
import torch
import torch.nn.functional as F
import torchaudio
import numpy as np
import matplotlib.pyplot as plt

sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder, ISTFTHead
from supertonic.data.preprocessing import AudioProcessor


def strip_ddp_prefix(state_dict):
    """Remove 'module.' prefix from DDP state dict keys."""
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith("module."):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v
    return new_state_dict


def load_checkpoint_for_diagnosis(checkpoint_path: str, device: str = "cuda"):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —á–µ–∫–ø–æ—ñ–Ω—Ç —ñ —Å—Ç–≤–æ—Ä—é—î –º–æ–¥–µ–ª—ñ."""
    print(f"\n{'='*70}")
    print(f"–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ß–ï–ö–ü–û–Ü–ù–¢–ê")
    print(f"{'='*70}")
    
    checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
    config = checkpoint.get("config", {})
    
    # Strip DDP prefix from state dicts
    encoder_state = strip_ddp_prefix(checkpoint["encoder"])
    decoder_state = strip_ddp_prefix(checkpoint["decoder"])
    
    # Detect params from weights
    n_mels = encoder_state["input_conv.weight"].shape[1]
    
    # Check if using WaveNeXtHead or ISTFTHead
    if "head.conv.weight" in decoder_state:
        # WaveNeXtHead - detect hop_length from head.fc.weight
        hop_length = decoder_state["head.fc.weight"].shape[0]
        print(f"  ‚úì Using WaveNeXtHead (hop_length={hop_length})")
        use_wavnext = True
    elif "istft_head.window" in decoder_state:
        # Old ISTFTHead
        n_fft = decoder_state["istft_head.window"].shape[0]
        hop_length = n_fft // 4
        print(f"  ‚ö†Ô∏è  Using deprecated ISTFTHead (n_fft={n_fft})")
        use_wavnext = False
    else:
        # Fallback - guess from config or defaults
        hop_length = 256
        print(f"  ‚ö†Ô∏è  Could not detect head type, assuming hop_length={hop_length}")
        use_wavnext = True
    
    # Determine n_fft and sample_rate
    if hop_length == 256:
        n_fft = 1024
        sample_rate = 22050
    else:
        n_fft = 2048
        sample_rate = 44100
    
    print(f"  n_fft: {n_fft}")
    print(f"  hop_length: {hop_length}")
    print(f"  n_mels: {n_mels}")
    print(f"  sample_rate: {sample_rate}")
    
    # Create models
    encoder = LatentEncoder(
        input_dim=n_mels,
        hidden_dim=512,
        output_dim=24,
        num_blocks=10,
        kernel_size=7,
    ).to(device)
    
    decoder = LatentDecoder(
        input_dim=24,
        hidden_dim=512,
        num_blocks=10,
        kernel_size=7,
        dilations=[1, 2, 4, 1, 2, 4, 1, 1, 1, 1],
        n_fft=n_fft,
        hop_length=hop_length,
        causal=True,
    ).to(device)
    
    encoder.load_state_dict(encoder_state)
    decoder.load_state_dict(decoder_state)
    
    encoder.eval()
    decoder.eval()
    
    audio_processor = AudioProcessor(
        sample_rate=sample_rate,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels
    )
    
    return encoder, decoder, audio_processor, {
        "n_fft": n_fft,
        "hop_length": hop_length,
        "n_mels": n_mels,
        "sample_rate": sample_rate
    }


def analyze_tensor(name: str, tensor: torch.Tensor, detailed: bool = True):
    """–ê–Ω–∞–ª—ñ–∑—É—î —Ç–µ–Ω–∑–æ—Ä —ñ –≤–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
    t = tensor.detach().float()
    
    stats = {
        "shape": list(t.shape),
        "dtype": str(t.dtype),
        "min": t.min().item(),
        "max": t.max().item(),
        "mean": t.mean().item(),
        "std": t.std().item(),
        "abs_max": t.abs().max().item(),
        "num_zeros": (t == 0).sum().item(),
        "num_nan": torch.isnan(t).sum().item(),
        "num_inf": torch.isinf(t).sum().item(),
    }
    
    # Check for clipping (values at boundaries)
    if t.min() >= -1 and t.max() <= 1:
        num_clipped_low = (t <= -0.99).sum().item()
        num_clipped_high = (t >= 0.99).sum().item()
        stats["clipped_low"] = num_clipped_low
        stats["clipped_high"] = num_clipped_high
        stats["clipped_percent"] = 100 * (num_clipped_low + num_clipped_high) / t.numel()
    
    print(f"\n  üìä {name}:")
    print(f"     Shape: {stats['shape']}")
    print(f"     Range: [{stats['min']:.6f}, {stats['max']:.6f}]")
    print(f"     Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}")
    print(f"     Abs Max: {stats['abs_max']:.6f}")
    
    if stats['num_nan'] > 0:
        print(f"     ‚ö†Ô∏è  NaN values: {stats['num_nan']}")
    if stats['num_inf'] > 0:
        print(f"     ‚ö†Ô∏è  Inf values: {stats['num_inf']}")
    if "clipped_percent" in stats and stats["clipped_percent"] > 1:
        print(f"     ‚ö†Ô∏è  Clipped: {stats['clipped_percent']:.2f}% ({stats['clipped_low']} low, {stats['clipped_high']} high)")
    
    if detailed:
        # Percentiles
        percentiles = [1, 5, 25, 50, 75, 95, 99]
        pct_values = [torch.quantile(t.flatten().float(), p/100).item() for p in percentiles]
        print(f"     Percentiles: {dict(zip(percentiles, [f'{v:.4f}' for v in pct_values]))}")
    
    return stats


def diagnose_decoder_internals(decoder, latent, device):
    """–î—ñ–∞–≥–Ω–æ—Å—Ç—É—î –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ —Å—Ç–∞–Ω–∏ decoder –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º."""
    print(f"\n{'='*70}")
    print("–î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê DECODER (–∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º)")
    print(f"{'='*70}")
    
    with torch.no_grad():
        # Step 1: Input projection
        x = decoder.input_conv(latent)
        analyze_tensor("–ü–æ—Å–ª–µ input_conv", x)
        
        x = decoder.input_norm(x)
        analyze_tensor("–ü–æ—Å–ª–µ input_norm (BatchNorm)", x)
        
        # Step 2: ConvNeXt blocks
        x = decoder.convnext(x)
        analyze_tensor("–ü–æ—Å–ª–µ ConvNeXt stack", x)
        
        # Step 3: Check which head type we have
        if hasattr(decoder, 'head'):
            # WaveNeXt head (new architecture) - expects [B, C, T]
            print(f"\n  --- WaveNeXt Head Details ---")
            
            head = decoder.head
            
            # BatchNorm (expects [B, C, T])
            x_norm = head.norm(x)
            analyze_tensor("After head.norm (BatchNorm)", x_norm)
            
            # Conv1d ‚Üí head_dim
            x_conv = head.conv(x_norm)
            analyze_tensor("After head.conv (‚Üí head_dim)", x_conv)
            
            # PReLU
            x_act = head.act(x_conv)
            analyze_tensor("After PReLU", x_act)
            
            # Transpose for Linear: [B, head_dim, T] ‚Üí [B, T, head_dim]
            x_t = x_act.transpose(1, 2)
            
            # Linear ‚Üí hop_length
            x_fc = head.fc(x_t)
            analyze_tensor("After head.fc (waveform frames)", x_fc)
            
            # Reshape to waveform
            batch_size, num_frames, _ = x_fc.shape
            audio_raw = x_fc.reshape(batch_size, num_frames * head.hop_length)
            analyze_tensor("Audio BEFORE tanh", audio_raw)
            
            audio_final = torch.tanh(audio_raw)
            analyze_tensor("Audio AFTER tanh", audio_final)
            
            return audio_raw, audio_final, None, None
            
        elif hasattr(decoder, 'istft_head'):
            # Legacy iSTFT head
            # Need to transpose for iSTFT head which expects [B, T, hidden]
            x = x.transpose(1, 2)
            print(f"\n  --- iSTFT Head Details (LEGACY) ---")
            
            istft = decoder.istft_head
            
            # Magnitude projection (before exp)
            mag_raw = istft.mag_proj(x)
            analyze_tensor("Magnitude (raw, before exp)", mag_raw)
            
            # Magnitude after exp
            mag = mag_raw.exp()
            analyze_tensor("Magnitude (after exp)", mag)
            
            # Phase projection
            phase_raw = istft.phase_proj(x)
            analyze_tensor("Phase (raw)", phase_raw)
            
            phase = torch.tanh(phase_raw) * 3.14159
            analyze_tensor("Phase (after tanh * œÄ)", phase)
            
            # Complex spectrum
            real = mag * torch.cos(phase)
            imag = mag * torch.sin(phase)
            analyze_tensor("STFT Real part", real)
            analyze_tensor("STFT Imag part", imag)
            
            spec = torch.complex(real, imag)
            spec = spec.transpose(1, 2)
            
            # iSTFT
            audio_raw = torch.istft(
                spec,
                n_fft=istft.n_fft,
                hop_length=istft.hop_length,
                win_length=istft.n_fft,
                window=istft.window.to(device),
                center=True,
                normalized=False,
                onesided=True,
                length=None,
                return_complex=False
            )
            analyze_tensor("Audio BEFORE tanh", audio_raw)
            
            # After tanh
            audio_final = torch.tanh(audio_raw)
            analyze_tensor("Audio AFTER tanh", audio_final)
            
            return audio_raw, audio_final, mag, phase
        else:
            print("  ‚ö†Ô∏è  Unknown head type!")
            return None, None, None, None


def diagnose_full_pipeline(checkpoint_path: str, audio_path: str, output_dir: str = "diagnostic_outputs"):
    """–ü–æ–≤–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ pipeline."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nüîß Device: {device}")
    
    # Create output dir
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Load checkpoint
    encoder, decoder, audio_processor, params = load_checkpoint_for_diagnosis(checkpoint_path, device)
    
    # Load audio
    print(f"\n{'='*70}")
    print("–í–•–Ü–î–ù–ï –ê–£–î–Ü–û")
    print(f"{'='*70}")
    
    audio, sr = torchaudio.load(audio_path)
    print(f"  –§–∞–π–ª: {audio_path}")
    print(f"  Original SR: {sr}")
    
    if audio.shape[0] > 1:
        audio = audio.mean(dim=0, keepdim=True)
    
    if sr != params["sample_rate"]:
        resampler = torchaudio.transforms.Resample(sr, params["sample_rate"])
        audio = resampler(audio)
    
    audio = audio.squeeze(0)
    analyze_tensor("–í—Ö—ñ–¥–Ω–µ –∞—É–¥—ñ–æ", audio)
    
    # Compute mel
    print(f"\n{'='*70}")
    print("MEL SPECTROGRAM")
    print(f"{'='*70}")
    
    mel = audio_processor.compute_mel(audio)
    analyze_tensor("Mel spectrogram", mel)
    
    # Prepare for model
    audio_batch = audio.unsqueeze(0).to(device)
    mel_batch = mel.unsqueeze(0).to(device)
    
    # Encoder
    print(f"\n{'='*70}")
    print("ENCODER")
    print(f"{'='*70}")
    
    with torch.no_grad():
        latent = encoder(mel_batch)
    analyze_tensor("Latent vectors", latent)
    
    # Decoder with detailed diagnostics
    audio_raw, audio_final, mag, phase = diagnose_decoder_internals(decoder, latent, device)
    
    # Compare with original
    print(f"\n{'='*70}")
    print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ó –û–†–ò–ì–Ü–ù–ê–õ–û–ú")
    print(f"{'='*70}")
    
    min_len = min(audio.shape[-1], audio_final.shape[-1])
    orig = audio[:min_len]
    recon = audio_final[0, :min_len].cpu()
    
    l1 = F.l1_loss(recon, orig).item()
    mse = F.mse_loss(recon, orig).item()
    
    # SNR
    signal_power = (orig ** 2).mean()
    noise_power = ((orig - recon) ** 2).mean()
    snr = 10 * torch.log10(signal_power / (noise_power + 1e-8)).item()
    
    print(f"\n  L1 Loss: {l1:.6f}")
    print(f"  MSE: {mse:.6f}")
    print(f"  SNR: {snr:.2f} dB")
    
    # Amplitude comparison
    print(f"\n  Amplitude comparison:")
    print(f"    Original - max: {orig.abs().max():.4f}, mean: {orig.abs().mean():.4f}")
    print(f"    Reconstructed - max: {recon.abs().max():.4f}, mean: {recon.abs().mean():.4f}")
    print(f"    Ratio (recon/orig): {recon.abs().mean() / (orig.abs().mean() + 1e-8):.4f}")
    
    # Save diagnostic plots
    print(f"\n{'='*70}")
    print("–ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –î–Ü–ê–ì–ù–û–°–¢–ò–ß–ù–ò–• –ì–†–ê–§–Ü–ö–Ü–í")
    print(f"{'='*70}")
    
    fig, axes = plt.subplots(4, 2, figsize=(16, 20))
    
    # 1. Waveforms
    axes[0, 0].plot(orig.numpy(), alpha=0.7, linewidth=0.5)
    axes[0, 0].set_title("Original Waveform")
    axes[0, 0].set_ylim(-1.1, 1.1)
    
    axes[0, 1].plot(recon.numpy(), alpha=0.7, linewidth=0.5, color='orange')
    axes[0, 1].set_title("Reconstructed Waveform")
    axes[0, 1].set_ylim(-1.1, 1.1)
    
    # 2. Mel spectrograms
    axes[1, 0].imshow(mel.numpy(), aspect='auto', origin='lower')
    axes[1, 0].set_title("Original Mel")
    axes[1, 0].set_ylabel("Mel bin")
    
    recon_mel = audio_processor.compute_mel(recon)
    axes[1, 1].imshow(recon_mel.numpy(), aspect='auto', origin='lower')
    axes[1, 1].set_title("Reconstructed Mel")
    
    # 3. Latent space
    latent_np = latent[0].cpu().numpy()
    axes[2, 0].imshow(latent_np, aspect='auto', origin='lower', cmap='RdBu')
    axes[2, 0].set_title(f"Latent vectors (range: [{latent_np.min():.2f}, {latent_np.max():.2f}])")
    axes[2, 0].set_ylabel("Latent dim")
    axes[2, 0].colorbar = plt.colorbar(axes[2, 0].images[0], ax=axes[2, 0])
    
    # 4. Magnitude spectrum (log scale) - only if available (not for WaveNeXtHead)
    if mag is not None:
        mag_np = mag[0].cpu().numpy()
        axes[2, 1].imshow(np.log10(mag_np.T + 1e-8), aspect='auto', origin='lower', cmap='magma')
        axes[2, 1].set_title(f"Magnitude (log10, range: [{mag_np.min():.2f}, {mag_np.max():.2f}])")
        axes[2, 1].set_ylabel("Freq bin")
    else:
        # WaveNeXtHead doesn't have magnitude - show waveform comparison instead
        orig_np = audio.cpu().numpy()
        recon_np = recon.cpu().numpy()
        t = np.arange(min(len(orig_np), 5000))
        axes[2, 1].plot(t, orig_np[:len(t)], alpha=0.7, linewidth=0.5, label='Original')
        axes[2, 1].plot(t, recon_np[:len(t)], alpha=0.7, linewidth=0.5, label='Reconstructed')
        axes[2, 1].set_title("Waveform comparison (first 5k samples)")
        axes[2, 1].legend(loc='upper right')
    
    # 5. Audio before tanh
    audio_raw_np = audio_raw[0].cpu().numpy()
    axes[3, 0].plot(audio_raw_np[:10000], alpha=0.7, linewidth=0.5, color='red')
    axes[3, 0].set_title(f"Audio BEFORE tanh (first 10k samples, range: [{audio_raw_np.min():.2f}, {audio_raw_np.max():.2f}])")
    axes[3, 0].axhline(y=1, color='black', linestyle='--', alpha=0.5)
    axes[3, 0].axhline(y=-1, color='black', linestyle='--', alpha=0.5)
    
    # 6. Histogram of raw audio values
    axes[3, 1].hist(audio_raw_np.flatten(), bins=100, alpha=0.7, color='red')
    axes[3, 1].axvline(x=1, color='black', linestyle='--', alpha=0.5)
    axes[3, 1].axvline(x=-1, color='black', linestyle='--', alpha=0.5)
    axes[3, 1].set_title(f"Histogram of audio BEFORE tanh")
    axes[3, 1].set_xlabel("Amplitude")
    axes[3, 1].set_ylabel("Count")
    
    plt.tight_layout()
    plot_path = output_path / "diagnostic_plots.png"
    plt.savefig(plot_path, dpi=150)
    print(f"  üìä Saved: {plot_path}")
    
    # Save audio files
    audio_raw_path = output_path / "audio_before_tanh.wav"
    audio_final_path = output_path / "audio_after_tanh.wav"
    original_path = output_path / "original.wav"
    
    # Normalize raw audio for listening (it might be very loud)
    audio_raw_cpu = audio_raw[0].cpu()
    audio_raw_normalized = audio_raw_cpu / (audio_raw_cpu.abs().max() + 1e-8)
    
    torchaudio.save(str(audio_raw_path), audio_raw_normalized.unsqueeze(0), params["sample_rate"])
    torchaudio.save(str(audio_final_path), audio_final[0].cpu().unsqueeze(0), params["sample_rate"])
    torchaudio.save(str(original_path), orig.unsqueeze(0), params["sample_rate"])
    
    print(f"  üîä Saved: {original_path}")
    print(f"  üîä Saved: {audio_raw_path} (normalized)")
    print(f"  üîä Saved: {audio_final_path}")
    
    # Summary
    print(f"\n{'='*70}")
    print("üìã SUMMARY / –í–ò–°–ù–û–í–ö–ò")
    print(f"{'='*70}")
    
    # Detect issues
    issues = []
    
    if audio_raw.abs().max() > 10:
        issues.append(f"‚ùå Audio before tanh has extreme values: max={audio_raw.abs().max():.2f}")
    
    if mag.max() > 100:
        issues.append(f"‚ùå Magnitude spectrum too large: max={mag.max():.2f}")
    
    if latent.abs().max() > 10:
        issues.append(f"‚ö†Ô∏è  Latent values might be too large: max={latent.abs().max():.2f}")
    
    clipped_percent = ((audio_final.abs() > 0.99).sum() / audio_final.numel() * 100).item()
    if clipped_percent > 5:
        issues.append(f"‚ùå {clipped_percent:.1f}% of output is clipped (saturated at ¬±1)")
    
    if snr < 0:
        issues.append(f"‚ùå Negative SNR ({snr:.1f} dB) - reconstruction is mostly noise")
    
    if len(issues) == 0:
        print("  ‚úÖ No major issues detected")
    else:
        print("  –í–∏—è–≤–ª–µ–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏:")
        for issue in issues:
            print(f"    {issue}")
    
    print(f"\n  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:")
    if mag.max() > 100:
        print("    - Magnitude –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π. –°–ø—Ä–æ–±—É–π clamp exp() output")
        print("    - –ê–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π log1p –∑–∞–º—ñ—Å—Ç—å exp")
    if audio_raw.abs().max() > 10:
        print("    - iSTFT –≥–µ–Ω–µ—Ä—É—î –∑–∞–Ω–∞–¥—Ç–æ –≥—É—á–Ω–∏–π —Å–∏–≥–Ω–∞–ª")
        print("    - –ú–æ–∂–ª–∏–≤–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è magnitude")
    if clipped_percent > 5:
        print("    - –ë–∞–≥–∞—Ç–æ –∫–ª—ñ–ø—ñ–Ω–≥—É. –ú–æ–¥–µ–ª—å —â–µ –≤—á–∏—Ç—å—Å—è –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ –∞–º–ø–ª—ñ—Ç—É–¥—É")
        print("    - –ó–∞—á–µ–∫–∞–π –±—ñ–ª—å—à–µ –∫—Ä–æ–∫—ñ–≤ –∞–±–æ –∑–±—ñ–ª—å—à waveform loss weight")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Diagnose autoencoder reconstruction")
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to checkpoint")
    parser.add_argument("--audio", type=str, required=True, help="Path to test audio")
    parser.add_argument("--output", type=str, default="diagnostic_outputs", help="Output directory")
    
    args = parser.parse_args()
    
    diagnose_full_pipeline(args.checkpoint, args.audio, args.output)



==================================================
–§–ê–ô–õ: scripts/fix_stage2_manifests.py
–†–û–ó–ú–Ü–†: 2.97 KB
==================================================

#!/usr/bin/env python3
"""
Fix Stage 2 manifests - keep ONLY Ukrainian (OpenTTS) data.
LibriTTS with <unk> tokens is poisoning the training.
"""

import json
import os
from pathlib import Path

def fix_manifests():
    manifests_dir = Path("data/manifests_stage2")
    
    for manifest_name in ["train.json", "val.json"]:
        manifest_path = manifests_dir / manifest_name
        backup_path = manifests_dir / f"{manifest_name}.backup_with_libritts"
        
        print(f"\n{'='*60}")
        print(f"Processing {manifest_name}")
        print(f"{'='*60}")
        
        # Read manifest
        with open(manifest_path) as f:
            content = f.read()
        
        # Parse
        try:
            samples = json.loads(content)
        except json.JSONDecodeError:
            samples = []
            for line in content.strip().split('\n'):
                line = line.strip()
                if line:
                    try:
                        samples.append(json.loads(line))
                    except:
                        continue
        
        print(f"Total samples: {len(samples)}")
        
        # Count by source
        sources = {}
        for s in samples:
            path = s.get('audio_path', '')
            if 'opentts' in path:
                src = 'opentts'
            elif 'libritts' in path:
                src = 'libritts'
            elif 'vctk' in path:
                src = 'vctk'
            else:
                src = 'unknown'
            sources[src] = sources.get(src, 0) + 1
        
        print(f"Sources: {sources}")
        
        # Backup original
        os.rename(manifest_path, backup_path)
        print(f"Backed up to {backup_path}")
        
        # Filter to OpenTTS only
        opentts_samples = [s for s in samples if 'opentts' in s.get('audio_path', '')]
        
        # Also filter for samples with text
        opentts_with_text = [s for s in opentts_samples if len(s.get('text', '')) > 5]
        
        print(f"OpenTTS samples: {len(opentts_samples)}")
        print(f"OpenTTS with text (>5 chars): {len(opentts_with_text)}")
        
        # Write new manifest (JSON Lines format)
        with open(manifest_path, 'w', encoding='utf-8') as f:
            for s in opentts_with_text:
                f.write(json.dumps(s, ensure_ascii=False) + '\n')
        
        print(f"‚úì Saved {len(opentts_with_text)} samples to {manifest_path}")
    
    print("\n" + "="*60)
    print("DONE! Now restart Stage 2 training FROM SCRATCH:")
    print("="*60)
    print("""
1. Kill current training:
   pkill -f train_text_to_latent

2. Remove old checkpoints:
   rm -rf outputs/text_to_latent/

3. Restart training:
   torchrun --nproc_per_node=4 train_text_to_latent.py \\
       --config config/22khz_optimal.yaml \\
       --autoencoder_checkpoint checkpoints/autoencoder/checkpoint_150000.pt
""")

if __name__ == '__main__':
    fix_manifests()



==================================================
–§–ê–ô–õ: scripts/prepare_data_stage2.py
–†–û–ó–ú–Ü–†: 31.54 KB
==================================================

#!/usr/bin/env python3
"""
SUPERTONIC V2 TTS - DATA PREPARATION FOR STAGE 2 (Text-to-Latent)
===================================================================
–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î multi-speaker –¥–∞—Ç–∞—Å–µ—Ç–∏ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è voice cloning.

–ú–ï–¢–ê: –ú–∞–∫—Å–∏–º—É–º —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤/—Ç–µ–º–±—Ä—ñ–≤ –¥–ª—è zero-shot voice cloning.
      –ú–æ–≤–∞ –ù–ï –≤–∞–∂–ª–∏–≤–∞ - –≤–∞–∂–ª–∏–≤–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å –≥–æ–ª–æ—Å—ñ–≤!

‚úÖ –ü–ï–†–ï–í–Ü–†–ï–ù–Ü –î–ê–¢–ê–°–ï–¢–ò:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Dataset                    | Speakers | Hours  | Size    | Language
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
LibriTTS-R clean-360       | 904      | 360h   | ~70 GB  | English
VCTK                       | 110      | 44h    | ~12 GB  | English
OpenTTS-UK                 | 5        | 24h    | ~1 GB   | Ukrainian
Common Voice UK (subset)   | 1000+    | ~80h   | ~15 GB  | Ukrainian
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
TOTAL (recommended)        | 2000+    | ~500h  | ~100 GB |
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

PRESETS:
  --minimal   : OpenTTS only (5 speakers, 24h) - testing
  --medium    : OpenTTS + VCTK (115 speakers, 68h) - good start
  --large     : + LibriTTS-R clean-100 (351 speakers, 168h)
  --full      : + LibriTTS-R clean-360 (1255 speakers, 500h+)

–í–ê–ñ–õ–ò–í–û –¥–ª—è Voice Cloning:
  - –ú—ñ–Ω—ñ–º—É–º 100+ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
  - –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ 500-1000+ —Å–ø—ñ–∫–µ—Ä—ñ–≤
  - –†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å: —á–æ–ª–æ–≤—ñ–∫–∏, –∂—ñ–Ω–∫–∏, —Ä—ñ–∑–Ω—ñ –≤—ñ–∫–∏, –∞–∫—Ü–µ–Ω—Ç–∏

Usage:
    python scripts/prepare_data_stage2.py --medium
    python scripts/prepare_data_stage2.py --large --output data_stage2
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import random
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import tarfile
import shutil
import time
import multiprocessing

# Use most of available CPUs for parallel processing
NUM_WORKERS = min(64, multiprocessing.cpu_count())

try:
    from tqdm import tqdm
except ImportError:
    # Fallback if tqdm not installed
    def tqdm(iterable, **kwargs):
        total = kwargs.get('total', None)
        desc = kwargs.get('desc', '')
        for i, item in enumerate(iterable):
            if total and i % max(1, total // 20) == 0:
                print(f"   {desc}: {i}/{total} ({100*i/total:.0f}%)")
            yield item

try:
    import torch
    import torchaudio
    TORCHAUDIO_AVAILABLE = True
except ImportError:
    TORCHAUDIO_AVAILABLE = False

try:
    from datasets import load_dataset
    from huggingface_hub import snapshot_download, hf_hub_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False


# ============================================================================
# DATASET DEFINITIONS
# ============================================================================

OPENTTS_VOICES = [
    ("speech-uk/opentts-lada", "lada", 6962),
    ("speech-uk/opentts-tetiana", "tetiana", 5227),
    ("speech-uk/opentts-mykyta", "mykyta", 6436),
    ("speech-uk/opentts-oleksa", "oleksa", 3555),
    ("speech-uk/opentts-kateryna", "kateryna", 1803),
]


def print_banner():
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë     SUPERTONIC V2 - STAGE 2 DATA PREPARATION                             ‚ïë
‚ïë     Multi-Speaker Data for Voice Cloning                                 ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")


def check_dependencies():
    """Check required packages."""
    errors = []
    if not HF_AVAILABLE:
        errors.append("huggingface_hub and datasets not found")
        errors.append("  Install: pip install huggingface_hub datasets")
    if not TORCHAUDIO_AVAILABLE:
        errors.append("torchaudio not found")
    if errors:
        print("‚ùå Missing dependencies:")
        for e in errors:
            print(f"   {e}")
        sys.exit(1)
    print("‚úÖ All dependencies available\n")


# ============================================================================
# AUDIO PROCESSING
# ============================================================================

def get_audio_duration(audio_path: str, target_sr: int = 22050) -> Optional[float]:
    """Get audio duration in seconds. Works with different torchaudio versions."""
    audio_path = str(audio_path)
    
    # Method 1: Try torchaudio.info (older versions)
    try:
        info = torchaudio.info(audio_path)
        return info.num_frames / info.sample_rate
    except (AttributeError, Exception):
        pass
    
    # Method 2: Load audio and calculate duration
    try:
        waveform, sr = torchaudio.load(audio_path)
        return waveform.shape[1] / sr
    except Exception as e:
        print(f"      ‚ö†Ô∏è Could not read {audio_path}: {e}")
        return None


def process_audio_file(args) -> Optional[Dict]:
    """Process single audio file."""
    audio_path, output_path, text, speaker_id, target_sr, source, language = args
    
    try:
        waveform, sr = torchaudio.load(str(audio_path))
        
        # Convert to mono
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # Resample
        if sr != target_sr:
            resampler = torchaudio.transforms.Resample(sr, target_sr)
            waveform = resampler(waveform)
        
        # Save
        torchaudio.save(str(output_path), waveform, target_sr)
        
        duration = waveform.shape[1] / target_sr
        
        return {
            "audio_path": str(output_path),
            "text": text,
            "speaker_id": speaker_id,
            "duration": round(duration, 3),
            "sample_rate": target_sr,
            "source": source,
            "language": language,
        }
    except Exception as e:
        return None


# ============================================================================
# DATASET DOWNLOADERS  
# ============================================================================

def download_opentts(output_dir: Path, target_sr: int = 22050) -> Tuple[List[Dict], int]:
    """Download OpenTTS-UK (5 speakers)."""
    print("\n" + "="*70)
    print("üì• OpenTTS-UK (5 Ukrainian studio voices)")
    print("="*70)
    
    entries = []
    speakers = set()
    audio_dir = output_dir / "audio" / "opentts"
    audio_dir.mkdir(parents=True, exist_ok=True)
    
    for repo_id, voice_name, expected_rows in OPENTTS_VOICES:
        voice_dir = audio_dir / voice_name
        
        # Check if exists
        existing_files = list(voice_dir.glob("*.wav")) if voice_dir.exists() else []
        
        if len(existing_files) >= expected_rows * 0.95:
            print(f"   ‚úÖ {voice_name}: Already downloaded ({len(existing_files)} files)")
            
            # Load from HuggingFace to get texts
            try:
                ds = load_dataset(repo_id, split="train")
                for idx, item in enumerate(ds):
                    audio_path = voice_dir / f"{voice_name}_{idx:06d}.wav"
                    if audio_path.exists():
                        text = item.get("text", item.get("sentence", ""))
                        duration = get_audio_duration(audio_path, target_sr)
                        if duration is None:
                            continue
                        
                        entries.append({
                            "audio_path": str(audio_path.relative_to(output_dir)),
                            "text": text,
                            "speaker_id": f"opentts_{voice_name}",
                            "duration": round(duration, 3),
                            "sample_rate": target_sr,
                            "source": "opentts",
                            "language": "uk",
                        })
                speakers.add(f"opentts_{voice_name}")
            except Exception as e:
                print(f"      ‚ö†Ô∏è Could not load texts: {e}")
            continue
        
        print(f"\n   üì• {voice_name} ({expected_rows} samples)")
        sys.stdout.flush()
        
        try:
            ds = load_dataset(repo_id, split="train")
            voice_dir.mkdir(exist_ok=True)
            
            # Process in parallel using ThreadPoolExecutor
            def process_item(args):
                idx, item = args
                try:
                    audio_array = item["audio"]["array"]
                    sr = item["audio"]["sampling_rate"]
                    text = item.get("text", item.get("sentence", ""))
                    
                    waveform = torch.tensor(audio_array).unsqueeze(0).float()
                    if sr != target_sr:
                        resampler = torchaudio.transforms.Resample(sr, target_sr)
                        waveform = resampler(waveform)
                    
                    audio_path = voice_dir / f"{voice_name}_{idx:06d}.wav"
                    torchaudio.save(str(audio_path), waveform, target_sr)
                    
                    duration = waveform.shape[1] / target_sr
                    
                    return {
                        "audio_path": str(audio_path.relative_to(output_dir)),
                        "text": text,
                        "speaker_id": f"opentts_{voice_name}",
                        "duration": round(duration, 3),
                        "sample_rate": target_sr,
                        "source": "opentts",
                        "language": "uk",
                    }
                except:
                    return None
            
            # Use ThreadPoolExecutor for I/O bound tasks
            with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
                results = list(tqdm(
                    executor.map(process_item, enumerate(ds)),
                    desc=voice_name,
                    total=len(ds)
                ))
            
            # Filter None results and add to entries
            for r in results:
                if r is not None:
                    entries.append(r)
            
            speakers.add(f"opentts_{voice_name}")
            print(f"   ‚úÖ {voice_name}: {len([e for e in entries if e['speaker_id'] == f'opentts_{voice_name}'])} samples")
            
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
    
    return entries, len(speakers)


def download_vctk(output_dir: Path, target_sr: int = 22050) -> Tuple[List[Dict], int]:
    """
    Download VCTK dataset (110 English speakers).
    Uses HuggingFace: speechcolab/vctk
    """
    print("\n" + "="*70)
    print("üì• VCTK (110 English speakers)")
    print("   Size: ~12 GB | Duration: ~44 hours")
    print("="*70)
    
    entries = []
    speakers = set()
    audio_dir = output_dir / "audio" / "vctk"
    
    # Check if already downloaded
    if audio_dir.exists():
        existing = list(audio_dir.glob("**/*.wav"))
        if len(existing) > 40000:
            print(f"   ‚úÖ VCTK: Already downloaded ({len(existing)} files)")
            # Count speakers
            speaker_dirs = [d for d in audio_dir.iterdir() if d.is_dir()]
            print(f"      Loading metadata...")
            
            for spk_dir in speaker_dirs:
                speaker_id = f"vctk_{spk_dir.name}"
                speakers.add(speaker_id)
                
                for wav_file in spk_dir.glob("*.wav"):
                    try:
                        duration = get_audio_duration(wav_file, target_sr)
                        if duration is None:
                            continue
                        
                        # VCTK text files
                        txt_file = wav_file.with_suffix('.txt')
                        text = ""
                        if txt_file.exists():
                            text = txt_file.read_text().strip()
                        
                        entries.append({
                            "audio_path": str(wav_file.relative_to(output_dir)),
                            "text": text,
                            "speaker_id": speaker_id,
                            "duration": round(duration, 3),
                            "sample_rate": target_sr,
                            "source": "vctk",
                            "language": "en",
                        })
                    except:
                        pass
            
            print(f"      ‚úÖ Loaded {len(entries)} entries, {len(speakers)} speakers")
            return entries, len(speakers)
    
    audio_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        print("   üì¶ Downloading VCTK from HuggingFace...")
        print("   üìä VCTK: 110 speakers, ~44 hours, 48kHz audio")
        sys.stdout.flush()
        
        start_time = time.time()
        # VCTK: Try loading without trust_remote_code first
        # The dataset has a loading script which may not work with newer datasets versions
        try:
            # Try parquet/arrow format first (newer datasets library)
            ds = load_dataset("CSTR-Edinburgh/vctk", num_proc=4)
            ds = ds[list(ds.keys())[0]]
        except Exception as e1:
            print(f"   ‚ö†Ô∏è Standard loading failed: {e1}")
            print("   ‚ö†Ô∏è VCTK uses deprecated loading script format")
            print("   ‚ö†Ô∏è Skipping VCTK - LibriTTS-R provides enough speaker diversity")
            print("   üí° To include VCTK, downgrade datasets: pip install datasets==2.14.0")
            return entries, len(speakers)
        
        print(f"   ‚úÖ Dataset loaded in {time.time() - start_time:.1f}s ({len(ds)} samples)")
        sys.stdout.flush()
        
        print("   üîÑ Processing audio files with {0} workers...".format(NUM_WORKERS))
        
        # Prepare items for parallel processing
        items_to_process = list(enumerate(ds))
        processed_count = [0]  # Use list for mutable counter in closure
        lock = __import__('threading').Lock()
        
        def process_vctk_item(args):
            idx, item = args
            try:
                audio = item["audio"]
                speaker = item["speaker_id"]
                text = item.get("text", "")
                
                speaker_dir = audio_dir / speaker
                with lock:
                    speaker_dir.mkdir(exist_ok=True)
                    speakers.add(f"vctk_{speaker}")
                
                audio_array = audio["array"]
                sr = audio["sampling_rate"]
                
                waveform = torch.tensor(audio_array).unsqueeze(0).float()
                if sr != target_sr:
                    resampler = torchaudio.transforms.Resample(sr, target_sr)
                    waveform = resampler(waveform)
                
                filename = f"{speaker}_{idx:06d}.wav"
                audio_path = speaker_dir / filename
                torchaudio.save(str(audio_path), waveform, target_sr)
                
                duration = waveform.shape[1] / target_sr
                
                return {
                    "audio_path": str(audio_path.relative_to(output_dir)),
                    "text": text,
                    "speaker_id": f"vctk_{speaker}",
                    "duration": round(duration, 3),
                    "sample_rate": target_sr,
                    "source": "vctk",
                    "language": "en",
                }
            except:
                return None
        
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            results = list(tqdm(
                executor.map(process_vctk_item, items_to_process),
                desc="VCTK",
                total=len(items_to_process)
            ))
        
        for r in results:
            if r is not None:
                entries.append(r)
        
        print(f"   ‚úÖ VCTK: {len(entries)} samples, {len(speakers)} speakers")
        
    except Exception as e:
        print(f"   ‚ùå Error downloading VCTK: {e}")
        import traceback
        traceback.print_exc()
    
    return entries, len(speakers)


def download_libritts(
    output_dir: Path, 
    target_sr: int = 22050,
    subset: str = "clean-100"  # "clean-100", "clean-360", "other-500"
) -> Tuple[List[Dict], int]:
    """
    Download LibriTTS-R dataset.
    
    Subsets:
      clean-100: 247 speakers, 100 hours
      clean-360: 904 speakers, 360 hours  
      other-500: 1166 speakers, 500 hours (noisier)
    """
    print("\n" + "="*70)
    print(f"üì• LibriTTS-R ({subset})")
    
    subset_info = {
        "clean-100": {"speakers": 247, "hours": 100, "size": "18 GB"},
        "clean-360": {"speakers": 904, "hours": 360, "size": "70 GB"},
        "other-500": {"speakers": 1166, "hours": 500, "size": "85 GB"},
    }
    
    info = subset_info.get(subset, {})
    print(f"   Speakers: ~{info.get('speakers', '?')} | Hours: ~{info.get('hours', '?')} | Size: ~{info.get('size', '?')}")
    print("="*70)
    
    entries = []
    speakers = set()
    
    subset_name = subset.replace("-", "_")  # clean-100 -> clean_100
    audio_dir = output_dir / "audio" / f"libritts_{subset_name}"
    
    # Check if already downloaded
    if audio_dir.exists():
        existing = list(audio_dir.glob("**/*.wav"))
        if len(existing) > 10000:
            print(f"   ‚úÖ LibriTTS-R {subset}: Already downloaded ({len(existing)} files)")
            print(f"      Loading metadata...")
            
            for wav_file in existing:
                try:
                    # Speaker ID from path: libritts_clean_100/1234/5678/1234_5678_000001.wav
                    parts = wav_file.stem.split("_")
                    speaker = parts[0] if parts else "unknown"
                    speaker_id = f"libritts_{speaker}"
                    speakers.add(speaker_id)
                    
                    duration = get_audio_duration(wav_file, target_sr)
                    if duration is None:
                        continue
                    
                    # Load normalized text
                    txt_file = wav_file.with_suffix('.normalized.txt')
                    text = ""
                    if txt_file.exists():
                        text = txt_file.read_text().strip()
                    
                    entries.append({
                        "audio_path": str(wav_file.relative_to(output_dir)),
                        "text": text,
                        "speaker_id": speaker_id,
                        "duration": round(duration, 3),
                        "sample_rate": target_sr,
                        "source": "libritts",
                        "language": "en",
                    })
                except:
                    pass
            
            print(f"      ‚úÖ Loaded {len(entries)} entries, {len(speakers)} speakers")
            return entries, len(speakers)
    
    audio_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        print(f"   üì¶ Downloading LibriTTS-R {subset} from HuggingFace...")
        print("   ‚è≥ This is a large dataset, download may take 10-30 minutes...")
        sys.stdout.flush()
        
        start_time = time.time()
        # LibriTTS-R: https://huggingface.co/datasets/mythicinfinity/libritts_r
        # Config: "clean", Splits: "train.clean.100", "train.clean.360"
        split_name = f"train.{subset.replace('-', '.')}"
        ds = load_dataset(
            "mythicinfinity/libritts_r",
            "clean",  # config name
            split=split_name,  # e.g., "train.clean.100"
            num_proc=4
        )
        print(f"   ‚úÖ Dataset loaded in {time.time() - start_time:.1f}s ({len(ds)} samples)")
        sys.stdout.flush()
        
        print("   üîÑ Processing audio files with {0} workers...".format(NUM_WORKERS))
        print("   ‚è≥ Preparing data (this may take a moment for large datasets)...")
        sys.stdout.flush()
        
        lock = __import__('threading').Lock()
        results = []
        
        def process_libritts_item(idx_item):
            idx, item = idx_item
            try:
                audio = item["audio"]
                speaker = str(item["speaker_id"])
                text = item.get("text_normalized", item.get("text", ""))
                utterance_id = item.get("id", str(idx))
                
                speaker_dir = audio_dir / speaker
                with lock:
                    speaker_dir.mkdir(exist_ok=True)
                    speakers.add(f"libritts_{speaker}")
                
                audio_array = audio["array"]
                sr = audio["sampling_rate"]
                
                waveform = torch.tensor(audio_array).unsqueeze(0).float()
                if sr != target_sr:
                    resampler = torchaudio.transforms.Resample(sr, target_sr)
                    waveform = resampler(waveform)
                
                filename = f"{utterance_id}.wav"
                audio_path = speaker_dir / filename
                torchaudio.save(str(audio_path), waveform, target_sr)
                
                duration = waveform.shape[1] / target_sr
                
                return {
                    "audio_path": str(audio_path.relative_to(output_dir)),
                    "text": text,
                    "speaker_id": f"libritts_{speaker}",
                    "duration": round(duration, 3),
                    "sample_rate": target_sr,
                    "source": "libritts",
                    "language": "en",
                }
            except:
                return None
        
        # Process in batches to avoid memory issues with large datasets
        batch_size = 5000
        total = len(ds)
        
        for batch_start in range(0, total, batch_size):
            batch_end = min(batch_start + batch_size, total)
            batch_items = [(i, ds[i]) for i in range(batch_start, batch_end)]
            
            with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
                batch_results = list(tqdm(
                    executor.map(process_libritts_item, batch_items),
                    desc=f"LibriTTS {subset} [{batch_start//1000}k-{batch_end//1000}k]",
                    total=len(batch_items)
                ))
            
            for r in batch_results:
                if r is not None:
                    entries.append(r)
        
        print(f"   ‚úÖ LibriTTS-R {subset}: {len(entries)} samples, {len(speakers)} speakers")
        
    except Exception as e:
        print(f"   ‚ùå Error downloading LibriTTS-R: {e}")
        import traceback
        traceback.print_exc()
    
    return entries, len(speakers)


# ============================================================================
# MANIFEST CREATION
# ============================================================================

def create_manifests(
    entries: List[Dict],
    output_dir: Path,
    val_ratio: float = 0.05,
    min_duration: float = 0.5,
    max_duration: float = 15.0,
    min_text_length: int = 5,
) -> Tuple[int, int]:
    """Create train/val manifests for Stage 2."""
    print("\n" + "="*70)
    print("üìù Creating manifests for Stage 2...")
    print("="*70)
    
    # Debug: show sample entries
    if entries:
        print(f"\n   üìä Sample entry for debugging:")
        sample = entries[0]
        print(f"      duration: {sample.get('duration', 'N/A')}")
        print(f"      text length: {len(sample.get('text', ''))}")
        print(f"      speaker_id: {sample.get('speaker_id', 'N/A')}")
    
    # Filter
    filtered = []
    rejected_duration = 0
    rejected_text = 0
    for e in entries:
        dur = e.get("duration", 0)
        text = e.get("text", "")
        if dur is None or dur < min_duration or dur > max_duration:
            rejected_duration += 1
            continue
        if len(text) < min_text_length:
            rejected_text += 1
            continue
        filtered.append(e)
    
    print(f"\n   Total entries: {len(entries)}")
    print(f"   Rejected (duration): {rejected_duration}")
    print(f"   Rejected (text too short): {rejected_text}")
    print(f"   After filtering: {len(filtered)}")
    
    # Count speakers
    speakers = set(e["speaker_id"] for e in filtered)
    print(f"   Total speakers: {len(speakers)}")
    
    # Count by language
    languages = {}
    for e in filtered:
        lang = e.get("language", "unknown")
        languages[lang] = languages.get(lang, 0) + 1
    
    print(f"   By language:")
    for lang, count in sorted(languages.items()):
        print(f"      {lang}: {count:,}")
    
    # Stratified split by speaker
    speaker_samples = {}
    for e in filtered:
        spk = e["speaker_id"]
        if spk not in speaker_samples:
            speaker_samples[spk] = []
        speaker_samples[spk].append(e)
    
    train_entries = []
    val_entries = []
    
    for spk, samples in speaker_samples.items():
        random.shuffle(samples)
        n_val = max(1, int(len(samples) * val_ratio))
        val_entries.extend(samples[:n_val])
        train_entries.extend(samples[n_val:])
    
    random.shuffle(train_entries)
    random.shuffle(val_entries)
    
    # Save manifests
    manifest_dir = output_dir / "manifests_stage2"
    manifest_dir.mkdir(parents=True, exist_ok=True)
    
    train_path = manifest_dir / "train.json"
    val_path = manifest_dir / "val.json"
    
    with open(train_path, "w", encoding="utf-8") as f:
        json.dump(train_entries, f, ensure_ascii=False, indent=2)
    
    with open(val_path, "w", encoding="utf-8") as f:
        json.dump(val_entries, f, ensure_ascii=False, indent=2)
    
    # Stats
    train_hours = sum(e["duration"] for e in train_entries) / 3600
    val_hours = sum(e["duration"] for e in val_entries) / 3600
    train_speakers = len(set(e["speaker_id"] for e in train_entries))
    val_speakers = len(set(e["speaker_id"] for e in val_entries))
    
    print(f"\n   Train: {len(train_entries):,} samples ({train_hours:.1f}h, {train_speakers} speakers)")
    print(f"   Val:   {len(val_entries):,} samples ({val_hours:.1f}h, {val_speakers} speakers)")
    print(f"\n   Saved: {train_path}")
    print(f"   Saved: {val_path}")
    
    return len(train_entries), len(val_entries)


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Prepare multi-speaker data for Stage 2 (Voice Cloning)",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    preset_group = parser.add_mutually_exclusive_group(required=True)
    preset_group.add_argument("--minimal", action="store_true", 
                              help="OpenTTS only (5 speakers, ~24h)")
    preset_group.add_argument("--medium", action="store_true",
                              help="OpenTTS + VCTK (115 speakers, ~68h)")
    preset_group.add_argument("--large", action="store_true",
                              help="+ LibriTTS clean-100 (362 speakers, ~168h)")
    preset_group.add_argument("--full", action="store_true",
                              help="+ LibriTTS clean-360 (1266 speakers, ~500h)")
    
    parser.add_argument("--output", type=str, default="data", help="Output directory")
    parser.add_argument("--sample-rate", type=int, default=22050, help="Target sample rate")
    parser.add_argument("--include-stage1", action="store_true",
                        help="Include Stage 1 manifest data (data/manifests/)")
    
    args = parser.parse_args()
    
    print_banner()
    check_dependencies()
    
    output_dir = Path(args.output).resolve()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_entries = []
    total_speakers = 0
    
    # Include Stage 1 data if available and requested
    if args.include_stage1:
        stage1_manifest = output_dir / "manifests" / "train.json"
        if stage1_manifest.exists():
            print("üìÇ Including Stage 1 data...")
            with open(stage1_manifest, "r", encoding="utf-8") as f:
                stage1_data = json.load(f)
            for entry in stage1_data:
                entry["language"] = "uk"
                all_entries.append(entry)
            s1_speakers = len(set(e["speaker_id"] for e in stage1_data))
            print(f"   ‚úÖ Added {len(stage1_data)} samples, {s1_speakers} speakers from Stage 1\n")
            total_speakers += s1_speakers
    
    # Always download OpenTTS
    entries, n_speakers = download_opentts(output_dir, args.sample_rate)
    all_entries.extend(entries)
    total_speakers += n_speakers
    
    # Medium+ includes VCTK
    if args.medium or args.large or args.full:
        entries, n_speakers = download_vctk(output_dir, args.sample_rate)
        all_entries.extend(entries)
        total_speakers += n_speakers
    
    # Large+ includes LibriTTS clean-100
    if args.large or args.full:
        entries, n_speakers = download_libritts(output_dir, args.sample_rate, "clean-100")
        all_entries.extend(entries)
        total_speakers += n_speakers
    
    # Full includes LibriTTS clean-360
    if args.full:
        entries, n_speakers = download_libritts(output_dir, args.sample_rate, "clean-360")
        all_entries.extend(entries)
        total_speakers += n_speakers
    
    # Create manifests
    if all_entries:
        train_count, val_count = create_manifests(all_entries, output_dir)
        
        actual_speakers = len(set(e["speaker_id"] for e in all_entries))
        
        print("\n" + "="*70)
        print("‚úÖ STAGE 2 DATA PREPARATION COMPLETE!")
        print("="*70)
        print(f"   Total samples: {train_count + val_count:,}")
        print(f"   Total speakers: {actual_speakers}")
        print(f"   Train: {train_count:,}")
        print(f"   Val: {val_count:,}")
        print(f"\n   Manifests: {output_dir}/manifests_stage2/")
        print(f"\n   Next step:")
        print(f"   ./run_train_stage2.sh")
        print("="*70 + "\n")
    else:
        print("\n‚ùå No data downloaded!")
        sys.exit(1)


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: scripts/prepare_data.py
–†–û–ó–ú–Ü–†: 31.08 KB
==================================================

#!/usr/bin/env python3
"""
SUPERTONIC V2 TTS - DATA PREPARATION SCRIPT (VERIFIED JANUARY 2026)
====================================================================
–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–∞ –≥–æ—Ç—É—î –¥–∞—Ç–∞—Å–µ—Ç–∏ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è TTS –º–æ–¥–µ–ª—ñ.

‚úÖ –ü–ï–†–ï–í–Ü–†–ï–ù–Ü –î–ê–¢–ê–°–ï–¢–ò (HuggingFace, –∞–∫—Ç–∏–≤–Ω—ñ –Ω–∞ —Å—ñ—á–µ–Ω—å 2026):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Dataset                        | Size     | Hours  | Rows   | License
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
OpenTTS-UK (5 voices):
  speech-uk/opentts-lada       | 201 MB   | ~7h    | 6,962  | Apache 2.0
  speech-uk/opentts-tetiana    | 156 MB   | ~5h    | 5,227  | Apache 2.0  
  speech-uk/opentts-mykyta     | 195 MB   | ~6h    | 6,436  | Apache 2.0
  speech-uk/opentts-oleksa     | 363 MB   | ~4h    | 3,555  | Apache 2.0
  speech-uk/opentts-kateryna   | 123 MB   | ~2h    | 1,803  | CC BY-NC 4.0
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  SUBTOTAL OpenTTS             | ~1 GB    | ~24h   | 23,983 |
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
UK-Pods                        | ~5 GB    | ~51h   | 34,231 | CC BY-NC 4.0
  taras-sereda/uk-pods
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Broadcast Speech UK            | 33.9 GB  | ~300h  | 136,736| Apache 2.0
  Yehor/broadcast-speech-uk
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Voice of America UK            | ~15 GB   | ~391h  | ~300k  | CC BY 4.0
  speech-uk/voice-of-america
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
TOTAL (all datasets)           | ~55 GB   | ~766h  | ~500k  |
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è M-AILABS (caito.de) - –ù–ï–î–û–°–¢–£–ü–ù–ò–ô (—Å–µ—Ä–≤–µ—Ä –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î)

–†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á:
  --quality    : OpenTTS only (~1 GB, 24h) - –Ω–∞–π–∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å, –º–∞–ª–æ –¥–∞–Ω–∏—Ö
  --medium     : OpenTTS + UK-Pods (~6 GB, 75h) - –≥–∞—Ä–Ω–∏–π –±–∞–ª–∞–Ω—Å
  --large      : + Broadcast (~40 GB, 375h) - –±–∞–≥–∞—Ç–æ –¥–∞–Ω–∏—Ö
  --full       : + VoA (~55 GB, 766h) - –º–∞–∫—Å–∏–º—É–º –¥–∞–Ω–∏—Ö

–î–ª—è vast.ai:
  --quality  : 20 GB disk
  --medium   : 30 GB disk  
  --large    : 75 GB disk
  --full     : 100 GB disk

Usage:
    python scripts/prepare_data.py --quality
    python scripts/prepare_data.py --medium
    python scripts/prepare_data.py --large
    python scripts/prepare_data.py --full
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Number of parallel workers for file I/O
# Note: Keep this low to avoid "Thread creation failed" errors on some systems
NUM_WORKERS = 8

try:
    import torch
    import torchaudio
    TORCHAUDIO_AVAILABLE = True
except ImportError:
    TORCHAUDIO_AVAILABLE = False

try:
    from datasets import load_dataset
    from huggingface_hub import snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False


# ============================================================================
# VERIFIED DATASET DEFINITIONS (January 2026)
# ============================================================================

OPENTTS_VOICES = [
    # (repo_id, voice_name, size_mb, rows, license)
    ("speech-uk/opentts-lada", "lada", 201, 6962, "Apache 2.0"),
    ("speech-uk/opentts-tetiana", "tetiana", 156, 5227, "Apache 2.0"),
    ("speech-uk/opentts-mykyta", "mykyta", 195, 6436, "Apache 2.0"),
    ("speech-uk/opentts-oleksa", "oleksa", 363, 3555, "Apache 2.0"),
    ("speech-uk/opentts-kateryna", "kateryna", 123, 1803, "CC BY-NC 4.0"),
]

DATASETS_INFO = {
    "ukpods": {
        "repo_id": "taras-sereda/uk-pods",
        "size_gb": 5,
        "hours": 51,
        "rows": 34231,
        "license": "CC BY-NC 4.0",
        "format": "tar.gz",
    },
    "broadcast": {
        "repo_id": "Yehor/broadcast-speech-uk",
        "size_gb": 33.9,
        "hours": 300,
        "rows": 136736,
        "license": "Apache 2.0",
        "format": "parquet",
    },
    "voa": {
        "repo_id": "speech-uk/voice-of-america",
        "size_gb": 15,
        "hours": 391,
        "rows": 300000,
        "license": "CC BY 4.0",
        "format": "files",
    },
}


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def print_banner():
    """Print startup banner."""
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         SUPERTONIC V2 TTS - DATA PREPARATION                             ‚ïë
‚ïë         Verified Datasets (January 2026)                                 ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")


def check_dependencies():
    """Check required packages."""
    errors = []
    
    if not HF_AVAILABLE:
        errors.append("huggingface_hub and datasets not found")
        errors.append("  Install: pip install huggingface_hub datasets")
    
    if not TORCHAUDIO_AVAILABLE:
        errors.append("torchaudio not found")
        errors.append("  Install: pip install torchaudio")
    
    if errors:
        print("‚ùå Missing dependencies:")
        for e in errors:
            print(f"   {e}")
        sys.exit(1)
    
    print("‚úÖ All dependencies available\n")


def estimate_disk_space(preset: str) -> str:
    """Estimate required disk space."""
    estimates = {
        "quality": "20 GB",
        "medium": "30 GB", 
        "large": "75 GB",
        "full": "100 GB",
    }
    return estimates.get(preset, "50 GB")


def estimate_hours(preset: str) -> str:
    """Estimate total hours."""
    estimates = {
        "quality": "~24 hours",
        "medium": "~75 hours",
        "large": "~375 hours", 
        "full": "~766 hours",
    }
    return estimates.get(preset, "unknown")


# ============================================================================
# PARALLEL PROCESSING HELPERS
# ============================================================================

def save_audio_worker(args):
    """Worker function for parallel audio saving."""
    idx, audio_array, sr, text, target_sr, audio_path, output_dir, speaker_id, source = args
    
    try:
        waveform = torch.tensor(audio_array).unsqueeze(0).float()
        
        if sr != target_sr:
            resampler = torchaudio.transforms.Resample(sr, target_sr)
            waveform = resampler(waveform)
        
        torchaudio.save(str(audio_path), waveform, target_sr)
        
        duration = waveform.shape[1] / target_sr
        
        return {
            "audio_path": str(audio_path.relative_to(output_dir)),
            "text": text,
            "speaker_id": speaker_id,
            "duration": round(duration, 3),
            "sample_rate": target_sr,
            "source": source,
        }
    except Exception as e:
        return None


def get_audio_info_worker(args):
    """Worker function for parallel audio info extraction."""
    wav_file, output_dir, text, speaker_id, target_sr, source = args
    
    try:
        info = torchaudio.info(str(wav_file))
        duration = info.num_frames / info.sample_rate
        
        try:
            rel_path = wav_file.relative_to(output_dir)
        except ValueError:
            rel_path = Path(source) / "audio" / speaker_id / wav_file.name
        
        return {
            "audio_path": str(rel_path),
            "text": text,
            "speaker_id": speaker_id,
            "duration": round(duration, 3),
            "sample_rate": target_sr,
            "source": source,
        }
    except Exception as e:
        return None


# ============================================================================
# DATASET DOWNLOADERS
# ============================================================================

def check_existing_data(audio_dir: Path, source_name: str, expected_count: int) -> Tuple[bool, int]:
    """
    Check if data already exists.
    Returns (is_complete, existing_count)
    """
    if not audio_dir.exists():
        return False, 0
    
    wav_files = list(audio_dir.glob("**/*.wav"))
    count = len(wav_files)
    
    # Consider complete if we have at least 95% of expected
    is_complete = count >= expected_count * 0.95
    
    return is_complete, count


def load_existing_manifest(output_dir: Path, source: str) -> List[Dict]:
    """Load entries from existing manifest for a specific source."""
    manifest_path = output_dir / "manifests" / "train.json"
    if not manifest_path.exists():
        return []
    
    try:
        with open(manifest_path, "r", encoding="utf-8") as f:
            all_entries = json.load(f)
        return [e for e in all_entries if e.get("source") == source]
    except:
        return []


def download_opentts(
    output_dir: Path,
    target_sr: int = 22050,
    voices: Optional[List[str]] = None
) -> List[Dict]:
    """
    Download OpenTTS-UK voices from HuggingFace.
    All 5 voices: ~1 GB, ~24 hours, ~24k samples
    """
    print("\n" + "="*70)
    print("üì• OpenTTS-UK (5 studio-quality voices)")
    print("   Size: ~1 GB | Duration: ~24 hours | Samples: ~24,000")
    print("="*70)
    
    if voices:
        selected = [v for v in OPENTTS_VOICES if v[1] in voices]
    else:
        selected = OPENTTS_VOICES
    
    print(f"\n   Voices: {[v[1] for v in selected]}")
    
    manifest_entries = []
    
    # Support both old structure (opentts/audio/) and new (audio/opentts/)
    old_audio_dir = output_dir / "opentts" / "audio"
    new_audio_dir = output_dir / "audio" / "opentts"
    
    # Use old structure if it exists, otherwise use new
    if old_audio_dir.exists():
        audio_dir = old_audio_dir
        print(f"   üìÇ Using existing structure: opentts/audio/")
    else:
        audio_dir = new_audio_dir
        print(f"   üìÇ Using structure: audio/opentts/")
    
    audio_dir.mkdir(parents=True, exist_ok=True)
    
    for repo_id, voice_name, size_mb, rows, license_type in selected:
        voice_dir = audio_dir / voice_name
        
        # Check if already downloaded
        is_complete, existing_count = check_existing_data(voice_dir, voice_name, rows)
        
        if is_complete:
            print(f"\n   ‚úÖ {voice_name}: Already downloaded ({existing_count} files)")
            print(f"      Loading texts from HuggingFace...")
            
            # Load dataset to get texts
            try:
                ds = load_dataset(repo_id, split="train")
                text_map = {}
                for idx, item in enumerate(ds):
                    filename = f"{voice_name}_{idx:06d}.wav"
                    text = item.get("text", item.get("sentence", ""))
                    text_map[filename] = text
                
                # Parallel loading of audio info
                wav_files = sorted(voice_dir.glob("*.wav"))
                tasks = []
                for wav_file in wav_files:
                    text = text_map.get(wav_file.name, "")
                    tasks.append((wav_file, output_dir, text, voice_name, target_sr, "opentts"))
                
                loaded_count = 0
                with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
                    futures = [executor.submit(get_audio_info_worker, task) for task in tasks]
                    for future in as_completed(futures):
                        result = future.result()
                        if result:
                            manifest_entries.append(result)
                            loaded_count += 1
                
                print(f"      ‚úÖ Loaded {loaded_count} entries with texts")
            except Exception as e:
                print(f"      ‚ö†Ô∏è Could not load texts: {e}")
                import traceback
                traceback.print_exc()
            continue
        
        print(f"\n   üì• {voice_name} ({size_mb} MB, {rows} samples, {license_type})")
        print(f"      URL: https://huggingface.co/datasets/{repo_id}")
        
        try:
            ds = load_dataset(repo_id, split="train")
            
            voice_dir.mkdir(exist_ok=True)
            
            # Collect all items first
            items_to_process = []
            for idx, item in enumerate(ds):
                audio_array = item["audio"]["array"]
                sr = item["audio"]["sampling_rate"]
                text = item.get("text", item.get("sentence", ""))
                audio_filename = f"{voice_name}_{idx:06d}.wav"
                audio_path = voice_dir / audio_filename
                items_to_process.append((idx, audio_array, sr, text, target_sr, audio_path, output_dir, voice_name, "opentts"))
            
            # Process in parallel
            processed = 0
            with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
                futures = {executor.submit(save_audio_worker, item): item for item in items_to_process}
                for future in as_completed(futures):
                    result = future.result()
                    if result:
                        manifest_entries.append(result)
                        processed += 1
                        if processed % 1000 == 0:
                            print(f"      Processed {processed}/{rows}...")
            
            print(f"   ‚úÖ {voice_name}: {processed} samples saved")
            
        except Exception as e:
            print(f"   ‚ùå Error downloading {voice_name}: {e}")
            continue
    
    return manifest_entries


def download_ukpods(
    output_dir: Path,
    target_sr: int = 22050
) -> List[Dict]:
    """
    Download UK-Pods from HuggingFace.
    Size: ~5 GB | Duration: ~51 hours | Samples: 34,231
    
    Note: This dataset uses audio_filepath format, need to download repo first.
    """
    print("\n" + "="*70)
    print("üì• UK-Pods (Ukrainian podcasts)")
    print("   Size: ~5 GB | Duration: ~51 hours | Samples: 34,231")
    print("   URL: https://huggingface.co/datasets/taras-sereda/uk-pods")
    print("="*70)
    
    manifest_entries = []
    audio_dir = output_dir / "audio" / "ukpods"
    
    # Check if already downloaded
    is_complete, existing_count = check_existing_data(audio_dir, "ukpods", 30000)
    
    if is_complete:
        print(f"\n   ‚úÖ UK-Pods: Already downloaded ({existing_count} files)")
        # Parallel loading
        wav_files = list(audio_dir.glob("*.wav"))
        tasks = [(wav_file, output_dir, "", "ukpods", target_sr, "ukpods") for wav_file in wav_files]
        
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            futures = [executor.submit(get_audio_info_worker, task) for task in tasks]
            for future in as_completed(futures):
                result = future.result()
                if result:
                    manifest_entries.append(result)
        
        print(f"      ‚úÖ Loaded {len(manifest_entries)} entries")
        return manifest_entries
    
    audio_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # First download the full dataset repo (includes audio files)
        print(f"\n   üì¶ Downloading dataset repository...")
        ukpods_cache = output_dir / "ukpods_raw"
        snapshot_download(
            repo_id="taras-sereda/uk-pods",
            repo_type="dataset",
            local_dir=str(ukpods_cache),
        )
        print(f"   ‚úÖ Repository downloaded to {ukpods_cache}")
        
        # Load metadata
        ds = load_dataset("taras-sereda/uk-pods", split="train")
        print(f"\n   Total samples: {len(ds)}")
        
        # Check dataset structure
        sample = ds[0]
        print(f"   Dataset keys: {list(sample.keys())}")
        
        # Process items - audio_filepath points to files in the downloaded repo
        def process_ukpods_item(args):
            idx, item, ukpods_cache, audio_dir, output_dir, target_sr = args
            try:
                audio_filepath = item.get("audio_filepath", "")
                text = item.get("text", item.get("text_normalized", ""))
                
                # Find the audio file in cache
                audio_source = ukpods_cache / audio_filepath
                if not audio_source.exists():
                    # Try without leading slash
                    audio_source = ukpods_cache / audio_filepath.lstrip("/")
                if not audio_source.exists():
                    return None
                
                # Load and resample
                waveform, sr = torchaudio.load(str(audio_source))
                
                if sr != target_sr:
                    resampler = torchaudio.transforms.Resample(sr, target_sr)
                    waveform = resampler(waveform)
                
                # Save to our structure
                audio_filename = f"ukpods_{idx:06d}.wav"
                audio_path = audio_dir / audio_filename
                torchaudio.save(str(audio_path), waveform, target_sr)
                
                duration = waveform.shape[1] / target_sr
                
                return {
                    "audio_path": str(audio_path.relative_to(output_dir)),
                    "text": text,
                    "speaker_id": "ukpods",
                    "duration": round(duration, 3),
                    "sample_rate": target_sr,
                    "source": "ukpods",
                }
            except Exception as e:
                return None
        
        # Prepare tasks
        tasks = [(idx, item, ukpods_cache, audio_dir, output_dir, target_sr) for idx, item in enumerate(ds)]
        
        # Process in parallel
        processed = 0
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            futures = [executor.submit(process_ukpods_item, task) for task in tasks]
            for future in as_completed(futures):
                result = future.result()
                if result:
                    manifest_entries.append(result)
                    processed += 1
                    if processed % 5000 == 0:
                        print(f"   Processed {processed}/{len(ds)}...")
        
        print(f"   ‚úÖ UK-Pods: {processed} samples saved")
        
    except Exception as e:
        print(f"   ‚ùå Error downloading UK-Pods: {e}")
        import traceback
        traceback.print_exc()
    
    return manifest_entries


def download_broadcast(
    output_dir: Path,
    target_sr: int = 22050
) -> List[Dict]:
    """
    Download Broadcast Speech UK from HuggingFace.
    Size: ~34 GB | Duration: ~300 hours | Samples: 136,736
    """
    print("\n" + "="*70)
    print("üì• Downloading Broadcast Speech UK")
    print("   Size: ~34 GB | Duration: ~300 hours | Samples: 136,736")
    print("   URL: https://huggingface.co/datasets/Yehor/broadcast-speech-uk")
    print("   ‚ö†Ô∏è  This will take a while...")
    print(f"   üöÄ Using {NUM_WORKERS} parallel workers")
    print("="*70)
    
    manifest_entries = []
    audio_dir = output_dir / "audio" / "broadcast"
    
    # Check if already downloaded
    is_complete, existing_count = check_existing_data(audio_dir, "broadcast", 136736)
    
    if is_complete:
        print(f"\n   ‚úÖ Broadcast: Already downloaded ({existing_count} files)")
        # Parallel loading
        wav_files = list(audio_dir.glob("*.wav"))
        tasks = [(wav_file, output_dir, "", "broadcast", target_sr, "broadcast") for wav_file in wav_files]
        
        loaded = 0
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            futures = [executor.submit(get_audio_info_worker, task) for task in tasks]
            for future in as_completed(futures):
                result = future.result()
                if result:
                    manifest_entries.append(result)
                    loaded += 1
                    if loaded % 20000 == 0:
                        print(f"      Loaded {loaded}/{existing_count}...")
        
        print(f"      ‚úÖ Loaded {len(manifest_entries)} entries")
        return manifest_entries
    
    audio_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # Use streaming for large dataset
        ds = load_dataset("Yehor/broadcast-speech-uk", split="train", streaming=True)
        
        # Batch processing for parallel saving
        BATCH_SIZE = NUM_WORKERS * 10
        batch = []
        processed = 0
        
        for idx, item in enumerate(ds):
            try:
                audio_array = item["audio"]["array"]
                sr = item["audio"]["sampling_rate"]
                text = item.get("text", item.get("sentence", item.get("transcription", "")))
                
                audio_filename = f"broadcast_{idx:06d}.wav"
                audio_path = audio_dir / audio_filename
                batch.append((idx, audio_array, sr, text, target_sr, audio_path, output_dir, "broadcast", "broadcast"))
                
                # Process batch in parallel
                if len(batch) >= BATCH_SIZE:
                    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
                        futures = [executor.submit(save_audio_worker, item) for item in batch]
                        for future in as_completed(futures):
                            result = future.result()
                            if result:
                                manifest_entries.append(result)
                                processed += 1
                    batch = []
                    
                    if processed % 10000 == 0:
                        print(f"   Processed {processed}...")
            except Exception as e:
                continue
        
        # Process remaining batch
        if batch:
            with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
                futures = [executor.submit(save_audio_worker, item) for item in batch]
                for future in as_completed(futures):
                    result = future.result()
                    if result:
                        manifest_entries.append(result)
                        processed += 1
        
        print(f"   ‚úÖ Broadcast: {processed} samples saved")
        
    except Exception as e:
        print(f"   ‚ùå Error downloading Broadcast: {e}")
        import traceback
        traceback.print_exc()
    
    return manifest_entries


def download_voa(
    output_dir: Path,
    target_sr: int = 22050
) -> List[Dict]:
    """
    Download Voice of America UK from HuggingFace.
    Size: ~15 GB | Duration: ~391 hours
    """
    print("\n" + "="*70)
    print("üì• Downloading Voice of America UK")
    print("   Size: ~15 GB | Duration: ~391 hours")
    print("   URL: https://huggingface.co/datasets/speech-uk/voice-of-america")
    print("   ‚ö†Ô∏è  This dataset uses custom format - downloading files...")
    print("="*70)
    
    manifest_entries = []
    
    try:
        voa_dir = output_dir / "voa_raw"
        
        print(f"\n   Downloading to: {voa_dir}")
        snapshot_download(
            repo_id="speech-uk/voice-of-america",
            repo_type="dataset",
            local_dir=str(voa_dir),
        )
        
        print(f"   ‚úÖ VoA downloaded to {voa_dir}")
        print(f"   ‚ö†Ô∏è  Manual processing required - check voa_raw/ folder")
        
    except Exception as e:
        print(f"   ‚ùå Error downloading VoA: {e}")
    
    return manifest_entries


# ============================================================================
# MANIFEST CREATION
# ============================================================================

def create_manifests(
    entries: List[Dict],
    output_dir: Path,
    val_ratio: float = 0.05,
    min_duration: float = 0.5,
    max_duration: float = 15.0
) -> Tuple[int, int]:
    """Create train/val manifest files."""
    print("\n" + "="*70)
    print("üìù Creating manifests...")
    print("="*70)
    
    # Filter by duration
    filtered = [
        e for e in entries 
        if min_duration <= e["duration"] <= max_duration
    ]
    
    print(f"   Total entries: {len(entries)}")
    print(f"   After filtering ({min_duration}s - {max_duration}s): {len(filtered)}")
    
    # Shuffle and split
    random.shuffle(filtered)
    
    val_count = int(len(filtered) * val_ratio)
    train_count = len(filtered) - val_count
    
    train_entries = filtered[:train_count]
    val_entries = filtered[train_count:]
    
    # Save manifests
    manifest_dir = output_dir / "manifests"
    manifest_dir.mkdir(parents=True, exist_ok=True)
    
    train_path = manifest_dir / "train.json"
    val_path = manifest_dir / "val.json"
    
    with open(train_path, "w", encoding="utf-8") as f:
        json.dump(train_entries, f, ensure_ascii=False, indent=2)
    
    with open(val_path, "w", encoding="utf-8") as f:
        json.dump(val_entries, f, ensure_ascii=False, indent=2)
    
    # Stats
    train_duration = sum(e["duration"] for e in train_entries) / 3600
    val_duration = sum(e["duration"] for e in val_entries) / 3600
    
    # Source breakdown
    sources = {}
    for e in train_entries:
        src = e.get("source", "unknown")
        sources[src] = sources.get(src, 0) + 1
    
    print(f"\n   Train: {train_count:,} samples ({train_duration:.1f} hours)")
    print(f"   Val:   {val_count:,} samples ({val_duration:.1f} hours)")
    print(f"\n   By source:")
    for src, count in sorted(sources.items()):
        print(f"      {src}: {count:,}")
    
    print(f"\n   Saved: {train_path}")
    print(f"   Saved: {val_path}")
    
    return train_count, val_count


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Prepare Ukrainian TTS datasets (verified January 2026)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Presets:
  --quality   OpenTTS only (~1 GB, 24h) - best quality, limited data
  --medium    + UK-Pods (~6 GB, 75h) - good balance
  --large     + Broadcast (~40 GB, 375h) - lots of data
  --full      + VoA (~55 GB, 766h) - maximum data

Examples:
  python scripts/prepare_data.py --quality
  python scripts/prepare_data.py --medium --sample-rate 22050
        """
    )
    
    # Presets
    preset_group = parser.add_mutually_exclusive_group(required=True)
    preset_group.add_argument("--quality", action="store_true", help="OpenTTS only (~1GB, 24h)")
    preset_group.add_argument("--medium", action="store_true", help="OpenTTS + UK-Pods (~6GB, 75h)")
    preset_group.add_argument("--large", action="store_true", help="+ Broadcast (~40GB, 375h)")
    preset_group.add_argument("--full", action="store_true", help="+ VoA (~55GB, 766h)")
    
    # Options
    parser.add_argument("--output", type=str, default="data", help="Output directory")
    parser.add_argument("--sample-rate", type=int, default=22050, help="Target sample rate")
    parser.add_argument("--voices", type=str, nargs="+", help="Specific OpenTTS voices")
    
    args = parser.parse_args()
    
    # Determine preset
    if args.quality:
        preset = "quality"
    elif args.medium:
        preset = "medium"
    elif args.large:
        preset = "large"
    else:
        preset = "full"
    
    print_banner()
    print(f"   Preset: {preset}")
    print(f"   Output: {args.output}")
    print(f"   Sample rate: {args.sample_rate} Hz")
    print(f"   Estimated disk: {estimate_disk_space(preset)}")
    print(f"   Estimated hours: {estimate_hours(preset)}")
    print()
    
    check_dependencies()
    
    output_dir = Path(args.output).resolve()  # Use absolute path
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_entries = []
    
    # Always download OpenTTS
    entries = download_opentts(output_dir, args.sample_rate, args.voices)
    all_entries.extend(entries)
    
    # Medium+ includes UK-Pods
    if preset in ["medium", "large", "full"]:
        entries = download_ukpods(output_dir, args.sample_rate)
        all_entries.extend(entries)
    
    # Large+ includes Broadcast
    if preset in ["large", "full"]:
        entries = download_broadcast(output_dir, args.sample_rate)
        all_entries.extend(entries)
    
    # Full includes VoA
    if preset == "full":
        entries = download_voa(output_dir, args.sample_rate)
        all_entries.extend(entries)
    
    # Create manifests
    if all_entries:
        train_count, val_count = create_manifests(all_entries, output_dir)
        
        print("\n" + "="*70)
        print("‚úÖ DATA PREPARATION COMPLETE!")
        print("="*70)
        print(f"   Total samples: {train_count + val_count:,}")
        print(f"   Train: {train_count:,}")
        print(f"   Val: {val_count:,}")
        print(f"\n   Next step:")
        print(f"   ./scripts/train_22khz_optimal.sh")
        print("="*70 + "\n")
    else:
        print("\n‚ùå No data downloaded!")
        sys.exit(1)


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: scripts/preprocess.py
–†–û–ó–ú–Ü–†: 2 KB
==================================================

#!/usr/bin/env python3
"""
Preprocess Script
–ì–µ–Ω–µ—Ä—É—î mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–∏ —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î —ó—Ö –Ω–∞ –¥–∏—Å–∫ –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è.
"""
import os
import sys
import argparse
import json
import torch
import torchaudio
from pathlib import Path
from tqdm import tqdm

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.data.preprocessing import AudioProcessor

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config/a100_optimized.yaml")
    parser.add_argument("--manifest-dir", default="data/manifests")
    parser.add_argument("--output-dir", default="data/processed")
    args = parser.parse_args()
    
    # Load manifest
    manifest_path = Path(args.manifest_dir) / "train.json"
    if not manifest_path.exists():
        print(f"Manifest not found: {manifest_path}")
        return

    with open(manifest_path) as f:
        data = json.load(f)
        
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Init processor
    processor = AudioProcessor(
        sample_rate=44100,
        n_mels=228,
        n_fft=2048,
        hop_length=512
    )
    
    print(f"üöÄ Preprocessing {len(data)} files...")
    
    for item in tqdm(data):
        audio_path = item["audio_path"]
        file_id = Path(audio_path).stem
        save_path = output_dir / f"{file_id}.pt"
        
        if save_path.exists():
            continue
            
        try:
            # Load & Compute Mel
            audio = processor.load(audio_path)
            mel = processor.compute_mel(audio)
            
            # Save compressed
            torch.save(mel.clone(), save_path)
            
        except Exception as e:
            # print(f"Error processing {audio_path}: {e}")
            pass
            
    print("‚úÖ Preprocessing complete!")

if __name__ == "__main__":
    main()


==================================================
–§–ê–ô–õ: scripts/test_checkpoint.py
–†–û–ó–ú–Ü–†: 34.66 KB
==================================================

#!/usr/bin/env python3
"""
SUPERTONIC V2 - INTERACTIVE CHECKPOINT TESTER
==============================================
–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤ autoencoder.

–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:
  - –í–∏–±—ñ—Ä —á–µ–∫–ø–æ—ñ–Ω—Ç–∞
  - Reconstruction –∞—É–¥—ñ–æ
  - –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —è–∫–æ—Å—Ç—ñ (PESQ, STOI, —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–∏)
  - A/B —Ç–µ—Å—Ç —Ä—ñ–∑–Ω–∏—Ö —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤
  - –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

Usage:
    python scripts/test_checkpoint.py
    python scripts/test_checkpoint.py --checkpoint checkpoints/autoencoder/step_5000.pt
    python scripts/test_checkpoint.py --audio test.wav
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional, Tuple, List
import json
from datetime import datetime

import torch
import torchaudio
import numpy as np

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from omegaconf import OmegaConf

from supertonic.data.preprocessing import AudioProcessor

# Optional quality metrics
try:
    from pesq import pesq
    PESQ_AVAILABLE = True
except ImportError:
    PESQ_AVAILABLE = False
    
try:
    from pystoi import stoi
    STOI_AVAILABLE = True
except ImportError:
    STOI_AVAILABLE = False

try:
    import matplotlib
    matplotlib.use('Agg')  # Non-interactive backend
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False


def load_checkpoint(checkpoint_path: str, device: str = "cuda") -> Tuple:
    """Load encoder and decoder from checkpoint."""
    from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder
    
    print(f"\nüì¶ Loading checkpoint: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # Get config from checkpoint or use defaults
    config = checkpoint.get("config", {})
    
    # Get audio params from config
    audio_config = config.get("audio", {})
    n_fft = audio_config.get("n_fft", 1024)
    hop_length = audio_config.get("hop_length", 256)
    n_mels = audio_config.get("n_mels", 100)
    sample_rate = audio_config.get("sample_rate", 22050)
    
    # CRITICAL: Detect actual parameters from checkpoint weights (config might be wrong due to bug)
    
    # Detect n_mels from encoder input_conv weight shape: [hidden, n_mels, 1]
    if "encoder" in checkpoint:
        encoder_state = checkpoint["encoder"]
        if "input_conv.weight" in encoder_state:
            actual_n_mels = encoder_state["input_conv.weight"].shape[1]
            if actual_n_mels != n_mels:
                print(f"   ‚ö†Ô∏è  Config says n_mels={n_mels}, but weights have n_mels={actual_n_mels}")
                n_mels = actual_n_mels
    
    # Detect n_fft from decoder istft_head.window shape
    if "decoder" in checkpoint:
        decoder_state = checkpoint["decoder"]
        if "istft_head.window" in decoder_state:
            actual_n_fft = decoder_state["istft_head.window"].shape[0]
            if actual_n_fft != n_fft:
                print(f"   ‚ö†Ô∏è  Config says n_fft={n_fft}, but weights have n_fft={actual_n_fft}")
                n_fft = actual_n_fft
                # Adjust hop_length proportionally
                if actual_n_fft == 2048:
                    hop_length = 512
                    sample_rate = 44100
                elif actual_n_fft == 1024:
                    hop_length = 256
                    sample_rate = 22050
    
    print(f"   ‚ö†Ô∏è  Using detected params: n_fft={n_fft}, hop={hop_length}, mels={n_mels}, sr={sample_rate}")
    print(f"   Audio config: n_fft={n_fft}, hop={hop_length}, mels={n_mels}, sr={sample_rate}")
    
    # Default architecture params
    encoder_params = {
        "input_dim": n_mels,
        "hidden_dim": 512,
        "output_dim": 24,
        "num_blocks": 10,
        "kernel_size": 7,
    }
    
    decoder_params = {
        "input_dim": 24,
        "hidden_dim": 512,
        "num_blocks": 10,
        "kernel_size": 7,
        "dilations": [1, 2, 4, 1, 2, 4, 1, 1, 1, 1],
        "causal": True,
        "n_fft": n_fft,
        "hop_length": hop_length,
    }
    
    # Override with config if available (but preserve n_fft/hop_length we detected from weights!)
    detected_n_fft = n_fft
    detected_hop = hop_length
    detected_mels = n_mels
    
    if "model" in config and "autoencoder" in config["model"]:
        ae_config = config["model"]["autoencoder"]
        if "encoder" in ae_config:
            encoder_params.update(ae_config["encoder"])
        if "decoder" in ae_config:
            for k, v in ae_config["decoder"].items():
                decoder_params[k] = v
    
    # Force use detected values (they come from actual weights, not buggy config)
    encoder_params["input_dim"] = detected_mels
    decoder_params["n_fft"] = detected_n_fft
    decoder_params["hop_length"] = detected_hop
    
    print(f"   Decoder params: n_fft={decoder_params.get('n_fft')}, hop={decoder_params.get('hop_length')}")
    
    # Create models
    encoder = LatentEncoder(**encoder_params).to(device)
    decoder = LatentDecoder(**decoder_params).to(device)
    
    # Helper to strip DDP "module." prefix from state dict
    def strip_ddp_prefix(state_dict):
        """Remove 'module.' prefix added by DistributedDataParallel."""
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[7:]] = v  # Remove "module." (7 chars)
            else:
                new_state_dict[k] = v
        return new_state_dict
    
    # Load weights (handle DDP prefix)
    encoder.load_state_dict(strip_ddp_prefix(checkpoint["encoder"]))
    decoder.load_state_dict(strip_ddp_prefix(checkpoint["decoder"]))
    
    encoder.eval()
    decoder.eval()
    
    # Try both "iteration" (new) and "step" (legacy) keys
    step = checkpoint.get("iteration", checkpoint.get("step", 0))
    print(f"   ‚úÖ Loaded step {step:,}")
    
    # Update config with detected values (for use in reconstruction)
    config["audio"] = {
        "n_fft": detected_n_fft,
        "hop_length": detected_hop,
        "n_mels": detected_mels,
        "sample_rate": sample_rate,
    }
    
    return encoder, decoder, step, config


def load_audio(audio_path: str, target_sr: int = 22050) -> torch.Tensor:
    """Load and resample audio."""
    audio, sr = torchaudio.load(audio_path)
    
    # Convert to mono
    if audio.shape[0] > 1:
        audio = audio.mean(dim=0, keepdim=True)
    
    # Resample
    if sr != target_sr:
        resampler = torchaudio.transforms.Resample(sr, target_sr)
        audio = resampler(audio)
    
    return audio.squeeze(0)


def compute_mel(audio: torch.Tensor, 
                sample_rate: int = 22050,
                n_fft: int = 1024,
                hop_length: int = 256,
                n_mels: int = 100) -> torch.Tensor:
    """Compute mel spectrogram using AudioProcessor (same as training!)."""
    audio_processor = AudioProcessor(
        sample_rate=sample_rate,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels,
        fmin=0.0,
        fmax=sample_rate / 2
    )
    
    return audio_processor.compute_mel(audio, log_scale=True)


def reconstruct_audio(encoder, decoder, audio: torch.Tensor, 
                      device: str = "cuda",
                      sample_rate: int = 22050,
                      n_fft: int = 1024,
                      hop_length: int = 256,
                      n_mels: int = 100) -> torch.Tensor:
    """Reconstruct audio through autoencoder."""
    with torch.no_grad():
        # Compute mel
        mel = compute_mel(audio, sample_rate=sample_rate, n_fft=n_fft, 
                         hop_length=hop_length, n_mels=n_mels)
        mel = mel.unsqueeze(0).to(device)  # [1, n_mels, T]
        
        # Encode
        latent = encoder(mel)  # [1, latent_dim, T']
        
        # Decode
        reconstructed = decoder(latent)  # [1, T]
        
    return reconstructed.squeeze(0).cpu()


def compute_metrics(original: torch.Tensor, 
                    reconstructed: torch.Tensor,
                    sample_rate: int = 22050) -> dict:
    """Compute quality metrics."""
    metrics = {}
    
    # Ensure same length
    min_len = min(len(original), len(reconstructed))
    original = original[:min_len]
    reconstructed = reconstructed[:min_len]
    
    # Convert to numpy
    orig_np = original.numpy()
    recon_np = reconstructed.numpy()
    
    # L1 loss
    metrics["l1_loss"] = float(np.mean(np.abs(orig_np - recon_np)))
    
    # MSE
    metrics["mse"] = float(np.mean((orig_np - recon_np) ** 2))
    
    # SNR
    signal_power = np.mean(orig_np ** 2)
    noise_power = np.mean((orig_np - recon_np) ** 2)
    if noise_power > 0:
        metrics["snr_db"] = float(10 * np.log10(signal_power / noise_power))
    else:
        metrics["snr_db"] = float("inf")
    
    # PESQ (if available)
    if PESQ_AVAILABLE:
        try:
            # PESQ requires 16kHz or 8kHz
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                orig_16k = resampler(original).numpy()
                recon_16k = resampler(reconstructed).numpy()
            else:
                orig_16k = orig_np
                recon_16k = recon_np
            
            metrics["pesq"] = float(pesq(16000, orig_16k, recon_16k, 'wb'))
        except Exception as e:
            metrics["pesq"] = f"Error: {e}"
    
    # STOI (if available)
    if STOI_AVAILABLE:
        try:
            metrics["stoi"] = float(stoi(orig_np, recon_np, sample_rate, extended=False))
        except Exception as e:
            metrics["stoi"] = f"Error: {e}"
    
    # Mel spectrogram loss
    orig_mel = compute_mel(original, sample_rate=sample_rate)
    recon_mel = compute_mel(reconstructed, sample_rate=sample_rate)
    min_mel_len = min(orig_mel.shape[1], recon_mel.shape[1])
    metrics["mel_l1"] = float(torch.mean(torch.abs(
        orig_mel[:, :min_mel_len] - recon_mel[:, :min_mel_len]
    )).item())
    
    return metrics


def plot_comparison(original: torch.Tensor,
                    reconstructed: torch.Tensor,
                    save_path: str,
                    sample_rate: int = 22050,
                    title: str = ""):
    """Plot waveform and spectrogram comparison."""
    if not MATPLOTLIB_AVAILABLE:
        print("   ‚ö†Ô∏è  matplotlib not available, skipping plot")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 8))
    
    # Waveforms
    time_orig = np.arange(len(original)) / sample_rate
    time_recon = np.arange(len(reconstructed)) / sample_rate
    
    axes[0, 0].plot(time_orig, original.numpy(), alpha=0.7, linewidth=0.5)
    axes[0, 0].set_title("Original Waveform")
    axes[0, 0].set_xlabel("Time (s)")
    axes[0, 0].set_ylabel("Amplitude")
    axes[0, 0].set_ylim(-1, 1)
    
    axes[0, 1].plot(time_recon, reconstructed.numpy(), alpha=0.7, linewidth=0.5, color='orange')
    axes[0, 1].set_title("Reconstructed Waveform")
    axes[0, 1].set_xlabel("Time (s)")
    axes[0, 1].set_ylabel("Amplitude")
    axes[0, 1].set_ylim(-1, 1)
    
    # Spectrograms
    orig_mel = compute_mel(original, sample_rate=sample_rate).numpy()
    recon_mel = compute_mel(reconstructed, sample_rate=sample_rate).numpy()
    
    im1 = axes[1, 0].imshow(orig_mel, aspect='auto', origin='lower', cmap='magma')
    axes[1, 0].set_title("Original Mel Spectrogram")
    axes[1, 0].set_xlabel("Time")
    axes[1, 0].set_ylabel("Mel bin")
    plt.colorbar(im1, ax=axes[1, 0])
    
    im2 = axes[1, 1].imshow(recon_mel, aspect='auto', origin='lower', cmap='magma')
    axes[1, 1].set_title("Reconstructed Mel Spectrogram")
    axes[1, 1].set_xlabel("Time")
    axes[1, 1].set_ylabel("Mel bin")
    plt.colorbar(im2, ax=axes[1, 1])
    
    if title:
        fig.suptitle(title, fontsize=14)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.close()
    print(f"   üìä Saved plot: {save_path}")


def list_checkpoints(checkpoint_dir: str = "checkpoints/autoencoder") -> List[str]:
    """List available checkpoints, sorted by step number."""
    checkpoint_dir = Path(checkpoint_dir)
    if not checkpoint_dir.exists():
        return []
    
    checkpoints = list(checkpoint_dir.glob("*.pt"))
    
    # Sort by step number (extract number from filename)
    def get_step(path):
        name = path.stem  # checkpoint_5000 -> checkpoint_5000
        parts = name.split("_")
        for p in reversed(parts):
            if p.isdigit():
                return int(p)
        return 0
    
    checkpoints = sorted(checkpoints, key=get_step)
    return [str(cp) for cp in checkpoints]


def list_audio_files(data_dir: str = "data", limit: int = 20) -> List[str]:
    """List some audio files for testing."""
    data_dir = Path(data_dir)
    audio_files = []
    
    for ext in ["*.wav", "*.mp3", "*.flac"]:
        for f in data_dir.rglob(ext):
            audio_files.append(str(f))
            if len(audio_files) >= limit:
                break
        if len(audio_files) >= limit:
            break
    
    return audio_files


def discover_datasets(data_dir: str = "data/audio") -> dict:
    """
    Discover available datasets and speakers.
    Returns: {dataset_name: [speaker_dirs] or None if flat}
    """
    base_dir = Path(data_dir)
    if not base_dir.exists():
        # Try alternative path
        base_dir = Path("data/audio")
        if not base_dir.exists():
            return {}
    
    datasets = {}
    
    for item in base_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.'):
            # Check if this dataset has speaker subdirectories
            subdirs = [d for d in item.iterdir() if d.is_dir() and not d.name.startswith('.')]
            
            # Check if subdirs contain audio or are speaker folders
            if subdirs:
                # Check first subdir for audio files
                first_subdir = subdirs[0]
                has_audio = any(first_subdir.glob("*.wav")) or any(first_subdir.glob("*.flac"))
                if has_audio:
                    # These are speaker directories
                    datasets[item.name] = [d.name for d in sorted(subdirs)]
                else:
                    # Nested structure, treat as flat
                    datasets[item.name] = None
            else:
                # Flat dataset (audio files directly in folder)
                datasets[item.name] = None
    
    return datasets


def list_audio_interactive(data_dir: str = "data/audio", limit: int = 40) -> Optional[str]:
    """
    Interactive audio file selection with dataset/speaker hierarchy.
    Returns selected audio path or None.
    """
    datasets = discover_datasets(data_dir)
    
    if not datasets:
        print("   ‚ùå No datasets found!")
        return None
    
    # Step 1: Select dataset
    print("\n   üìÅ AVAILABLE DATASETS:")
    dataset_list = sorted(datasets.keys())
    for i, ds in enumerate(dataset_list):
        speakers = datasets[ds]
        if speakers:
            print(f"      [{i}] {ds}/ ({len(speakers)} speakers: {', '.join(speakers[:3])}{'...' if len(speakers) > 3 else ''})")
        else:
            print(f"      [{i}] {ds}/")
    
    ds_input = input("\n   Select dataset [number]: ").strip()
    if not ds_input.isdigit() or int(ds_input) >= len(dataset_list):
        print("   ‚ùå Invalid selection")
        return None
    
    selected_dataset = dataset_list[int(ds_input)]
    speakers = datasets[selected_dataset]
    dataset_path = Path(data_dir) / selected_dataset
    
    # Step 2: Select speaker (if applicable)
    if speakers:
        print(f"\n   üé§ SPEAKERS in {selected_dataset}:")
        for i, sp in enumerate(speakers):
            # Count files
            sp_path = dataset_path / sp
            file_count = len(list(sp_path.glob("*.wav")))
            print(f"      [{i}] {sp} ({file_count} files)")
        
        sp_input = input("\n   Select speaker [number]: ").strip()
        if not sp_input.isdigit() or int(sp_input) >= len(speakers):
            print("   ‚ùå Invalid selection")
            return None
        
        selected_speaker = speakers[int(sp_input)]
        audio_dir = dataset_path / selected_speaker
    else:
        audio_dir = dataset_path
    
    # Step 3: List and select audio files
    audio_files = sorted(audio_dir.glob("*.wav"))[:limit]
    
    if not audio_files:
        # Try flac
        audio_files = sorted(audio_dir.glob("*.flac"))[:limit]
    
    if not audio_files:
        print(f"   ‚ùå No audio files found in {audio_dir}")
        return None
    
    print(f"\n   üéµ AUDIO FILES ({len(audio_files)} shown, limit={limit}):")
    for i, af in enumerate(audio_files):
        print(f"      [{i}] {af.name}")
    
    # Option for random selection
    print(f"\n      [r] Random file from this folder")
    print(f"      [q] Cancel")
    
    file_input = input("\n   Select file [number/r/q]: ").strip().lower()
    
    if file_input == 'q':
        return None
    elif file_input == 'r':
        import random
        # Get all files, not just first N
        all_files = list(audio_dir.glob("*.wav"))
        if not all_files:
            all_files = list(audio_dir.glob("*.flac"))
        selected = random.choice(all_files)
        print(f"   üé≤ Random: {selected.name}")
        return str(selected)
    elif file_input.isdigit() and int(file_input) < len(audio_files):
        return str(audio_files[int(file_input)])
    else:
        print("   ‚ùå Invalid selection")
        return None


def interactive_menu():
    """Interactive menu for checkpoint testing."""
    print("\n" + "="*70)
    print("üéµ SUPERTONIC V2 - CHECKPOINT TESTER")
    print("="*70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Device: {device}")
    
    # Audio params (will be updated from checkpoint config)
    audio_params = {
        "sample_rate": 22050,
        "n_fft": 1024,
        "hop_length": 256,
        "n_mels": 100,
    }
    
    encoder = None
    decoder = None
    current_step = 0
    
    output_dir = Path("test_outputs")
    output_dir.mkdir(exist_ok=True)
    
    while True:
        print("\n" + "-"*50)
        print("üìã MENU:")
        print("   1. List checkpoints")
        print("   2. Load checkpoint")
        print("   3. Browse audio (dataset ‚Üí speaker ‚Üí file)")
        print("   4. Test reconstruction")
        print("   5. Compare two checkpoints")
        print("   6. Batch test (multiple files)")
        print("   0. Exit")
        print("-"*50)
        
        choice = input("Choose option: ").strip()
        
        if choice == "0":
            print("\nüëã Bye!")
            break
            
        elif choice == "1":
            checkpoints = list_checkpoints()
            if not checkpoints:
                print("   ‚ùå No checkpoints found in checkpoints/autoencoder/")
            else:
                print(f"\n   üì¶ Found {len(checkpoints)} checkpoints:")
                for i, cp in enumerate(checkpoints):
                    print(f"      [{i}] {Path(cp).name}")
                    
        elif choice == "2":
            checkpoints = list_checkpoints()
            if not checkpoints:
                print("   ‚ùå No checkpoints found")
                continue
                
            print(f"\n   Available checkpoints:")
            for i, cp in enumerate(checkpoints):
                print(f"      [{i}] {Path(cp).name}")
            
            idx = input("   Enter index or path: ").strip()
            
            try:
                if idx.isdigit():
                    cp_path = checkpoints[int(idx)]
                else:
                    cp_path = idx
                    
                encoder, decoder, current_step, config = load_checkpoint(cp_path, device)
                
                # Update audio params from config
                if config and "audio" in config:
                    audio_params.update({
                        "sample_rate": config["audio"].get("sample_rate", 22050),
                        "n_fft": config["audio"].get("n_fft", 1024),
                        "hop_length": config["audio"].get("hop_length", 256),
                        "n_mels": config["audio"].get("n_mels", 100),
                    })
                    print(f"   üìä Audio params: {audio_params}")
                
                print(f"   ‚úÖ Loaded checkpoint at step {current_step:,}")
            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                import traceback
                traceback.print_exc()
                
        elif choice == "3":
            # Interactive dataset/speaker/file selection
            selected_audio = list_audio_interactive(limit=40)
            if selected_audio:
                # Store for use in option 4
                last_selected_audio = selected_audio
                print(f"\n   ‚úÖ Selected: {selected_audio}")
                print(f"   üí° Use option 4 to test reconstruction, or enter path directly")
                    
        elif choice == "4":
            if encoder is None:
                print("   ‚ùå Load a checkpoint first (option 2)")
                continue
            
            print("\n   Options:")
            print("      [i] Interactive selection (dataset ‚Üí speaker ‚Üí file)")
            print("      [p] Enter path directly")
            audio_mode = input("   Choose [i/p]: ").strip().lower()
            
            if audio_mode == 'i':
                audio_path = list_audio_interactive(limit=40)
                if not audio_path:
                    continue
            else:
                audio_path = input("   Enter audio path: ").strip()
            
            try:
                
                sample_rate = audio_params["sample_rate"]
                
                print(f"\n   üéµ Loading: {audio_path}")
                audio = load_audio(audio_path, target_sr=sample_rate)
                print(f"   Duration: {len(audio)/sample_rate:.2f}s")
                
                # Limit length for testing
                max_samples = sample_rate * 10  # 10 seconds max
                if len(audio) > max_samples:
                    audio = audio[:max_samples]
                    print(f"   Truncated to {max_samples/sample_rate:.1f}s")
                
                print("\n   üîÑ Reconstructing...")
                reconstructed = reconstruct_audio(
                    encoder, decoder, audio, device, 
                    sample_rate=audio_params["sample_rate"],
                    n_fft=audio_params["n_fft"],
                    hop_length=audio_params["hop_length"],
                    n_mels=audio_params["n_mels"]
                )
                
                print("\n   üìä Computing metrics...")
                metrics = compute_metrics(audio, reconstructed, sample_rate)
                
                print("\n   Results:")
                print(f"      L1 Loss:     {metrics['l1_loss']:.6f}")
                print(f"      MSE:         {metrics['mse']:.6f}")
                print(f"      SNR:         {metrics['snr_db']:.2f} dB")
                print(f"      Mel L1:      {metrics['mel_l1']:.4f}")
                if 'pesq' in metrics:
                    print(f"      PESQ:        {metrics['pesq']}")
                if 'stoi' in metrics:
                    print(f"      STOI:        {metrics['stoi']}")
                
                # Save outputs
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_name = Path(audio_path).stem
                
                # Save audio
                orig_path = output_dir / f"{base_name}_step{current_step}_original.wav"
                recon_path = output_dir / f"{base_name}_step{current_step}_reconstructed.wav"
                
                torchaudio.save(str(orig_path), audio.unsqueeze(0), sample_rate)
                torchaudio.save(str(recon_path), reconstructed.unsqueeze(0), sample_rate)
                
                print(f"\n   üíæ Saved:")
                print(f"      {orig_path}")
                print(f"      {recon_path}")
                
                # Plot
                plot_path = output_dir / f"{base_name}_step{current_step}_comparison.png"
                plot_comparison(audio, reconstructed, str(plot_path), sample_rate, 
                              f"Step {current_step:,} - {base_name}")
                
                # Save metrics
                metrics_path = output_dir / f"{base_name}_step{current_step}_metrics.json"
                with open(metrics_path, "w") as f:
                    json.dump({
                        "audio_path": audio_path,
                        "step": current_step,
                        "metrics": metrics,
                        "timestamp": timestamp,
                    }, f, indent=2)
                print(f"      {metrics_path}")
                
            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                import traceback
                traceback.print_exc()
                
        elif choice == "5":
            print("\n   üîÑ Compare two checkpoints")
            checkpoints = list_checkpoints()
            
            if len(checkpoints) < 2:
                print("   ‚ùå Need at least 2 checkpoints")
                continue
            
            print("   Available checkpoints:")
            for i, cp in enumerate(checkpoints):
                print(f"      [{i}] {Path(cp).name}")
            
            try:
                idx1 = int(input("   First checkpoint index: ").strip())
                idx2 = int(input("   Second checkpoint index: ").strip())
                audio_idx = input("   Audio file (path or index): ").strip()
                
                if audio_idx.isdigit():
                    audio_files = list_audio_files()
                    audio_path = audio_files[int(audio_idx)]
                else:
                    audio_path = audio_idx
                
                results = []
                for idx in [idx1, idx2]:
                    cp_path = checkpoints[idx]
                    enc, dec, step, config = load_checkpoint(cp_path, device)
                    
                    # Get audio params from this checkpoint's config
                    ap = {
                        "sample_rate": config.get("audio", {}).get("sample_rate", 22050),
                        "n_fft": config.get("audio", {}).get("n_fft", 1024),
                        "hop_length": config.get("audio", {}).get("hop_length", 256),
                        "n_mels": config.get("audio", {}).get("n_mels", 100),
                    }
                    
                    # Load audio with this checkpoint's sample rate
                    audio = load_audio(audio_path, target_sr=ap["sample_rate"])
                    max_samples = ap["sample_rate"] * 10
                    if len(audio) > max_samples:
                        audio = audio[:max_samples]
                    
                    recon = reconstruct_audio(enc, dec, audio, device, **ap)
                    metrics = compute_metrics(audio, recon, ap["sample_rate"])
                    results.append({
                        "step": step,
                        "path": cp_path,
                        "metrics": metrics,
                        "reconstructed": recon,
                        "sample_rate": ap["sample_rate"],
                    })
                
                print("\n   üìä Comparison:")
                print(f"   {'Metric':<15} {'Step '+str(results[0]['step']):<15} {'Step '+str(results[1]['step']):<15} {'Better':<10}")
                print("   " + "-"*55)
                
                for metric in ["l1_loss", "mse", "mel_l1"]:
                    v1 = results[0]["metrics"][metric]
                    v2 = results[1]["metrics"][metric]
                    better = "‚Üê" if v1 < v2 else "‚Üí"
                    print(f"   {metric:<15} {v1:<15.6f} {v2:<15.6f} {better:<10}")
                
                for metric in ["snr_db"]:
                    if metric in results[0]["metrics"]:
                        v1 = results[0]["metrics"][metric]
                        v2 = results[1]["metrics"][metric]
                        better = "‚Üê" if v1 > v2 else "‚Üí"
                        print(f"   {metric:<15} {v1:<15.2f} {v2:<15.2f} {better:<10}")
                
                # Save both
                base_name = Path(audio_path).stem
                for r in results:
                    sr = r.get("sample_rate", 22050)
                    recon_path = output_dir / f"{base_name}_step{r['step']}_reconstructed.wav"
                    torchaudio.save(str(recon_path), r['reconstructed'].unsqueeze(0), sr)
                    print(f"\n   üíæ Saved: {recon_path}")
                
            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                import traceback
                traceback.print_exc()
                
        elif choice == "6":
            if encoder is None:
                print("   ‚ùå Load a checkpoint first (option 2)")
                continue
            
            audio_files = list_audio_files(limit=50)
            n = input(f"   How many files to test? (max {len(audio_files)}): ").strip()
            n = min(int(n), len(audio_files))
            
            sample_rate = audio_params["sample_rate"]
            
            all_metrics = []
            for i, af in enumerate(audio_files[:n]):
                print(f"\n   [{i+1}/{n}] {Path(af).name}")
                try:
                    audio = load_audio(af, target_sr=sample_rate)
                    max_samples = sample_rate * 10
                    if len(audio) > max_samples:
                        audio = audio[:max_samples]
                    
                    recon = reconstruct_audio(
                        encoder, decoder, audio, device,
                        sample_rate=audio_params["sample_rate"],
                        n_fft=audio_params["n_fft"],
                        hop_length=audio_params["hop_length"],
                        n_mels=audio_params["n_mels"]
                    )
                    metrics = compute_metrics(audio, recon, sample_rate)
                    metrics["file"] = af
                    all_metrics.append(metrics)
                    
                    print(f"      L1: {metrics['l1_loss']:.6f}, Mel L1: {metrics['mel_l1']:.4f}, SNR: {metrics['snr_db']:.1f}dB")
                except Exception as e:
                    print(f"      ‚ùå Error: {e}")
            
            # Summary
            if all_metrics:
                print("\n   üìä SUMMARY:")
                avg_l1 = np.mean([m["l1_loss"] for m in all_metrics])
                avg_mel = np.mean([m["mel_l1"] for m in all_metrics])
                avg_snr = np.mean([m["snr_db"] for m in all_metrics])
                
                print(f"      Avg L1 Loss:  {avg_l1:.6f}")
                print(f"      Avg Mel L1:   {avg_mel:.4f}")
                print(f"      Avg SNR:      {avg_snr:.2f} dB")
                
                # Save summary
                summary_path = output_dir / f"batch_test_step{current_step}.json"
                with open(summary_path, "w") as f:
                    json.dump({
                        "step": current_step,
                        "n_files": n,
                        "avg_l1_loss": avg_l1,
                        "avg_mel_l1": avg_mel,
                        "avg_snr_db": avg_snr,
                        "files": all_metrics,
                    }, f, indent=2)
                print(f"\n   üíæ Saved: {summary_path}")
        
        else:
            print("   ‚ùå Unknown option")


def main():
    parser = argparse.ArgumentParser(description="Test autoencoder checkpoints")
    parser.add_argument("--checkpoint", type=str, help="Checkpoint path")
    parser.add_argument("--audio", type=str, help="Audio file to test")
    parser.add_argument("--output", type=str, default="test_outputs", help="Output directory")
    parser.add_argument("--interactive", "-i", action="store_true", help="Interactive mode")
    
    args = parser.parse_args()
    
    # If no args or interactive flag, run interactive mode
    if args.interactive or (args.checkpoint is None and args.audio is None):
        interactive_menu()
        return
    
    # Quick test mode
    if args.checkpoint and args.audio:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        output_dir = Path(args.output)
        output_dir.mkdir(exist_ok=True)
        
        encoder, decoder, step, config = load_checkpoint(args.checkpoint, device)
        
        # Get audio params from config
        audio_params = {
            "sample_rate": config.get("audio", {}).get("sample_rate", 22050),
            "n_fft": config.get("audio", {}).get("n_fft", 1024),
            "hop_length": config.get("audio", {}).get("hop_length", 256),
            "n_mels": config.get("audio", {}).get("n_mels", 100),
        }
        sample_rate = audio_params["sample_rate"]
        
        print(f"\nüéµ Loading: {args.audio}")
        audio = load_audio(args.audio, target_sr=sample_rate)
        
        print("üîÑ Reconstructing...")
        reconstructed = reconstruct_audio(encoder, decoder, audio, device, **audio_params)
        
        print("üìä Computing metrics...")
        metrics = compute_metrics(audio, reconstructed, sample_rate)
        
        print("\nüìä Results:")
        for k, v in metrics.items():
            if isinstance(v, float):
                print(f"   {k}: {v:.6f}")
            else:
                print(f"   {k}: {v}")
        
        # Save
        base_name = Path(args.audio).stem
        recon_path = output_dir / f"{base_name}_step{step}_reconstructed.wav"
        torchaudio.save(str(recon_path), reconstructed.unsqueeze(0), sample_rate)
        print(f"\nüíæ Saved: {recon_path}")
        
        # Plot
        plot_path = output_dir / f"{base_name}_step{step}_comparison.png"
        plot_comparison(audio, reconstructed, str(plot_path), sample_rate, f"Step {step:,}")


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: scripts/test_stage2_checkpoint.py
–†–û–ó–ú–Ü–†: 20 KB
==================================================

#!/usr/bin/env python3
"""
SUPERTONIC V2 - STAGE 2 (TEXT-TO-LATENT) CHECKPOINT TESTER
===========================================================
–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è Stage 2 —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤.

–ë–µ–∑ Duration Predictor (Stage 3) - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ ground truth durations.

–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:
  - Synthesis –∑ —Ç–µ–∫—Å—Ç—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ reference audio
  - Ground truth duration alignment (–±–µ–∑ Stage 3)
  - –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª–æ–º
  - A/B —Ç–µ—Å—Ç —Ä—ñ–∑–Ω–∏—Ö —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤

Usage:
    python scripts/test_stage2_checkpoint.py
    python scripts/test_stage2_checkpoint.py --checkpoint outputs/text_to_latent/checkpoints/checkpoint_100000.pt
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional, Tuple, List, Dict
import json
import random

import torch
import torch.nn.functional as F
import torchaudio
import numpy as np

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from omegaconf import OmegaConf

from supertonic.models.speech_autoencoder import LatentEncoder, LatentDecoder
from supertonic.models.text_to_latent import TextToLatent
from supertonic.losses.flow_matching_loss import compress_latents, decompress_latents
from supertonic.data.preprocessing import AudioProcessor
from supertonic.data.tokenizer import CharacterTokenizer

try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False


def strip_ddp_prefix(state_dict: dict) -> dict:
    """Remove 'module.' prefix from DDP state dict."""
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith("module."):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v
    return new_state_dict


def load_autoencoder(checkpoint_path: str, config: dict, device: str = "cuda") -> Tuple:
    """Load autoencoder (encoder + decoder) from Stage 1 checkpoint."""
    print(f"\nüì¶ Loading autoencoder: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # Get encoder config
    ae_config = config.get("model", {}).get("autoencoder", config.get("autoencoder", {}))
    enc_config = ae_config.get("encoder", {})
    dec_config = ae_config.get("decoder", {})
    
    # Create encoder
    encoder = LatentEncoder(
        input_dim=enc_config.get("input_dim", 100),
        hidden_dim=enc_config.get("hidden_dim", 512),
        output_dim=enc_config.get("output_dim", 24),
        num_blocks=enc_config.get("num_blocks", 10),
        kernel_size=enc_config.get("kernel_size", 7),
    ).to(device)
    
    # Create decoder
    decoder = LatentDecoder(
        input_dim=dec_config.get("input_dim", 24),
        hidden_dim=dec_config.get("hidden_dim", 512),
        num_blocks=dec_config.get("num_blocks", 10),
        kernel_size=dec_config.get("kernel_size", 7),
        dilations=dec_config.get("dilations", [1, 2, 4, 1, 2, 4, 1, 1, 1, 1]),
        causal=dec_config.get("causal", True),
        n_fft=config.get("audio", {}).get("n_fft", 1024),
        hop_length=config.get("audio", {}).get("hop_length", 256),
        use_hifigan=dec_config.get("use_hifigan", True),
        upsample_rates=dec_config.get("upsample_rates", [8, 8, 2, 2]),
        upsample_kernel_sizes=dec_config.get("upsample_kernel_sizes", [16, 16, 4, 4]),
        upsample_initial_channel=dec_config.get("upsample_initial_channel", 512),
        resblock_kernel_sizes=dec_config.get("resblock_kernel_sizes", [3, 7, 11]),
        resblock_dilation_sizes=dec_config.get("resblock_dilation_sizes", [[1, 3, 5], [1, 3, 5], [1, 3, 5]]),
    ).to(device)
    
    # Load weights
    encoder.load_state_dict(strip_ddp_prefix(checkpoint["encoder"]))
    decoder.load_state_dict(strip_ddp_prefix(checkpoint["decoder"]))
    
    encoder.eval()
    decoder.eval()
    
    step = checkpoint.get("iteration", checkpoint.get("step", 0))
    print(f"   ‚úÖ Autoencoder loaded (step {step:,})")
    
    return encoder, decoder


def load_text_to_latent(checkpoint_path: str, config: dict, device: str = "cuda") -> TextToLatent:
    """Load Text-to-Latent model from Stage 2 checkpoint."""
    print(f"\nüì¶ Loading Text-to-Latent: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # Detect vocab_size from checkpoint weights
    state_dict = strip_ddp_prefix(checkpoint["model"])
    vocab_size = state_dict["text_encoder.char_embed.embedding.weight"].shape[0]
    print(f"   Detected vocab_size: {vocab_size}")
    
    # Get config
    ttl_config = config.get("model", {}).get("text_to_latent", config.get("text_to_latent", {}))
    ref_config = ttl_config.get("reference_encoder", {})
    text_config = ttl_config.get("text_encoder", {})
    vf_config = ttl_config.get("vector_field", {})
    
    # Create model with simplified interface
    model = TextToLatent(
        latent_dim=ref_config.get("input_dim", 144),
        vocab_size=vocab_size,  # Use detected vocab_size
        text_embed_dim=text_config.get("embed_dim", 128),
        text_hidden_dim=text_config.get("hidden_dim", 512),
        ref_hidden_dim=ref_config.get("hidden_dim", 128),
        vf_hidden_dim=vf_config.get("hidden_dim", 512),
        num_ref_vectors=ref_config.get("num_output_vectors", 50),
        sigma_min=config.get("flow_matching", {}).get("sigma_min", 1e-4),
        p_uncond=0.0,  # No dropout at inference
        cfg_scale=config.get("flow_matching", {}).get("cfg_scale", 3.0),
        gamma=config.get("larope", {}).get("gamma", 0.85),
    ).to(device)
    
    # Load weights
    model.load_state_dict(strip_ddp_prefix(checkpoint["model"]))
    model.eval()
    
    step = checkpoint.get("iteration", checkpoint.get("step", 0))
    print(f"   ‚úÖ Text-to-Latent loaded (step {step:,})")
    
    return model


@torch.no_grad()
def synthesize_with_gt_duration(
    text: str,
    reference_audio: torch.Tensor,
    target_audio: torch.Tensor,  # For ground truth duration
    text_to_latent: TextToLatent,
    latent_encoder: LatentEncoder,
    latent_decoder: LatentDecoder,
    tokenizer: CharacterTokenizer,
    audio_processor: AudioProcessor,
    device: str = "cuda",
    nfe: int = 32,
    cfg_scale: float = 3.0,
) -> Tuple[torch.Tensor, dict]:
    """
    Synthesize speech using ground truth duration (from target audio length).
    
    Returns:
        audio: Generated audio [T]
        info: Dict with timing and debug info
    """
    import time
    start_time = time.time()
    
    # 1. Tokenize text
    text_ids = tokenizer.encode(text)
    text_ids = torch.tensor(text_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, L]
    text_mask = torch.ones_like(text_ids, dtype=torch.bool)
    
    # 2. Compute reference mel and encode (mel computation on CPU, then move to GPU)
    if reference_audio.dim() == 1:
        reference_audio = reference_audio.unsqueeze(0)
    ref_mel = audio_processor.compute_mel(reference_audio.cpu(), log_scale=True)  # [1, n_mels, T_ref]
    ref_mel = ref_mel.to(device)
    
    # Encode reference to latent
    ref_latent = latent_encoder(ref_mel)  # [1, 24, T_ref_latent]
    ref_compressed = compress_latents(ref_latent, compression_factor=6)  # [1, 144, T_ref_compressed]
    
    # 3. Get target duration from target audio
    if target_audio.dim() == 1:
        target_audio = target_audio.unsqueeze(0)
    target_mel = audio_processor.compute_mel(target_audio.cpu(), log_scale=True)
    target_mel = target_mel.to(device)
    target_latent = latent_encoder(target_mel)
    target_compressed = compress_latents(target_latent, compression_factor=6)
    target_length = target_compressed.shape[2]  # This is our target output length
    
    # 4. Use model's built-in generate() method
    generated_compressed = text_to_latent.generate(
        text=text_ids,
        ref_latent=ref_compressed,
        num_frames=target_length,
        nfe=nfe,
        cfg_scale=cfg_scale,
        text_mask=text_mask,
    )
    
    # 5. Decompress and decode
    generated_latent = decompress_latents(generated_compressed, compression_factor=6)  # [1, 24, T*6]
    generated_audio = latent_decoder(generated_latent)  # [1, T_audio]
    
    end_time = time.time()
    
    info = {
        "text_length": text_ids.shape[1],
        "ref_length": ref_compressed.shape[2],
        "target_latent_length": target_length,
        "generated_audio_length": generated_audio.shape[1],
        "inference_time": end_time - start_time,
        "nfe": nfe,
        "cfg_scale": cfg_scale,
    }
    
    return generated_audio.squeeze(0).cpu(), info


def list_checkpoints(checkpoint_dir: str) -> List[str]:
    """List available checkpoints, sorted by step."""
    checkpoint_dir = Path(checkpoint_dir)
    if not checkpoint_dir.exists():
        return []
    
    checkpoints = list(checkpoint_dir.glob("checkpoint_*.pt"))
    
    def get_step(path):
        name = path.stem
        parts = name.split("_")
        for p in reversed(parts):
            if p.isdigit():
                return int(p)
        return 0
    
    return sorted([str(cp) for cp in checkpoints], key=lambda x: get_step(Path(x)))


def discover_test_samples(manifest_path: str, num_samples: int = 10, dataset_filter: str = None) -> List[dict]:
    """Get random samples from validation manifest.
    
    Args:
        manifest_path: Path to manifest JSON
        num_samples: Number of samples to return
        dataset_filter: Optional filter by dataset name (e.g., 'opentts' for Ukrainian)
    """
    with open(manifest_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # Support both JSON array and JSON Lines
    try:
        samples = json.loads(content)
        if not isinstance(samples, list):
            samples = [samples]
    except json.JSONDecodeError:
        samples = []
        for line in content.strip().split('\n'):
            if line.strip():
                try:
                    samples.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    # Filter by dataset if specified (do this first, before text filter)
    if dataset_filter:
        samples = [s for s in samples if dataset_filter.lower() in s.get("audio_path", "").lower()]
        print(f"   Filtered to {len(samples)} samples matching '{dataset_filter}'")
    
    # Filter samples with sufficient text (min 10 chars to avoid conv kernel issues)
    samples = [s for s in samples if s.get("text") and len(s.get("text", "")) >= 10]
    print(f"   Samples with text (>=10 chars): {len(samples)}")
    
    # Random sample
    if len(samples) > num_samples:
        samples = random.sample(samples, num_samples)
    
    return samples


def plot_comparison(
    original_mel: torch.Tensor,
    generated_mel: torch.Tensor,
    save_path: str,
    title: str = ""
):
    """Plot mel spectrogram comparison."""
    if not MATPLOTLIB_AVAILABLE:
        return
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 6))
    
    # Original
    im1 = axes[0].imshow(original_mel.numpy(), aspect='auto', origin='lower', cmap='magma')
    axes[0].set_title("Original (Ground Truth)")
    axes[0].set_ylabel("Mel bin")
    plt.colorbar(im1, ax=axes[0])
    
    # Generated
    im2 = axes[1].imshow(generated_mel.numpy(), aspect='auto', origin='lower', cmap='magma')
    axes[1].set_title("Generated (Stage 2)")
    axes[1].set_ylabel("Mel bin")
    axes[1].set_xlabel("Time")
    plt.colorbar(im2, ax=axes[1])
    
    if title:
        fig.suptitle(title, fontsize=12)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.close()
    print(f"   üìä Saved: {save_path}")


def main():
    parser = argparse.ArgumentParser(description="Test Stage 2 (Text-to-Latent) checkpoint")
    parser.add_argument("--config", type=str, default="config/22khz_optimal.yaml")
    parser.add_argument("--autoencoder", type=str, default=None, help="Autoencoder checkpoint")
    parser.add_argument("--checkpoint", type=str, default=None, help="Stage 2 checkpoint")
    parser.add_argument("--val-manifest", type=str, default="data/manifests_stage2/val.json")
    parser.add_argument("--data-dir", type=str, default="data")
    parser.add_argument("--output-dir", type=str, default="outputs/stage2_test")
    parser.add_argument("--num-samples", type=int, default=5)
    parser.add_argument("--nfe", type=int, default=32, help="ODE solver steps")
    parser.add_argument("--cfg-scale", type=float, default=3.0)
    parser.add_argument("--dataset", type=str, default="opentts", 
                        help="Filter by dataset name (default: opentts for Ukrainian)")
    parser.add_argument("--device", type=str, default="cuda")
    args = parser.parse_args()
    
    device = args.device
    
    print("\n" + "="*60)
    print("  üé§ SUPERTONIC V2 - Stage 2 Checkpoint Tester")
    print("="*60)
    
    # Load config
    config = OmegaConf.load(args.config)
    config = OmegaConf.to_container(config)
    
    # Find autoencoder checkpoint
    if args.autoencoder:
        ae_path = args.autoencoder
    else:
        ae_paths = ["checkpoints/autoencoder/checkpoint_150000.pt",
                    "outputs/autoencoder_hifigan/checkpoints/autoencoder/checkpoint_150000.pt"]
        ae_path = None
        for p in ae_paths:
            if Path(p).exists():
                ae_path = p
                break
        if not ae_path:
            print("‚ùå No autoencoder checkpoint found!")
            return
    
    # Find Stage 2 checkpoint
    if args.checkpoint:
        s2_path = args.checkpoint
    else:
        s2_dirs = ["outputs/text_to_latent/checkpoints", "checkpoints/text_to_latent"]
        s2_path = None
        for d in s2_dirs:
            checkpoints = list_checkpoints(d)
            if checkpoints:
                s2_path = checkpoints[-1]  # Latest
                break
        if not s2_path:
            print("‚ùå No Stage 2 checkpoint found!")
            return
    
    # Load models
    latent_encoder, latent_decoder = load_autoencoder(ae_path, config, device)
    text_to_latent = load_text_to_latent(s2_path, config, device)
    
    # Create tokenizer and audio processor
    tokenizer = CharacterTokenizer(languages=config.get("languages", {}).get("supported", ["uk", "en"]))
    audio_processor = AudioProcessor(
        sample_rate=config["audio"]["sample_rate"],
        n_fft=config["audio"]["n_fft"],
        hop_length=config["audio"]["hop_length"],
        n_mels=config["audio"]["n_mels"],
    )  # Stays on CPU, we'll move mel to GPU after
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Get test samples (filter by dataset, default: opentts for Ukrainian)
    if Path(args.val_manifest).exists():
        samples = discover_test_samples(args.val_manifest, args.num_samples, args.dataset)
    else:
        print(f"‚ùå Validation manifest not found: {args.val_manifest}")
        return
    
    if not samples:
        print(f"‚ùå No samples found matching dataset filter '{args.dataset}'")
        return
    
    print(f"\nüî¨ Testing {len(samples)} samples...")
    print(f"   NFE: {args.nfe}, CFG: {args.cfg_scale}")
    print(f"   NFE: {args.nfe}, CFG: {args.cfg_scale}")
    
    results = []
    
    for i, sample in enumerate(samples):
        print(f"\n{'='*60}")
        print(f"Sample {i+1}/{len(samples)}")
        print(f"  Text: {sample['text'][:80]}...")
        print(f"  Speaker: {sample.get('speaker_id', 'unknown')}")
        
        # Load audio
        audio_path = Path(args.data_dir) / sample["audio_path"]
        if not audio_path.exists():
            print(f"  ‚ö†Ô∏è Audio not found: {audio_path}")
            continue
        
        target_audio, sr = torchaudio.load(audio_path)
        if sr != config["audio"]["sample_rate"]:
            target_audio = torchaudio.functional.resample(target_audio, sr, config["audio"]["sample_rate"])
        target_audio = target_audio.mean(dim=0)  # Mono
        
        # Use different reference audio (from same speaker if possible, or same audio)
        ref_audio = target_audio  # For now, use same audio as reference
        
        # Synthesize
        try:
            generated_audio, info = synthesize_with_gt_duration(
                text=sample["text"],
                reference_audio=ref_audio,
                target_audio=target_audio,
                text_to_latent=text_to_latent,
                latent_encoder=latent_encoder,
                latent_decoder=latent_decoder,
                tokenizer=tokenizer,
                audio_processor=audio_processor,
                device=device,
                nfe=args.nfe,
                cfg_scale=args.cfg_scale,
            )
            
            print(f"  ‚è±Ô∏è Inference: {info['inference_time']:.2f}s")
            print(f"  üìè Generated: {generated_audio.shape[0] / config['audio']['sample_rate']:.2f}s")
            
            # Save audio
            sample_name = f"sample_{i+1:02d}"
            torchaudio.save(
                output_dir / f"{sample_name}_generated.wav",
                generated_audio.unsqueeze(0),
                config["audio"]["sample_rate"]
            )
            torchaudio.save(
                output_dir / f"{sample_name}_original.wav",
                target_audio.unsqueeze(0),
                config["audio"]["sample_rate"]
            )
            
            # Compute mel for comparison (on CPU)
            with torch.no_grad():
                orig_mel = audio_processor.compute_mel(target_audio.unsqueeze(0), log_scale=True)
                gen_mel = audio_processor.compute_mel(generated_audio.unsqueeze(0), log_scale=True)
                
                # Mel L1
                min_len = min(orig_mel.shape[2], gen_mel.shape[2])
                mel_l1 = F.l1_loss(orig_mel[:, :, :min_len], gen_mel[:, :, :min_len]).item()
            
            print(f"  üìä Mel L1: {mel_l1:.4f}")
            
            # Plot
            if MATPLOTLIB_AVAILABLE:
                plot_comparison(
                    orig_mel.squeeze(0),
                    gen_mel.squeeze(0),
                    str(output_dir / f"{sample_name}_comparison.png"),
                    title=f"Sample {i+1}: {sample['text'][:50]}..."
                )
            
            results.append({
                "sample": sample_name,
                "text": sample["text"],
                "speaker": sample.get("speaker_id", "unknown"),
                "mel_l1": mel_l1,
                "inference_time": info["inference_time"],
            })
            
        except Exception as e:
            print(f"  ‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
    
    # Summary
    print("\n" + "="*60)
    print("  üìã SUMMARY")
    print("="*60)
    
    if results:
        avg_mel_l1 = np.mean([r["mel_l1"] for r in results])
        avg_time = np.mean([r["inference_time"] for r in results])
        print(f"  Average Mel L1: {avg_mel_l1:.4f}")
        print(f"  Average inference time: {avg_time:.2f}s")
        print(f"  Output: {output_dir}")
        
        # Save results
        with open(output_dir / "results.json", "w") as f:
            json.dump({
                "checkpoint": s2_path,
                "autoencoder": ae_path,
                "nfe": args.nfe,
                "cfg_scale": args.cfg_scale,
                "avg_mel_l1": avg_mel_l1,
                "avg_inference_time": avg_time,
                "samples": results,
            }, f, indent=2, ensure_ascii=False)
    else:
        print("  No samples processed successfully")


if __name__ == "__main__":
    main()



==================================================
–§–ê–ô–õ: supertonic/__init__.py
–†–û–ó–ú–Ü–†: 0.86 KB
==================================================

"""
Supertonic v2 TTS - –†–µ—ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏

–£–ª—å—Ç—Ä–∞–µ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–≤–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ text-to-speech –∑ 66M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤,
—â–æ –¥–æ—Å—è–≥–∞—î 167√ó —à–≤–∏–¥—à–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É –Ω–∞ —Å–ø–æ–∂–∏–≤—á–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ.

–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
- Speech Autoencoder (~47M): Vocos-based, 24-dim latent space
- Text-to-Latent (~19M): Conditional flow matching –∑ LARoPE
- Duration Predictor (~0.5M): Utterance-level prediction

–ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ –º–æ–≤–∏: English, Korean, Spanish, Portuguese, French, Ukrainian
"""

__version__ = "2.0.0-uk"
__author__ = "Reimplementation for Ukrainian TTS"

from supertonic.models import (
    SpeechAutoencoder,
    TextToLatent,
    DurationPredictor,
)

__all__ = [
    "SpeechAutoencoder",
    "TextToLatent",
    "DurationPredictor",
]



==================================================
–§–ê–ô–õ: supertonic/losses/__init__.py
–†–û–ó–ú–Ü–†: 0.83 KB
==================================================

"""
Loss functions –¥–ª—è Supertonic v2 TTS

–ú–æ–¥—É–ª—ñ:
- autoencoder_loss: Multi-resolution reconstruction + GAN losses
- flow_matching_loss: Conditional Flow Matching loss
- duration_loss: L1 loss –¥–ª—è duration prediction
"""

from supertonic.losses.autoencoder_loss import (
    AutoencoderLoss,
    MultiResolutionMelLoss,
    GANLoss,
    FeatureMatchingLoss
)

from supertonic.losses.flow_matching_loss import (
    FlowMatchingLoss,
    flow_matching_loss,
    create_reference_mask
)

from supertonic.losses.duration_loss import (
    DurationLoss,
    duration_loss
)

__all__ = [
    "AutoencoderLoss",
    "MultiResolutionMelLoss",
    "GANLoss",
    "FeatureMatchingLoss",
    "FlowMatchingLoss",
    "flow_matching_loss",
    "create_reference_mask",
    "DurationLoss",
    "duration_loss"
]



==================================================
–§–ê–ô–õ: supertonic/losses/autoencoder_loss.py
–†–û–ó–ú–Ü–†: 23.34 KB
==================================================

"""
Autoencoder Loss - Multi-resolution reconstruction + GAN losses

Based on SupertonicTTS paper (arXiv:2503.23108v3)

Paper Loss formula (Appendix B.1):
    L_G = Œª_recon √ó L_recon + Œª_adv √ó L_adv + Œª_fm √ó L_fm

Where:
- L_recon: Multi-resolution spectral L1 loss in mel domain
          FFT sizes: [1024, 2048, 4096] @ 44.1kHz
          Mel bands: [64, 128, 128] (different per resolution!)
          Œª_recon = 45

- L_adv: LSGAN adversarial loss
         L_adv(G;D) = E[(D(G(x)) - 1)^2]
         Œª_adv = 1

- L_fm: Feature matching L1 loss
        L_fm = (1/L) Œ£ ||phi_l(G(x)) - phi_l(x)||_1
        Œª_fm = 0.1

Discriminators (paper uses VERY lightweight - Table 7):
- MPD: Multi-Period Discriminator (periods [2, 3, 5, 7, 11])
       Hidden sizes: [16, 64, 256, 512, 512, 1]
- MRD: Multi-Resolution Discriminator (FFT [512, 1024, 2048])
       All conv layers have hidden_channels = 16 (!)

For 22kHz we scale FFT sizes proportionally:
    [1024, 2048, 4096] @ 44.1kHz -> [512, 1024, 2048] @ 22kHz

Reference: HiFi-GAN, Vocos, SupertonicTTS paper
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Dict, Optional
import math


class MultiResolutionSTFT(nn.Module):
    """
    Multi-Resolution STFT –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è mel spectrogram –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö scales.
    
    Paper (Appendix B.1) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –†–Ü–ó–ù–Ü n_mels –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ FFT size:
    - FFT 1024 -> 64 mels
    - FFT 2048 -> 128 mels  
    - FFT 4096 -> 128 mels
    """
    
    def __init__(
        self,
        fft_sizes: List[int] = [512, 1024, 2048],
        hop_sizes: Optional[List[int]] = None,
        win_sizes: Optional[List[int]] = None,
        sample_rate: int = 22050,
        n_mels: Optional[List[int]] = None  # Paper: different per resolution!
    ):
        super().__init__()
        
        self.fft_sizes = fft_sizes
        self.hop_sizes = hop_sizes or [s // 4 for s in fft_sizes]
        self.win_sizes = win_sizes or fft_sizes
        self.sample_rate = sample_rate
        
        # Paper uses different n_mels per resolution: [64, 128, 128]
        if n_mels is None:
            self.n_mels_list = [64, 128, 128]  # Paper default
        elif isinstance(n_mels, int):
            self.n_mels_list = [n_mels] * len(fft_sizes)
        else:
            self.n_mels_list = n_mels
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ mel filterbanks –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ resolution
        for i, (fft_size, hop_size, win_size) in enumerate(
            zip(self.fft_sizes, self.hop_sizes, self.win_sizes)
        ):
            n_mels_i = self.n_mels_list[i] if i < len(self.n_mels_list) else self.n_mels_list[-1]
            mel_fb = self._create_mel_filterbank(
                n_fft=fft_size,
                n_mels=n_mels_i,
                sample_rate=sample_rate
            )
            self.register_buffer(f"mel_fb_{i}", mel_fb)
            self.register_buffer(f"n_mels_{i}", torch.tensor(n_mels_i))
            
            # Hann window
            window = torch.hann_window(win_size)
            self.register_buffer(f"window_{i}", window)
    
    def _create_mel_filterbank(
        self,
        n_fft: int,
        n_mels: int,
        sample_rate: int,
        f_min: float = 0.0,
        f_max: Optional[float] = None
    ) -> torch.Tensor:
        """–°—Ç–≤–æ—Ä—é—î mel filterbank."""
        f_max = f_max or sample_rate / 2
        
        def hz_to_mel(f):
            return 2595 * math.log10(1 + f / 700)
        
        def mel_to_hz(m):
            return 700 * (10 ** (m / 2595) - 1)
        
        n_freqs = n_fft // 2 + 1
        fft_freqs = torch.linspace(0, sample_rate / 2, n_freqs)
        
        mel_min = hz_to_mel(f_min)
        mel_max = hz_to_mel(f_max)
        mel_points = torch.linspace(mel_min, mel_max, n_mels + 2)
        hz_points = torch.tensor([mel_to_hz(m) for m in mel_points])
        
        mel_fb = torch.zeros(n_mels, n_freqs)
        
        for i in range(n_mels):
            left = hz_points[i]
            center = hz_points[i + 1]
            right = hz_points[i + 2]
            
            rising = (fft_freqs - left) / (center - left + 1e-8)
            falling = (right - fft_freqs) / (right - center + 1e-8)
            
            mel_fb[i] = torch.maximum(
                torch.zeros_like(fft_freqs),
                torch.minimum(rising, falling)
            )
        
        # Slaney normalization
        enorm = 2.0 / (hz_points[2:n_mels + 2] - hz_points[:n_mels] + 1e-8)
        mel_fb *= enorm.unsqueeze(1)
        
        return mel_fb
    
    def forward(
        self,
        audio: torch.Tensor
    ) -> List[torch.Tensor]:
        """
        –û–±—á–∏—Å–ª—é—î mel spectrograms –Ω–∞ –≤—Å—ñ—Ö resolutions.
        
        Args:
            audio: [B, T] –∞–±–æ [B, 1, T]
            
        Returns:
            List of mel spectrograms
        """
        if audio.dim() == 3:
            audio = audio.squeeze(1)
        
        mels = []
        
        for i, (fft_size, hop_size, win_size) in enumerate(
            zip(self.fft_sizes, self.hop_sizes, self.win_sizes)
        ):
            mel_fb = getattr(self, f"mel_fb_{i}")
            window = getattr(self, f"window_{i}")
            
            # Ensure window is on same device as audio
            if window.device != audio.device:
                window = window.to(audio.device)
            
            # STFT
            spec = torch.stft(
                audio,
                n_fft=fft_size,
                hop_length=hop_size,
                win_length=win_size,
                window=window,
                center=True,
                pad_mode='reflect',
                normalized=False,
                onesided=True,
                return_complex=True
            )
            
            # Magnitude
            mag = spec.abs()  # [B, n_freqs, T]
            
            # Ensure mel_fb is on same device as mag
            if mel_fb.device != mag.device:
                mel_fb = mel_fb.to(mag.device)
            
            # Mel
            mel = torch.matmul(mel_fb, mag)
            mel = torch.log(torch.clamp(mel, min=1e-5))
            
            mels.append(mel)
        
        return mels


class SpectralConvergenceLoss(nn.Module):
    """
    Spectral Convergence + Log Magnitude Loss (HiFi-GAN style).
    
    –ü—Ä–∞—Ü—é—î –Ω–∞ –õ–Ü–ù–Ü–ô–ù–û–ú–£ —Å–ø–µ–∫—Ç—Ä—ñ (–Ω–µ Mel!) –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ —Ñ–∞–∑–∏.
    SC = ||mag_real - mag_fake||_F / ||mag_real||_F
    LM = ||log(mag_real) - log(mag_fake)||_1
    
    –¶–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —É—Å—É–Ω–µ–Ω–Ω—è "–º–µ—Ç–∞–ª–µ–≤–æ–≥–æ" –∑–≤—É–∫—É!
    """
    
    def __init__(
        self,
        fft_sizes: List[int] = [512, 1024, 2048],
        hop_sizes: Optional[List[int]] = None,
        win_sizes: Optional[List[int]] = None,
    ):
        super().__init__()
        
        self.fft_sizes = fft_sizes
        self.hop_sizes = hop_sizes or [s // 4 for s in fft_sizes]
        self.win_sizes = win_sizes or fft_sizes
        
        # Register windows
        for i, win_size in enumerate(self.win_sizes):
            window = torch.hann_window(win_size)
            self.register_buffer(f"window_{i}", window)
    
    def _stft_magnitude(self, audio: torch.Tensor, fft_size: int, 
                        hop_size: int, win_size: int, window: torch.Tensor) -> torch.Tensor:
        """Compute STFT magnitude."""
        if audio.dim() == 3:
            audio = audio.squeeze(1)
        
        spec = torch.stft(
            audio,
            n_fft=fft_size,
            hop_length=hop_size,
            win_length=win_size,
            window=window,
            center=True,
            pad_mode='reflect',
            normalized=False,
            onesided=True,
            return_complex=True
        )
        return spec.abs()
    
    def forward(self, real: torch.Tensor, generated: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            sc_loss: Spectral Convergence loss
            mag_loss: Log Magnitude loss  
        """
        if real.dim() == 3:
            real = real.squeeze(1)
        if generated.dim() == 3:
            generated = generated.squeeze(1)
            
        # Match lengths
        min_len = min(real.size(-1), generated.size(-1))
        real = real[..., :min_len]
        generated = generated[..., :min_len]
        
        sc_loss = 0.0
        mag_loss = 0.0
        
        for i, (fft_size, hop_size, win_size) in enumerate(
            zip(self.fft_sizes, self.hop_sizes, self.win_sizes)
        ):
            window = getattr(self, f"window_{i}")
            if window.device != real.device:
                window = window.to(real.device)
            
            mag_real = self._stft_magnitude(real, fft_size, hop_size, win_size, window)
            mag_fake = self._stft_magnitude(generated, fft_size, hop_size, win_size, window)
            
            # Spectral Convergence: Frobenius norm ratio
            sc_loss += torch.norm(mag_real - mag_fake, p="fro") / (torch.norm(mag_real, p="fro") + 1e-9)
            
            # Log Magnitude L1
            log_mag_real = torch.log(mag_real.clamp(min=1e-5))
            log_mag_fake = torch.log(mag_fake.clamp(min=1e-5))
            mag_loss += F.l1_loss(log_mag_fake, log_mag_real)
        
        n = len(self.fft_sizes)
        return sc_loss / n, mag_loss / n


class MultiResolutionMelLoss(nn.Module):
    """
    Multi-Resolution Mel L1 Loss (Paper Appendix B.1).
    
    Paper configuration:
    - FFT sizes: [1024, 2048, 4096] @ 44.1kHz
    - Mel bands: [64, 128, 128] - DIFFERENT per resolution!
    - Hop sizes: FFT/4
    - Window: Hann, size = FFT size
    
    For 22kHz we scale FFT sizes by 0.5:
    - FFT sizes: [512, 1024, 2048]
    - Mel bands: [64, 128, 128]
    """
    
    def __init__(
        self,
        fft_sizes: List[int] = [512, 1024, 2048],  # Scaled for 22kHz
        sample_rate: int = 22050,
        n_mels: Optional[List[int]] = None  # Paper: [64, 128, 128]
    ):
        super().__init__()
        
        # Paper default: different n_mels per resolution
        if n_mels is None:
            n_mels = [64, 128, 128]
        
        self.stft = MultiResolutionSTFT(
            fft_sizes=fft_sizes,
            sample_rate=sample_rate,
            n_mels=n_mels
        )
    
    def forward(
        self,
        real: torch.Tensor,
        generated: torch.Tensor
    ) -> torch.Tensor:
        """
        –û–±—á–∏—Å–ª—é—î multi-resolution mel L1 loss.
        
        Args:
            real: Real audio [B, T]
            generated: Generated audio [B, T]
            
        Returns:
            loss: Scalar tensor
        """
        # –í–∏—Ä—ñ–≤–Ω—é—î–º–æ –¥–æ–≤–∂–∏–Ω–∏
        min_len = min(real.size(-1), generated.size(-1))
        real = real[..., :min_len]
        generated = generated[..., :min_len]
        
        real_mels = self.stft(real)
        gen_mels = self.stft(generated)
        
        loss = 0.0
        for real_mel, gen_mel in zip(real_mels, gen_mels):
            # –í–∏—Ä—ñ–≤–Ω—é—î–º–æ –ø–æ —á–∞—Å—É
            min_t = min(real_mel.size(-1), gen_mel.size(-1))
            loss += F.l1_loss(
                gen_mel[..., :min_t],
                real_mel[..., :min_t]
            )
        
        return loss / len(real_mels)


class GANLoss(nn.Module):
    """
    GAN Loss –¥–ª—è adversarial training.
    
    –ü—ñ–¥—Ç—Ä–∏–º—É—î:
    - LSGAN (least squares)
    - Hinge loss
    - Vanilla GAN
    """
    
    def __init__(self, loss_type: str = "lsgan"):
        super().__init__()
        
        self.loss_type = loss_type
        
    def discriminator_loss(
        self,
        real_outputs: List[torch.Tensor],
        fake_outputs: List[torch.Tensor]
    ) -> torch.Tensor:
        """
        Discriminator loss.
        
        Paper (Appendix B.1, Eq. 5):
        L_adv(D;G) = E[(D(G(x)) + 1)¬≤ + (D(x) - 1)¬≤]
        
        This uses -1/+1 labels (real=+1, fake=-1), NOT 0/1!
        
        Args:
            real_outputs: List of discriminator outputs for real samples
            fake_outputs: List of discriminator outputs for fake samples
            
        Returns:
            loss: Scalar tensor
        """
        loss = 0.0
        
        for real_out, fake_out in zip(real_outputs, fake_outputs):
            if self.loss_type == "lsgan":
                # Paper Eq. 5: (D(x) - 1)¬≤ + (D(G(x)) + 1)¬≤
                # real ‚Üí +1, fake ‚Üí -1
                loss += torch.mean((real_out - 1) ** 2) + torch.mean((fake_out + 1) ** 2)
            elif self.loss_type == "hinge":
                loss += torch.mean(F.relu(1 - real_out)) + torch.mean(F.relu(1 + fake_out))
            else:  # vanilla
                loss += F.binary_cross_entropy_with_logits(
                    real_out, torch.ones_like(real_out)
                ) + F.binary_cross_entropy_with_logits(
                    fake_out, torch.zeros_like(fake_out)
                )
        
        return loss / len(real_outputs)
    
    def generator_loss(
        self,
        fake_outputs: List[torch.Tensor]
    ) -> torch.Tensor:
        """
        Generator adversarial loss.
        
        Args:
            fake_outputs: List of discriminator outputs for generated samples
            
        Returns:
            loss: Scalar tensor
        """
        loss = 0.0
        
        for fake_out in fake_outputs:
            if self.loss_type == "lsgan":
                loss += torch.mean((fake_out - 1) ** 2)
            elif self.loss_type == "hinge":
                loss += -torch.mean(fake_out)
            else:  # vanilla
                loss += F.binary_cross_entropy_with_logits(
                    fake_out, torch.ones_like(fake_out)
                )
        
        return loss / len(fake_outputs)


class FeatureMatchingLoss(nn.Module):
    """
    Feature Matching Loss.
    
    L1 loss –Ω–∞ intermediate features discriminator'–∞.
    –°—Ç–∞–±—ñ–ª—ñ–∑—É—î GAN training.
    """
    
    def __init__(self):
        super().__init__()
    
    def forward(
        self,
        real_features: List[List[torch.Tensor]],
        fake_features: List[List[torch.Tensor]]
    ) -> torch.Tensor:
        """
        –û–±—á–∏—Å–ª—é—î feature matching loss.
        
        Args:
            real_features: List of feature lists from discriminators (real)
            fake_features: List of feature lists from discriminators (fake)
            
        Returns:
            loss: Scalar tensor
        """
        loss = 0.0
        num_features = 0
        
        for real_feat_list, fake_feat_list in zip(real_features, fake_features):
            for real_feat, fake_feat in zip(real_feat_list, fake_feat_list):
                loss += F.l1_loss(fake_feat, real_feat.detach())
                num_features += 1
        
        return loss / max(num_features, 1)


class AutoencoderLoss(nn.Module):
    """
    Loss –¥–ª—è Speech Autoencoder (—Ç–æ—á–Ω–æ —è–∫ –≤ SupertonicTTS paper).
    
    Paper formula (Appendix B.1):
        L_G = Œª_recon √ó L_recon + Œª_adv √ó L_adv + Œª_fm √ó L_fm
    
    Where:
        L_recon: Multi-resolution spectral L1 loss in mel domain
        L_adv: LSGAN adversarial loss  
        L_fm: Feature matching L1 loss
    
    Paper weights:
        Œª_recon = 45
        Œª_adv = 1
        Œª_fm = 0.1
    
    Paper config (for 44.1kHz):
        FFT sizes: [1024, 2048, 4096]
        Mel bands: [64, 128, 128]
        
    For 22kHz we scale FFT sizes by 0.5.
    """
    
    def __init__(
        self,
        lambda_recon: float = 45.0,   # Paper: Œª_recon = 45
        lambda_adv: float = 1.0,      # Paper: Œª_adv = 1
        lambda_fm: float = 0.1,       # Paper: Œª_fm = 0.1
        fft_sizes: List[int] = [512, 1024, 2048],  # Scaled for 22kHz
        n_mels: List[int] = [64, 128, 128],        # Paper: different per resolution
        sample_rate: int = 22050,
        gan_type: str = "lsgan",
        # Legacy params (kept for config compat)
        lambda_mel: float = None,     # Alias for lambda_recon
        lambda_wave: float = None,    # Not used in paper
        lambda_sc: float = None,
        lambda_mag: float = None,
    ):
        super().__init__()
        
        # Handle legacy param names
        if lambda_mel is not None:
            lambda_recon = lambda_mel
        
        self.lambda_recon = lambda_recon
        self.lambda_adv = lambda_adv
        self.lambda_fm = lambda_fm
        
        # Mel Loss (paper's reconstruction objective)
        self.mel_loss = MultiResolutionMelLoss(
            fft_sizes=fft_sizes,
            sample_rate=sample_rate,
            n_mels=n_mels
        )
        self.gan_loss = GANLoss(loss_type=gan_type)
        self.fm_loss = FeatureMatchingLoss()
    
    def generator_loss(
        self,
        real_audio: torch.Tensor,
        generated_audio: torch.Tensor,
        disc_fake_outputs: List[torch.Tensor],
        real_features: List[List[torch.Tensor]],
        fake_features: List[List[torch.Tensor]],
        use_adv: bool = True
    ) -> Dict[str, torch.Tensor]:
        """
        Generator loss (Paper Appendix B.1).
        
        L_G = Œª_recon √ó L_recon + Œª_adv √ó L_adv + Œª_fm √ó L_fm
        
        Args:
            real_audio: Real audio [B, T]
            generated_audio: Generated audio [B, T]
            disc_fake_outputs: Discriminator outputs for generated audio
            real_features: Discriminator features for real audio
            fake_features: Discriminator features for generated audio
            use_adv: Whether to use adversarial loss (False during warmup)
            
        Returns:
            Dict with loss components and total loss
        """
        # Match lengths
        min_len = min(real_audio.size(-1), generated_audio.size(-1))
        real_audio_crop = real_audio[..., :min_len]
        generated_audio_crop = generated_audio[..., :min_len]
        
        # L_recon: Multi-resolution mel L1 loss (paper's main objective)
        l_recon = self.mel_loss(real_audio_crop, generated_audio_crop)
        
        # Adversarial + Feature Matching (can be disabled during warmup)
        if use_adv and len(disc_fake_outputs) > 0:
            # L_adv: LSGAN loss
            l_adv = self.gan_loss.generator_loss(disc_fake_outputs)
            # L_fm: Feature matching L1
            l_fm = self.fm_loss(real_features, fake_features)
        else:
            l_adv = torch.tensor(0.0, device=real_audio.device)
            l_fm = torch.tensor(0.0, device=real_audio.device)
        
        # Paper formula: L_G = Œª_recon √ó L_recon + Œª_adv √ó L_adv + Œª_fm √ó L_fm
        total = (
            self.lambda_recon * l_recon +
            self.lambda_adv * l_adv +
            self.lambda_fm * l_fm
        )
        
        return {
            "total": total,
            "reconstruction": l_recon,
            "adversarial": l_adv,
            "feature_matching": l_fm
        }
    
    def discriminator_loss(
        self,
        real_outputs: List[torch.Tensor],
        fake_outputs: List[torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        Discriminator loss.
        
        Args:
            real_outputs: Discriminator outputs for real audio
            fake_outputs: Discriminator outputs for generated audio
            
        Returns:
            Dict with loss
        """
        loss = self.gan_loss.discriminator_loss(real_outputs, fake_outputs)
        
        return {"total": loss, "discriminator": loss}


# ============================================================================
# Unit tests
# ============================================================================

def _test_losses():
    """–¢–µ—Å—Ç loss functions (paper configuration)."""
    print("Testing Autoencoder Losses (SupertonicTTS paper config)...")
    
    batch_size = 4
    audio_len = 22050  # 1 second at 22kHz
    
    # Test multi-resolution STFT with paper config
    stft = MultiResolutionSTFT(
        fft_sizes=[512, 1024, 2048],  # Scaled for 22kHz
        sample_rate=22050,
        n_mels=[64, 128, 128]  # Paper: different per resolution
    )
    
    audio = torch.randn(batch_size, audio_len)
    mels = stft(audio)
    
    assert len(mels) == 3
    print(f"  Multi-resolution STFT shapes: {[m.shape for m in mels]}")
    print(f"    - FFT 512:  {mels[0].shape} (64 mels) ‚úì")
    print(f"    - FFT 1024: {mels[1].shape} (128 mels) ‚úì")
    print(f"    - FFT 2048: {mels[2].shape} (128 mels) ‚úì")
    
    # Test mel loss
    mel_loss = MultiResolutionMelLoss(
        fft_sizes=[512, 1024, 2048],
        sample_rate=22050,
        n_mels=[64, 128, 128]
    )
    real = torch.randn(batch_size, audio_len)
    fake = torch.randn(batch_size, audio_len)
    
    loss = mel_loss(real, fake)
    assert loss.dim() == 0
    print(f"  Mel L1 loss: {loss.item():.4f} ‚úì")
    
    # Test GAN loss (LSGAN as in paper)
    gan_loss = GANLoss(loss_type="lsgan")
    real_out = [torch.randn(batch_size, 100) for _ in range(5)]
    fake_out = [torch.randn(batch_size, 100) for _ in range(5)]
    
    d_loss = gan_loss.discriminator_loss(real_out, fake_out)
    g_loss = gan_loss.generator_loss(fake_out)
    
    print(f"  Discriminator loss: {d_loss.item():.4f} ‚úì")
    print(f"  Generator adv loss: {g_loss.item():.4f} ‚úì")
    
    # Test feature matching loss
    fm_loss = FeatureMatchingLoss()
    real_features = [[torch.randn(batch_size, 64, 100) for _ in range(3)] for _ in range(5)]
    fake_features = [[torch.randn(batch_size, 64, 100) for _ in range(3)] for _ in range(5)]
    
    fm = fm_loss(real_features, fake_features)
    print(f"  Feature matching loss: {fm.item():.4f} ‚úì")
    
    # Test full autoencoder loss with EXACT paper config
    ae_loss = AutoencoderLoss(
        lambda_recon=45.0,  # Paper: Œª_recon = 45
        lambda_adv=1.0,     # Paper: Œª_adv = 1
        lambda_fm=0.1,      # Paper: Œª_fm = 0.1
        fft_sizes=[512, 1024, 2048],
        n_mels=[64, 128, 128],
        sample_rate=22050
    )
    
    gen_losses = ae_loss.generator_loss(
        real_audio=real,
        generated_audio=fake,
        disc_fake_outputs=fake_out,
        real_features=real_features,
        fake_features=fake_features
    )
    
    print(f"\n  Paper loss formula: L_G = 45√óL_recon + 1√óL_adv + 0.1√óL_fm")
    print(f"  Total generator loss: {gen_losses['total'].item():.4f}")
    print(f"    - L_recon (mel):    {gen_losses['reconstruction'].item():.4f} √ó 45 = {gen_losses['reconstruction'].item() * 45:.4f}")
    print(f"    - L_adv:            {gen_losses['adversarial'].item():.4f} √ó 1  = {gen_losses['adversarial'].item():.4f}")
    print(f"    - L_fm:             {gen_losses['feature_matching'].item():.4f} √ó 0.1 = {gen_losses['feature_matching'].item() * 0.1:.4f}")
    
    disc_losses = ae_loss.discriminator_loss(real_out, fake_out)
    print(f"  Discriminator loss: {disc_losses['total'].item():.4f}")
    
    print("\nAll Autoencoder loss tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_losses()



==================================================
–§–ê–ô–õ: supertonic/losses/duration_loss.py
–†–û–ó–ú–Ü–†: 6.82 KB
==================================================

"""
Duration Loss - L1 loss –¥–ª—è Duration Predictor

–ü—Ä–æ—Å—Ç–∏–π L1 loss –¥–ª—è utterance-level duration prediction:
    L_dur = L1(d_predicted, d_groundtruth)

Duration –≤–∏–º—ñ—Ä—é—î—Ç—å—Å—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –∞–±–æ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ñ—Ä–µ–π–º—ñ–≤.

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional


def duration_loss(
    predicted: torch.Tensor,
    target: torch.Tensor,
    reduction: str = "mean"
) -> torch.Tensor:
    """
    L1 loss –¥–ª—è duration prediction.
    
    Args:
        predicted: Predicted duration [B] –∞–±–æ [B, 1]
        target: Ground-truth duration [B] –∞–±–æ [B, 1]
        reduction: "mean", "sum", –∞–±–æ "none"
        
    Returns:
        loss: L1 loss tensor
    """
    # Flatten
    predicted = predicted.view(-1)
    target = target.view(-1)
    
    return F.l1_loss(predicted, target, reduction=reduction)


class DurationLoss(nn.Module):
    """
    Duration Loss module.
    
    –û–±—á–∏—Å–ª—é—î L1 loss + optional percentage error.
    
    Args:
        reduction: "mean", "sum", –∞–±–æ "none"
    """
    
    def __init__(self, reduction: str = "mean"):
        super().__init__()
        self.reduction = reduction
    
    def forward(
        self,
        predicted: torch.Tensor,
        target: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Compute duration loss.
        
        Args:
            predicted: Predicted duration [B] –∞–±–æ [B, 1]
            target: Ground-truth duration [B] –∞–±–æ [B, 1]
            
        Returns:
            Dict with loss components
        """
        # Flatten
        predicted = predicted.view(-1)
        target = target.view(-1)
        
        # L1 loss
        l1 = duration_loss(predicted, target, reduction=self.reduction)
        
        # Percentage error (for logging)
        with torch.no_grad():
            percent_error = ((predicted - target).abs() / (target + 1e-8)).mean() * 100
        
        return {
            "total": l1,
            "l1": l1,
            "percent_error": percent_error
        }


class DurationLossWithMask(nn.Module):
    """
    Duration Loss –∑ mask support.
    
    –î–ª—è batch training –¥–µ –¥–µ—è–∫—ñ samples –º–æ–∂—É—Ç—å –±—É—Ç–∏ padded.
    """
    
    def __init__(self):
        super().__init__()
    
    def forward(
        self,
        predicted: torch.Tensor,
        target: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute masked duration loss.
        
        Args:
            predicted: [B] –∞–±–æ [B, 1]
            target: [B] –∞–±–æ [B, 1]
            mask: Optional valid sample mask [B]
            
        Returns:
            Dict with loss components
        """
        predicted = predicted.view(-1)
        target = target.view(-1)
        
        if mask is None:
            mask = torch.ones_like(predicted)
        else:
            mask = mask.view(-1).float()
        
        # Masked L1
        l1_per_sample = (predicted - target).abs()
        l1 = (l1_per_sample * mask).sum() / (mask.sum() + 1e-8)
        
        # Masked percentage error
        with torch.no_grad():
            percent_per_sample = l1_per_sample / (target + 1e-8) * 100
            percent_error = (percent_per_sample * mask).sum() / (mask.sum() + 1e-8)
        
        return {
            "total": l1,
            "l1": l1,
            "percent_error": percent_error
        }


# ============================================================================
# Utilities
# ============================================================================

def duration_to_frames(
    duration_seconds: torch.Tensor,
    sample_rate: int = 44100,
    hop_length: int = 512,
    compression_factor: int = 6
) -> torch.Tensor:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î duration –∑ —Å–µ–∫—É–Ω–¥ –≤ –∫—ñ–ª—å–∫—ñ—Å—Ç—å compressed frames.
    
    Args:
        duration_seconds: Duration in seconds [B]
        sample_rate: Audio sample rate
        hop_length: STFT hop length
        compression_factor: Temporal compression factor (Kc)
        
    Returns:
        num_frames: Number of compressed frames [B]
    """
    # Samples ‚Üí mel frames ‚Üí compressed frames
    num_samples = duration_seconds * sample_rate
    num_mel_frames = num_samples / hop_length
    num_compressed_frames = num_mel_frames / compression_factor
    
    return num_compressed_frames


def frames_to_duration(
    num_frames: torch.Tensor,
    sample_rate: int = 44100,
    hop_length: int = 512,
    compression_factor: int = 6
) -> torch.Tensor:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å compressed frames –≤ —Å–µ–∫—É–Ω–¥–∏.
    
    Args:
        num_frames: Number of compressed frames [B]
        sample_rate: Audio sample rate
        hop_length: STFT hop length
        compression_factor: Temporal compression factor (Kc)
        
    Returns:
        duration_seconds: Duration in seconds [B]
    """
    num_mel_frames = num_frames * compression_factor
    num_samples = num_mel_frames * hop_length
    duration_seconds = num_samples / sample_rate
    
    return duration_seconds


# ============================================================================
# Unit tests
# ============================================================================

def _test_duration_loss():
    """–¢–µ—Å—Ç duration loss."""
    print("Testing Duration Loss...")
    
    batch_size = 8
    
    # Test basic loss
    predicted = torch.tensor([2.5, 3.0, 1.5, 4.0, 2.0, 3.5, 1.8, 2.2])
    target = torch.tensor([2.3, 3.2, 1.4, 4.1, 2.1, 3.3, 1.9, 2.0])
    
    loss = duration_loss(predicted, target)
    print(f"  Basic L1 loss: {loss.item():.4f} ‚úì")
    
    # Test module
    loss_fn = DurationLoss()
    losses = loss_fn(predicted, target)
    
    print(f"  Module L1 loss: {losses['l1'].item():.4f}")
    print(f"  Percent error: {losses['percent_error'].item():.2f}%")
    
    # Test masked loss
    masked_loss_fn = DurationLossWithMask()
    mask = torch.tensor([1, 1, 1, 1, 0, 0, 1, 1])  # Ignore samples 4, 5
    
    masked_losses = masked_loss_fn(predicted, target, mask)
    print(f"  Masked L1 loss: {masked_losses['l1'].item():.4f}")
    
    # Test duration conversions
    duration_sec = torch.tensor([2.0, 3.0, 1.5])
    frames = duration_to_frames(duration_sec)
    print(f"  Duration {duration_sec.tolist()} sec ‚Üí {frames.tolist()} frames")
    
    back_to_sec = frames_to_duration(frames)
    print(f"  Frames {frames.tolist()} ‚Üí {back_to_sec.tolist()} sec")
    
    print("All Duration Loss tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_duration_loss()



==================================================
–§–ê–ô–õ: supertonic/losses/flow_matching_loss.py
–†–û–ó–ú–Ü–†: 15.73 KB
==================================================

"""
Flow Matching Loss –¥–ª—è Text-to-Latent module

Conditional Flow Matching (CFM) loss:
    L_FM = E[||m ¬∑ (v_Œ∏(z_t, z_ref, c, t) - (z_1 - (1-œÉ_min)z_0))||_1]

–î–µ:
- z_0: Noise sample ~ N(0, I)
- z_1: Target latents
- z_t: Interpolated latents = (1 - (1-œÉ_min)t) * z_0 + t * z_1
- v_Œ∏: Predicted velocity field
- m: Reference mask (–¥–ª—è reference masking)
- œÉ_min = 1e-8

Classifier-Free Guidance (CFG):
- p_uncond = 0.05 (5% unconditional training)
- –ü—Ä–∏ unconditional: conditional inputs –∑–∞–º—ñ–Ω—é—é—Ç—å—Å—è –Ω–∞ learnable parameters

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper (2509.11084), Matcha-TTS
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict
import math


def create_reference_mask(
    shape: Tuple[int, ...],
    mask_ratio_min: float = 0.3,
    mask_ratio_max: float = 0.7,
    device: Optional[torch.device] = None
) -> torch.Tensor:
    """
    –°—Ç–≤–æ—Ä—é—î –º–∞—Å–∫—É –¥–ª—è reference masking –≤ flow-matching loss.
    
    Reference masking –∑–∞–ø–æ–±—ñ–≥–∞—î information leakage:
    –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–ø—ñ—é–≤–∞—Ç–∏ reference.
    
    Args:
        shape: Shape of the mask (B, C, T) –∞–±–æ (B, T)
        mask_ratio_min: Minimum ratio to mask
        mask_ratio_max: Maximum ratio to mask
        device: Target device
        
    Returns:
        mask: Binary mask tensor (1 = masked/train, 0 = visible/reference)
    """
    if len(shape) == 3:
        batch_size, channels, seq_len = shape
        mask_shape = (batch_size, 1, seq_len)  # Broadcast over channels
    else:
        batch_size, seq_len = shape
        mask_shape = (batch_size, seq_len)
    
    mask = torch.zeros(mask_shape, device=device)
    
    for b in range(batch_size):
        # Random mask ratio
        ratio = torch.rand(1).item() * (mask_ratio_max - mask_ratio_min) + mask_ratio_min
        mask_len = int(seq_len * ratio)
        
        # Random start position
        if seq_len > mask_len:
            start = torch.randint(0, seq_len - mask_len, (1,)).item()
        else:
            start = 0
            mask_len = seq_len
        
        # Apply mask
        if len(shape) == 3:
            mask[b, :, start:start + mask_len] = 1.0
        else:
            mask[b, start:start + mask_len] = 1.0
    
    return mask


def interpolate_latents(
    z0: torch.Tensor,
    z1: torch.Tensor,
    t: torch.Tensor,
    sigma_min: float = 1e-8
) -> torch.Tensor:
    """
    –Ü–Ω—Ç–µ—Ä–ø–æ–ª—é—î –º—ñ–∂ noise (z0) —Ç–∞ target (z1).
    
    z_t = (1 - (1-œÉ_min)t) * z_0 + t * z_1
    
    Args:
        z0: Noise samples [B, C, T]
        z1: Target latents [B, C, T]
        t: Timesteps [B, 1, 1] –∞–±–æ [B]
        sigma_min: Minimum sigma value
        
    Returns:
        z_t: Interpolated latents [B, C, T]
    """
    # Ensure t has correct shape
    while t.dim() < z0.dim():
        t = t.unsqueeze(-1)
    
    # Interpolation: z_t = (1 - (1-œÉ)t) * z_0 + t * z_1
    coef_z0 = 1 - (1 - sigma_min) * t
    coef_z1 = t
    
    z_t = coef_z0 * z0 + coef_z1 * z1
    
    return z_t


def compute_target_velocity(
    z0: torch.Tensor,
    z1: torch.Tensor,
    sigma_min: float = 1e-8
) -> torch.Tensor:
    """
    –û–±—á–∏—Å–ª—é—î target velocity (straight path).
    
    v_target = z_1 - (1-œÉ_min) * z_0
    
    Args:
        z0: Noise samples [B, C, T]
        z1: Target latents [B, C, T]
        sigma_min: Minimum sigma value
        
    Returns:
        target_velocity: [B, C, T]
    """
    return z1 - (1 - sigma_min) * z0


def flow_matching_loss(
    model: nn.Module,
    z1: torch.Tensor,
    text_encoding: torch.Tensor,
    reference_encoding: torch.Tensor,
    text_mask: Optional[torch.Tensor] = None,
    sigma_min: float = 1e-8,
    p_uncond: float = 0.05,
    mask_ratio_min: float = 0.3,
    mask_ratio_max: float = 0.7
) -> Dict[str, torch.Tensor]:
    """
    Flow-matching loss –¥–ª—è Text-to-Latent training.
    
    Args:
        model: VectorFieldEstimator model
        z1: Target latents [B, C, T]
        text_encoding: Encoded text [B, L, D]
        reference_encoding: Reference vectors [B, 50, D]
        text_mask: Optional text mask [B, L]
        sigma_min: Minimum sigma
        p_uncond: Probability of unconditional training (CFG)
        mask_ratio_min: Minimum reference mask ratio
        mask_ratio_max: Maximum reference mask ratio
        
    Returns:
        Dict with loss components
    """
    batch_size = z1.shape[0]
    device = z1.device
    
    # Sample timestep t ~ U[0, 1]
    t = torch.rand(batch_size, device=device)
    
    # Sample noise z_0 ~ N(0, I)
    z0 = torch.randn_like(z1)
    
    # Interpolate: z_t = (1 - (1-œÉ)t) * z_0 + t * z_1
    z_t = interpolate_latents(z0, z1, t, sigma_min)
    
    # Target velocity (straight path)
    target_velocity = compute_target_velocity(z0, z1, sigma_min)
    
    # Reference masking (prevent information leakage)
    mask = create_reference_mask(
        z1.shape,
        mask_ratio_min=mask_ratio_min,
        mask_ratio_max=mask_ratio_max,
        device=device
    )
    z_ref = (1 - mask) * z1  # Visible reference (unmasked parts)
    
    # Classifier-Free Guidance: sometimes train unconditionally
    uncond_mask = torch.rand(batch_size, device=device) < p_uncond
    
    # For unconditional samples, we null out the conditioning
    text_cond = text_encoding.clone()
    z_ref_cond = z_ref.clone()
    
    if uncond_mask.any():
        # Zero out conditioning for unconditional samples
        text_cond[uncond_mask] = 0
        z_ref_cond[uncond_mask] = 0  # Null reference for unconditional samples
    
    # Predict velocity
    # Note: VectorFieldEstimator uses z_ref (masked latents) for reference conditioning,
    # not the 50-vector reference_encoding.
    predicted_velocity = model(
        z_t=z_t,
        z_ref=z_ref_cond,
        text_encoding=text_cond,
        t=t,
        text_mask=text_mask
    )
    
    # L1 loss –∑ masking (—Ç—ñ–ª—å–∫–∏ –Ω–∞ masked regions)
    loss_per_sample = (mask * (predicted_velocity - target_velocity).abs()).sum(dim=(1, 2))
    loss_per_sample = loss_per_sample / (mask.sum(dim=(1, 2)) + 1e-8)
    
    loss = loss_per_sample.mean()
    
    return {
        "total": loss,
        "flow_matching": loss,
        "mean_velocity_error": (predicted_velocity - target_velocity).abs().mean()
    }


class FlowMatchingLoss(nn.Module):
    """
    Flow Matching Loss module.
    
    Encapsulates flow-matching loss computation –∑ configurable parameters.
    
    Args:
        sigma_min: Minimum sigma value (default 1e-8)
        p_uncond: Unconditional training probability for CFG (default 0.05)
        mask_ratio_min: Minimum reference mask ratio
        mask_ratio_max: Maximum reference mask ratio
    """
    
    def __init__(
        self,
        sigma_min: float = 1e-8,
        p_uncond: float = 0.05,
        mask_ratio_min: float = 0.3,
        mask_ratio_max: float = 0.7
    ):
        super().__init__()
        
        self.sigma_min = sigma_min
        self.p_uncond = p_uncond
        self.mask_ratio_min = mask_ratio_min
        self.mask_ratio_max = mask_ratio_max
    
    def forward(
        self,
        model: nn.Module,
        z1: torch.Tensor,
        text_encoding: torch.Tensor,
        reference_encoding: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute flow-matching loss.
        
        Args:
            model: VectorFieldEstimator
            z1: Target compressed latents [B, 144, T]
            text_encoding: Encoded text [B, L, D]
            reference_encoding: Reference vectors [B, 50, D]
            text_mask: Optional text mask
            
        Returns:
            Loss dict
        """
        return flow_matching_loss(
            model=model,
            z1=z1,
            text_encoding=text_encoding,
            reference_encoding=reference_encoding,
            text_mask=text_mask,
            sigma_min=self.sigma_min,
            p_uncond=self.p_uncond,
            mask_ratio_min=self.mask_ratio_min,
            mask_ratio_max=self.mask_ratio_max
        )


class ODESolver:
    """
    Euler ODE Solver –¥–ª—è inference.
    
    –ì–µ–Ω–µ—Ä—É—î –ª–∞—Ç–µ–Ω—Ç–∏ –∑ noise —á–µ—Ä–µ–∑ Euler integration:
        z_{i+1} = z_i + v(z_i, t_i) * dt
    
    Args:
        nfe: Number of function evaluations (default 32)
        cfg_scale: Classifier-free guidance scale (default 3.0)
    """
    
    def __init__(
        self,
        nfe: int = 32,
        cfg_scale: float = 3.0
    ):
        self.nfe = nfe
        self.cfg_scale = cfg_scale
    
    @torch.no_grad()
    def solve(
        self,
        model: nn.Module,
        z_shape: Tuple[int, ...],
        text_encoding: torch.Tensor,
        reference_encoding: torch.Tensor,
        z_ref: Optional[torch.Tensor] = None,
        text_mask: Optional[torch.Tensor] = None,
        device: Optional[torch.device] = None
    ) -> torch.Tensor:
        """
        Solve ODE to generate latents.
        
        Args:
            model: VectorFieldEstimator
            z_shape: Shape of latents to generate (B, 144, T)
            text_encoding: Encoded text [B, L, D]
            reference_encoding: Reference vectors [B, 50, D]
            z_ref: Optional reference latents for conditioning
            text_mask: Optional text mask
            device: Target device
            
        Returns:
            Generated latents [B, 144, T]
        """
        device = device or text_encoding.device
        
        # Initialize from noise
        z = torch.randn(z_shape, device=device)
        
        if z_ref is None:
            z_ref = torch.zeros_like(z)
        
        # Euler integration from t=0 to t=1
        dt = 1.0 / self.nfe
        
        for step in range(self.nfe):
            t = torch.full((z_shape[0],), step * dt, device=device)
            
            # Conditional velocity
            velocity_cond = model(
                z_t=z,
                z_ref=z_ref,
                text_encoding=text_encoding,
                t=t,
                text_mask=text_mask
            )
            
            # Unconditional velocity (for CFG)
            if self.cfg_scale > 1.0:
                # Null conditioning
                text_null = torch.zeros_like(text_encoding)
                z_ref_null = torch.zeros_like(z_ref)
                
                velocity_uncond = model(
                    z_t=z,
                    z_ref=z_ref_null,
                    text_encoding=text_null,
                    t=t,
                    text_mask=text_mask
                )
                
                # CFG: v = v_uncond + scale * (v_cond - v_uncond)
                velocity = velocity_uncond + self.cfg_scale * (velocity_cond - velocity_uncond)
            else:
                velocity = velocity_cond
            
            # Euler step
            z = z + velocity * dt
        
        return z


# ============================================================================
# Temporal Compression utilities
# ============================================================================

def compress_latents(
    latents: torch.Tensor,
    compression_factor: int = 6
) -> torch.Tensor:
    """
    Temporal compression: —Å—Ç–µ–∫ Kc —Ñ—Ä–µ–π–º—ñ–≤ –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä.
    
    (C=24, T) ‚Üí (C√óKc=144, T/Kc)
    
    Args:
        latents: [B, C, T] –¥–µ C=24
        compression_factor: Kc (default 6)
        
    Returns:
        compressed: [B, C*Kc, T//Kc]
    """
    batch_size, channels, seq_len = latents.shape
    
    # Pad to multiple of compression_factor
    if seq_len % compression_factor != 0:
        pad_len = compression_factor - (seq_len % compression_factor)
        latents = F.pad(latents, (0, pad_len))
        seq_len = latents.shape[-1]
    
    # Reshape: [B, C, T] ‚Üí [B, C, T/Kc, Kc] ‚Üí [B, C*Kc, T/Kc]
    latents = latents.view(batch_size, channels, seq_len // compression_factor, compression_factor)
    latents = latents.permute(0, 1, 3, 2)  # [B, C, Kc, T/Kc]
    latents = latents.reshape(batch_size, channels * compression_factor, seq_len // compression_factor)
    
    return latents


def decompress_latents(
    compressed: torch.Tensor,
    compression_factor: int = 6
) -> torch.Tensor:
    """
    Temporal decompression: reverse of compress_latents.
    
    (C√óKc=144, T/Kc) ‚Üí (C=24, T)
    
    Args:
        compressed: [B, C*Kc, T_compressed]
        compression_factor: Kc (default 6)
        
    Returns:
        latents: [B, C, T]
    """
    batch_size, compressed_channels, compressed_len = compressed.shape
    channels = compressed_channels // compression_factor
    seq_len = compressed_len * compression_factor
    
    # Reshape: [B, C*Kc, T/Kc] ‚Üí [B, C, Kc, T/Kc] ‚Üí [B, C, T]
    compressed = compressed.view(batch_size, channels, compression_factor, compressed_len)
    compressed = compressed.permute(0, 1, 3, 2)  # [B, C, T/Kc, Kc]
    latents = compressed.reshape(batch_size, channels, seq_len)
    
    return latents


# ============================================================================
# Unit tests
# ============================================================================

def _test_flow_matching():
    """–¢–µ—Å—Ç flow matching loss."""
    print("Testing Flow Matching Loss...")
    
    batch_size = 4
    latent_dim = 144
    seq_len = 50
    text_len = 100
    hidden_dim = 128
    
    # Test interpolation
    z0 = torch.randn(batch_size, latent_dim, seq_len)
    z1 = torch.randn(batch_size, latent_dim, seq_len)
    t = torch.rand(batch_size)
    
    z_t = interpolate_latents(z0, z1, t, sigma_min=1e-8)
    assert z_t.shape == z0.shape
    print(f"  Interpolation: {z0.shape} ‚Üí {z_t.shape} ‚úì")
    
    # Test target velocity
    target_v = compute_target_velocity(z0, z1, sigma_min=1e-8)
    assert target_v.shape == z0.shape
    print(f"  Target velocity: {target_v.shape} ‚úì")
    
    # Test reference masking
    mask = create_reference_mask((batch_size, latent_dim, seq_len))
    assert mask.shape == (batch_size, 1, seq_len)
    print(f"  Reference mask: {mask.shape}, ratio={mask.mean().item():.2f} ‚úì")
    
    # Test compression/decompression
    latents_24 = torch.randn(batch_size, 24, 300)
    compressed = compress_latents(latents_24, compression_factor=6)
    assert compressed.shape == (batch_size, 144, 50)
    print(f"  Compression: {latents_24.shape} ‚Üí {compressed.shape} ‚úì")
    
    decompressed = decompress_latents(compressed, compression_factor=6)
    assert decompressed.shape == latents_24.shape
    print(f"  Decompression: {compressed.shape} ‚Üí {decompressed.shape} ‚úì")
    
    # Test ODE solver (mock model)
    class MockVF(nn.Module):
        def forward(self, z_t, z_ref, text_encoding, t, text_mask=None):
            return torch.randn_like(z_t)
    
    mock_model = MockVF()
    solver = ODESolver(nfe=8, cfg_scale=3.0)
    
    text_enc = torch.randn(batch_size, text_len, hidden_dim)
    ref_enc = torch.randn(batch_size, 50, hidden_dim)
    
    generated = solver.solve(
        model=mock_model,
        z_shape=(batch_size, latent_dim, seq_len),
        text_encoding=text_enc,
        reference_encoding=ref_enc
    )
    
    assert generated.shape == (batch_size, latent_dim, seq_len)
    print(f"  ODE solver: generated shape {generated.shape} ‚úì")
    
    print("All Flow Matching tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_flow_matching()



==================================================
–§–ê–ô–õ: supertonic/models/__init__.py
–†–û–ó–ú–Ü–†: 1.01 KB
==================================================

"""
Supertonic v2 - –ú–æ–¥—É–ª—ñ –º–æ–¥–µ–ª–µ–π

–¢—Ä–∏ –∫–ª—é—á–æ–≤—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:
1. SpeechAutoencoder - –∫–æ–¥—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä
2. TextToLatent - –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –ª–∞—Ç–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É —á–µ—Ä–µ–∑ flow matching
3. DurationPredictor - –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ utterance
"""

from supertonic.models.speech_autoencoder import SpeechAutoencoder
from supertonic.models.text_to_latent import TextToLatent
from supertonic.models.duration_predictor import DurationPredictor
from supertonic.models.convnext import ConvNeXtBlock, ConvNeXtStack
from supertonic.models.attention import (
    MultiHeadAttention,
    SelfAttentionBlock,
    CrossAttentionBlock,
)
from supertonic.models.larope import LARoPE, apply_larope

__all__ = [
    "SpeechAutoencoder",
    "TextToLatent",
    "DurationPredictor",
    "ConvNeXtBlock",
    "ConvNeXtStack",
    "MultiHeadAttention",
    "SelfAttentionBlock",
    "CrossAttentionBlock",
    "LARoPE",
    "apply_larope",
]



==================================================
–§–ê–ô–õ: supertonic/models/attention.py
–†–û–ó–ú–Ü–†: 15.1 KB
==================================================

"""
Attention –º–æ–¥—É–ª—ñ –¥–ª—è Supertonic v2

–í–∫–ª—é—á–∞—î:
- MultiHeadAttention –∑ RoPE (–¥–ª—è self-attention)
- SelfAttentionBlock (ConvNeXt-style wrapper)
- CrossAttentionBlock –∑ LARoPE (–¥–ª—è text-speech alignment)

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper - Text Encoder –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î 4 self-attention –±–ª–æ–∫–∏
–∑ RoPE, Reference/Text Encoder –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å cross-attention –¥–ª—è conditioning.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple
from einops import rearrange

from supertonic.models.larope import LARoPE, apply_larope, LARoPECrossAttention


class RotaryEmbedding(nn.Module):
    """
    Standard Rotary Position Embedding (RoPE) –¥–ª—è self-attention.

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ Text Encoder –¥–ª—è self-attention –º—ñ–∂ —Ç–æ–∫–µ–Ω–∞–º–∏.

    Args:
        dim: –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å per head (–º–∞—î –±—É—Ç–∏ –ø–∞—Ä–Ω–∏–º)
        base: –ë–∞–∑–∞ –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–∏—Ö —Å–º—É–≥ (default 10000)
        max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
    """

    def __init__(
        self,
        dim: int,
        base: float = 10000.0,
        max_seq_len: int = 8192
    ):
        super().__init__()

        assert dim % 2 == 0, f"Dimension must be even, got {dim}"

        self.dim = dim
        self.base = base
        self.max_seq_len = max_seq_len

        # Precompute inverse frequencies
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

        # Cache sin/cos values
        self._build_cache(max_seq_len)

    def _build_cache(self, seq_len: int):
        """–ü–æ–±—É–¥–æ–≤–∞ –∫–µ—à—É sin/cos –∑–Ω–∞—á–µ–Ω—å."""
        positions = torch.arange(seq_len)
        angles = positions.unsqueeze(-1).float() * self.inv_freq.unsqueeze(0)

        cos_cache = torch.cos(angles)
        sin_cache = torch.sin(angles)

        self.register_buffer("cos_cache", cos_cache, persistent=False)
        self.register_buffer("sin_cache", sin_cache, persistent=False)

    def forward(
        self,
        x: torch.Tensor,
        seq_len: Optional[int] = None
    ) -> torch.Tensor:
        """
        –ó–∞—Å—Ç–æ—Å–æ–≤—É—î RoPE –¥–æ input tensor.

        Args:
            x: Input tensor [B, H, T, D] –∞–±–æ [B, T, D]
            seq_len: Optional sequence length (for offset)

        Returns:
            Rotated tensor —Ç—ñ—î—ó –∂ —Ñ–æ—Ä–º–∏
        """
        if x.dim() == 3:
            batch_size, t, dim = x.shape
            x = x.unsqueeze(1)
            squeeze = True
        else:
            batch_size, num_heads, t, dim = x.shape
            squeeze = False

        # –û—Ç—Ä–∏–º—É—î–º–æ sin/cos –∑ –∫–µ—à—É
        if t > self.max_seq_len:
            self._build_cache(t)

        cos = self.cos_cache[:t].unsqueeze(0).unsqueeze(0)  # [1, 1, T, D/2]
        sin = self.sin_cache[:t].unsqueeze(0).unsqueeze(0)

        # Apply rotation
        x_even, x_odd = x[..., 0::2], x[..., 1::2]
        x_rotated_even = x_even * cos - x_odd * sin
        x_rotated_odd = x_even * sin + x_odd * cos

        x_rotated = torch.stack([x_rotated_even, x_rotated_odd], dim=-1).flatten(-2)

        if squeeze:
            x_rotated = x_rotated.squeeze(1)

        return x_rotated


class MultiHeadAttention(nn.Module):
    """
    Multi-Head Self-Attention –∑ RoPE.

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ Text Encoder (4 –±–ª–æ–∫–∏, 4 heads, 512 filters).

    Args:
        dim: Model dimension
        num_heads: Number of attention heads
        head_dim: Dimension per head
        dropout: Attention dropout
        use_rope: Whether to use Rotary Position Embedding
    """

    def __init__(
        self,
        dim: int,
        num_heads: int = 4,
        head_dim: Optional[int] = None,
        dropout: float = 0.0,
        use_rope: bool = True,
        bias: bool = True
    ):
        super().__init__()

        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = head_dim or dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.use_rope = use_rope

        inner_dim = self.num_heads * self.head_dim

        # QKV projection
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=bias)

        # RoPE
        if use_rope:
            self.rope = RotaryEmbedding(dim=self.head_dim)

        # Output projection
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )

        self.attn_dropout = nn.Dropout(dropout)

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        causal: bool = False
    ) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor [B, T, D]
            mask: Optional attention mask [B, T] –∞–±–æ [B, T, T]
            causal: Whether to use causal attention

        Returns:
            Output tensor [B, T, D]
        """
        batch_size, seq_len, _ = x.shape

        # Project to Q, K, V
        qkv = self.to_qkv(x)
        qkv = rearrange(qkv, 'b l (three h d) -> three b h l d',
                        three=3, h=self.num_heads)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Apply RoPE
        if self.use_rope:
            q = self.rope(q)
            k = self.rope(k)

        # Scaled dot-product attention
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale

        # Causal mask
        if causal:
            causal_mask = torch.triu(
                torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool),
                diagonal=1
            )
            attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))

        # Apply custom mask
        if mask is not None:
            if mask.dim() == 2:
                # [B, T] -> [B, 1, 1, T]
                mask = mask.unsqueeze(1).unsqueeze(2)
            elif mask.dim() == 3:
                # [B, T, T] -> [B, 1, T, T]
                mask = mask.unsqueeze(1)
            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))

        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)

        # Weighted sum
        out = torch.matmul(attn_weights, v)
        out = rearrange(out, 'b h l d -> b l (h d)')

        return self.to_out(out)


class SelfAttentionBlock(nn.Module):
    """
    Self-Attention –±–ª–æ–∫ –∑ pre-norm —Ç–∞ residual connection.

    –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
    1. LayerNorm
    2. MultiHeadAttention –∑ RoPE
    3. Residual connection

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ Text Encoder: 4 –±–ª–æ–∫–∏ –ø—ñ—Å–ª—è ConvNeXt —Å—Ç–µ–∫—É.

    Args:
        dim: Model dimension
        num_heads: Number of attention heads
        dropout: Dropout rate
    """

    def __init__(
        self,
        dim: int,
        num_heads: int = 4,
        dropout: float = 0.0,
        use_rope: bool = True
    ):
        super().__init__()

        self.norm = nn.LayerNorm(dim)
        self.attn = MultiHeadAttention(
            dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            use_rope=use_rope
        )

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor [B, T, D]
            mask: Optional attention mask

        Returns:
            Output tensor [B, T, D]
        """
        # Pre-norm + attention + residual
        return x + self.attn(self.norm(x), mask=mask)


class CrossAttentionBlock(nn.Module):
    """
    Cross-Attention –±–ª–æ–∫ –∑ LARoPE –¥–ª—è text-speech conditioning.

    –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
    1. LayerNorm (for query)
    2. LARoPECrossAttention
    3. Residual connection

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤:
    - Reference Encoder: 2 cross-attention layers
    - Text Encoder: 2 cross-attention layers –∑ reference vectors

    Args:
        dim: Model dimension
        context_dim: Context (key/value) dimension (default: same as dim)
        num_heads: Number of attention heads
        gamma: LARoPE gamma parameter
        dropout: Dropout rate
    """

    def __init__(
        self,
        dim: int,
        context_dim: Optional[int] = None,
        num_heads: int = 4,
        gamma: float = 10.0,
        dropout: float = 0.0
    ):
        super().__init__()

        context_dim = context_dim or dim

        self.norm_q = nn.LayerNorm(dim)
        self.norm_kv = nn.LayerNorm(context_dim)

        self.cross_attn = LARoPECrossAttention(
            dim=dim,
            num_heads=num_heads,
            gamma=gamma,
            dropout=dropout
        )

        # Optional: projection if context_dim != dim
        if context_dim != dim:
            self.context_proj = nn.Linear(context_dim, dim)
        else:
            self.context_proj = nn.Identity()

    def forward(
        self,
        x: torch.Tensor,           # Query [B, Lq, D]
        context: torch.Tensor,     # Key/Value [B, Lk, D_context]
        x_mask: Optional[torch.Tensor] = None,
        context_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Query tensor [B, Lq, D]
            context: Context tensor [B, Lk, D_context]
            x_mask: Optional mask for queries
            context_mask: Optional mask for context

        Returns:
            Output tensor [B, Lq, D]
        """
        # Normalize
        x_norm = self.norm_q(x)
        context_norm = self.norm_kv(context)
        context_proj = self.context_proj(context_norm)

        # Cross-attention –∑ LARoPE + residual
        out = self.cross_attn(x_norm, context_proj, x_mask, context_mask)
        return x + out


class FeedForward(nn.Module):
    """
    Feed-Forward Network –¥–ª—è Transformer-style –±–ª–æ–∫—ñ–≤.

    –°—Ç—Ä—É–∫—Ç—É—Ä–∞: Linear ‚Üí GELU ‚Üí Dropout ‚Üí Linear ‚Üí Dropout

    Args:
        dim: Input/output dimension
        hidden_dim: Hidden dimension (default: dim * 4)
        dropout: Dropout rate
    """

    def __init__(
        self,
        dim: int,
        hidden_dim: Optional[int] = None,
        dropout: float = 0.0
    ):
        super().__init__()

        hidden_dim = hidden_dim or dim * 4

        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class TransformerBlock(nn.Module):
    """
    Standard Transformer –±–ª–æ–∫: Self-Attention + FFN.

    Args:
        dim: Model dimension
        num_heads: Number of attention heads
        ff_mult: FFN hidden dim multiplier
        dropout: Dropout rate
    """

    def __init__(
        self,
        dim: int,
        num_heads: int = 4,
        ff_mult: int = 4,
        dropout: float = 0.0,
        use_rope: bool = True
    ):
        super().__init__()

        self.attn_block = SelfAttentionBlock(
            dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            use_rope=use_rope
        )

        self.ff_norm = nn.LayerNorm(dim)
        self.ff = FeedForward(
            dim=dim,
            hidden_dim=dim * ff_mult,
            dropout=dropout
        )

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        x = self.attn_block(x, mask)
        x = x + self.ff(self.ff_norm(x))
        return x


# ============================================================================
# Unit tests
# ============================================================================

def _test_rotary_embedding():
    """–¢–µ—Å—Ç RoPE."""
    print("Testing RotaryEmbedding...")

    batch_size = 2
    num_heads = 4
    seq_len = 100
    head_dim = 64

    rope = RotaryEmbedding(dim=head_dim)

    # Test 3D input
    x_3d = torch.randn(batch_size, seq_len, head_dim)
    y_3d = rope(x_3d)
    assert y_3d.shape == x_3d.shape
    print(f"  3D: {x_3d.shape} -> {y_3d.shape} ‚úì")

    # Test 4D input
    x_4d = torch.randn(batch_size, num_heads, seq_len, head_dim)
    y_4d = rope(x_4d)
    assert y_4d.shape == x_4d.shape
    print(f"  4D: {x_4d.shape} -> {y_4d.shape} ‚úì")

    print("RotaryEmbedding tests passed! ‚úì\n")


def _test_multi_head_attention():
    """–¢–µ—Å—Ç MultiHeadAttention."""
    print("Testing MultiHeadAttention...")

    batch_size = 2
    seq_len = 100
    dim = 512
    num_heads = 4

    attn = MultiHeadAttention(
        dim=dim,
        num_heads=num_heads,
        use_rope=True
    )

    x = torch.randn(batch_size, seq_len, dim)
    out = attn(x)
    assert out.shape == x.shape
    print(f"  Basic: {x.shape} -> {out.shape} ‚úì")

    # With causal mask
    out_causal = attn(x, causal=True)
    assert out_causal.shape == x.shape
    print(f"  Causal: {out_causal.shape} ‚úì")

    # With padding mask
    mask = torch.ones(batch_size, seq_len, dtype=torch.bool)
    mask[0, 80:] = False
    out_masked = attn(x, mask=mask)
    assert out_masked.shape == x.shape
    print(f"  Masked: {out_masked.shape} ‚úì")

    num_params = sum(p.numel() for p in attn.parameters())
    print(f"  Parameters: {num_params:,}")

    print("MultiHeadAttention tests passed! ‚úì\n")


def _test_self_attention_block():
    """–¢–µ—Å—Ç SelfAttentionBlock."""
    print("Testing SelfAttentionBlock...")

    batch_size = 2
    seq_len = 100
    dim = 512

    block = SelfAttentionBlock(dim=dim, num_heads=4)

    x = torch.randn(batch_size, seq_len, dim)
    out = block(x)
    assert out.shape == x.shape
    print(f"  {x.shape} -> {out.shape} ‚úì")

    print("SelfAttentionBlock tests passed! ‚úì\n")


def _test_cross_attention_block():
    """–¢–µ—Å—Ç CrossAttentionBlock."""
    print("Testing CrossAttentionBlock...")

    batch_size = 2
    dim = 512

    # Speech features (query)
    lq = 100
    x = torch.randn(batch_size, lq, dim)

    # Text embeddings (context)
    lk = 20
    context = torch.randn(batch_size, lk, dim)

    block = CrossAttentionBlock(
        dim=dim,
        num_heads=4,
        gamma=10.0
    )

    out = block(x, context)
    assert out.shape == x.shape
    print(f"  Query: {x.shape}, Context: {context.shape} -> {out.shape} ‚úì")

    num_params = sum(p.numel() for p in block.parameters())
    print(f"  Parameters: {num_params:,}")

    print("CrossAttentionBlock tests passed! ‚úì\n")


def _test_transformer_block():
    """–¢–µ—Å—Ç TransformerBlock."""
    print("Testing TransformerBlock...")

    batch_size = 2
    seq_len = 100
    dim = 512

    block = TransformerBlock(
        dim=dim,
        num_heads=4,
        ff_mult=4
    )

    x = torch.randn(batch_size, seq_len, dim)
    out = block(x)
    assert out.shape == x.shape
    print(f"  {x.shape} -> {out.shape} ‚úì")

    num_params = sum(p.numel() for p in block.parameters())
    print(f"  Parameters: {num_params:,}")

    print("TransformerBlock tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_rotary_embedding()
    _test_multi_head_attention()
    _test_self_attention_block()
    _test_cross_attention_block()
    _test_transformer_block()
    print("All attention tests passed! ‚úì")



==================================================
–§–ê–ô–õ: supertonic/models/convnext.py
–†–û–ó–ú–Ü–†: 11.9 KB
==================================================

"""
ConvNeXt Block - –±–∞–∑–æ–≤–∏–π –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–π –±–ª–æ–∫ –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥—É–ª—ñ–≤ Supertonic v2

ConvNeXt ‚Äî –º–æ–¥–µ—Ä–Ω—ñ–∑–æ–≤–∞–Ω–∞ CNN –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑ transformer-–ø–æ–¥—ñ–±–Ω–∏–º–∏ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏:
- Depthwise separable convolution
- Inverted bottleneck (expand 4x)
- Layer scale –¥–ª—è —Å—Ç–∞–±—ñ–ª—ñ–∑–∞—Ü—ñ—ó –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂
- GELU activation

–†–µ—Ñ–µ—Ä–µ–Ω—Å: "A ConvNet for the 2020s" (Liu et al., 2022)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List, Tuple
from einops import rearrange


class LayerNorm1d(nn.Module):
    """
    Layer Normalization –¥–ª—è 1D –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π.
    –ü—ñ–¥—Ç—Ä–∏–º—É—î —Ñ–æ—Ä–º–∞—Ç channels-first [B, C, T] —Ç–∞ channels-last [B, T, C].
    """

    def __init__(
        self,
        normalized_shape: int,
        eps: float = 1e-6,
        data_format: str = "channels_first"
    ):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        self.normalized_shape = (normalized_shape,)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            # [B, C, T] -> normalize over C
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None] * x + self.bias[:, None]
            return x
        else:
            raise ValueError(f"Unknown data_format: {self.data_format}")


class ConvNeXtBlock(nn.Module):
    """
    ConvNeXt Block ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–π –±–ª–æ–∫ Supertonic v2.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    1. Depthwise convolution (groups=dim) ‚Äî –ª–æ–∫–∞–ª—å–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏
    2. LayerNorm
    3. Pointwise conv 1 (expand 4x) ‚Äî inverted bottleneck
    4. GELU activation
    5. Pointwise conv 2 (contract back)
    6. Layer scale (Œ≥ ‚âà 1e-6 init –¥–ª—è –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂)
    7. Residual connection

    Args:
        dim: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–Ω–∞–ª—ñ–≤ (–≤—Ö—ñ–¥ = –≤–∏—Ö—ñ–¥)
        intermediate_dim: –†–æ–∑–º—ñ—Ä –ø—Ä–∏—Ö–æ–≤–∞–Ω–æ–≥–æ —à–∞—Ä—É (default: dim * 4)
        kernel_size: –†–æ–∑–º—ñ—Ä —è–¥—Ä–∞ depthwise conv (default: 7)
        dilation: Dilation factor –¥–ª—è dilated convolutions
        layer_scale_init: –ü–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è layer scale
        causal: –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ causal convolution (–¥–ª—è streaming)
    """

    def __init__(
        self,
        dim: int,
        intermediate_dim: Optional[int] = None,
        kernel_size: int = 7,
        dilation: int = 1,
        layer_scale_init: float = 1e-6,
        causal: bool = False,
        dropout: float = 0.0
    ):
        super().__init__()

        intermediate_dim = intermediate_dim or dim * 4

        # Causal padding: –≤–µ—Å—å padding –∑–ª—ñ–≤–∞
        effective_kernel = kernel_size + (kernel_size - 1) * (dilation - 1)
        if causal:
            self.padding = (effective_kernel - 1, 0)  # Left padding only
        else:
            self.padding = ((effective_kernel - 1) // 2, (effective_kernel - 1) // 2)

        self.causal = causal
        self.kernel_size = kernel_size
        self.dilation = dilation

        # Depthwise convolution (groups=dim –¥–ª—è spatial mixing)
        # –ù–µ –≤–∫–ª—é—á–∞—î–º–æ padding —Ç—É—Ç ‚Äî —Ä–æ–±–∏–º–æ –≤—Ä—É—á–Ω—É –¥–ª—è causal
        self.dwconv = nn.Conv1d(
            dim, dim,
            kernel_size=kernel_size,
            padding=0,  # Manual padding
            groups=dim,
            dilation=dilation
        )

        # LayerNorm —É channels-last —Ñ–æ—Ä–º–∞—Ç—ñ
        self.norm = nn.LayerNorm(dim, eps=1e-6)

        # Inverted bottleneck: expand ‚Üí GELU ‚Üí contract
        self.pwconv1 = nn.Linear(dim, intermediate_dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(intermediate_dim, dim)

        # Layer scale ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –º–∞–ª–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        self.gamma = nn.Parameter(layer_scale_init * torch.ones(dim))

        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor [B, C, T]

        Returns:
            Output tensor [B, C, T] (same shape)
        """
        residual = x

        # Manual padding –¥–ª—è causal conv
        x = F.pad(x, self.padding)

        # Depthwise convolution
        x = self.dwconv(x)

        # [B, C, T] -> [B, T, C] –¥–ª—è LayerNorm —Ç–∞ Linear
        x = x.transpose(1, 2)
        x = self.norm(x)

        # Inverted bottleneck
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)

        # Layer scale
        x = self.gamma * x
        x = self.dropout(x)

        # [B, T, C] -> [B, C, T]
        x = x.transpose(1, 2)

        # Residual connection
        return residual + x


class ConvNeXtStack(nn.Module):
    """
    –°—Ç–µ–∫ ConvNeXt –±–ª–æ–∫—ñ–≤ –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º–∏ dilations.

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤:
    - Latent Encoder: 10 blocks, dilations=[1]*10
    - Latent Decoder: 10 blocks, dilations=[1,2,4,1,2,4,1,1,1,1]
    - Text Encoder: 6 blocks
    - Reference Encoder: 6 blocks
    - VF Estimator: 8 blocks –∑ dilations

    Args:
        dim: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–Ω–∞–ª—ñ–≤
        num_blocks: –ö—ñ–ª—å–∫—ñ—Å—Ç—å ConvNeXt –±–ª–æ–∫—ñ–≤
        intermediate_dim: –†–æ–∑–º—ñ—Ä –ø—Ä–∏—Ö–æ–≤–∞–Ω–æ–≥–æ —à–∞—Ä—É
        kernel_size: –†–æ–∑–º—ñ—Ä —è–¥—Ä–∞
        dilations: –°–ø–∏—Å–æ–∫ dilation factors –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –±–ª–æ–∫—É
        causal: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ causal convolutions
        dropout: Dropout rate
        gradient_checkpointing: Use gradient checkpointing to save memory
    """

    def __init__(
        self,
        dim: int,
        num_blocks: int,
        intermediate_dim: Optional[int] = None,
        kernel_size: int = 7,
        dilations: Optional[List[int]] = None,
        causal: bool = False,
        dropout: float = 0.0,
        layer_scale_init: float = 1e-6,
        gradient_checkpointing: bool = False
    ):
        super().__init__()
        
        self.gradient_checkpointing = gradient_checkpointing

        # –Ø–∫—â–æ dilations –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî –≤—Å—ñ 1
        if dilations is None:
            dilations = [1] * num_blocks
        else:
            assert len(dilations) == num_blocks, \
                f"Dilations length ({len(dilations)}) must match num_blocks ({num_blocks})"

        self.blocks = nn.ModuleList([
            ConvNeXtBlock(
                dim=dim,
                intermediate_dim=intermediate_dim,
                kernel_size=kernel_size,
                dilation=dilations[i],
                layer_scale_init=layer_scale_init,
                causal=causal,
                dropout=dropout
            )
            for i in range(num_blocks)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass —á–µ—Ä–µ–∑ –≤—Å—ñ –±–ª–æ–∫–∏.

        Args:
            x: Input tensor [B, C, T]

        Returns:
            Output tensor [B, C, T]
        """
        for block in self.blocks:
            if self.gradient_checkpointing and self.training:
                x = torch.utils.checkpoint.checkpoint(block, x, use_reentrant=False)
            else:
                x = block(x)
        return x


class ConvNeXtDownsample(nn.Module):
    """
    Downsampling –º–æ–¥—É–ª—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ ConvNeXt.
    –ó–º–µ–Ω—à—É—î temporal resolution.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        factor: int = 2
    ):
        super().__init__()
        self.norm = LayerNorm1d(in_channels, data_format="channels_first")
        self.conv = nn.Conv1d(
            in_channels, out_channels,
            kernel_size=factor,
            stride=factor
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.norm(x)
        x = self.conv(x)
        return x


class ConvNeXtUpsample(nn.Module):
    """
    Upsampling –º–æ–¥—É–ª—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ ConvNeXt.
    –ó–±—ñ–ª—å—à—É—î temporal resolution.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        factor: int = 2
    ):
        super().__init__()
        self.norm = LayerNorm1d(in_channels, data_format="channels_first")
        self.conv = nn.ConvTranspose1d(
            in_channels, out_channels,
            kernel_size=factor,
            stride=factor
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.norm(x)
        x = self.conv(x)
        return x


# ============================================================================
# Unit tests
# ============================================================================

def _test_convnext_block():
    """–¢–µ—Å—Ç ConvNeXt –±–ª–æ–∫—É."""
    print("Testing ConvNeXtBlock...")

    batch_size = 4
    channels = 512
    seq_len = 100

    # Standard block
    block = ConvNeXtBlock(
        dim=channels,
        intermediate_dim=2048,
        kernel_size=7,
        dilation=1,
        causal=False
    )

    x = torch.randn(batch_size, channels, seq_len)
    y = block(x)

    assert y.shape == x.shape, f"Shape mismatch: {y.shape} vs {x.shape}"
    print(f"  Standard block: input {x.shape} -> output {y.shape} ‚úì")

    # Causal block
    block_causal = ConvNeXtBlock(
        dim=channels,
        kernel_size=7,
        dilation=1,
        causal=True
    )

    y_causal = block_causal(x)
    assert y_causal.shape == x.shape, f"Causal shape mismatch"
    print(f"  Causal block: input {x.shape} -> output {y_causal.shape} ‚úì")

    # Dilated block
    block_dilated = ConvNeXtBlock(
        dim=channels,
        kernel_size=7,
        dilation=4,
        causal=False
    )

    y_dilated = block_dilated(x)
    assert y_dilated.shape == x.shape, f"Dilated shape mismatch"
    print(f"  Dilated block (d=4): input {x.shape} -> output {y_dilated.shape} ‚úì")

    # Parameter count
    num_params = sum(p.numel() for p in block.parameters())
    print(f"  Parameters per block: {num_params:,}")

    print("ConvNeXtBlock tests passed! ‚úì\n")


def _test_convnext_stack():
    """–¢–µ—Å—Ç —Å—Ç–µ–∫—É ConvNeXt –±–ª–æ–∫—ñ–≤."""
    print("Testing ConvNeXtStack...")

    batch_size = 4
    channels = 512
    seq_len = 100

    # Encoder-style stack (no dilation)
    encoder_stack = ConvNeXtStack(
        dim=channels,
        num_blocks=10,
        intermediate_dim=2048,
        kernel_size=7,
        dilations=None,
        causal=False
    )

    x = torch.randn(batch_size, channels, seq_len)
    y = encoder_stack(x)

    assert y.shape == x.shape
    print(f"  Encoder stack (10 blocks): {x.shape} -> {y.shape} ‚úì")

    # Decoder-style stack (with dilations)
    decoder_dilations = [1, 2, 4, 1, 2, 4, 1, 1, 1, 1]
    decoder_stack = ConvNeXtStack(
        dim=channels,
        num_blocks=10,
        intermediate_dim=2048,
        kernel_size=7,
        dilations=decoder_dilations,
        causal=True  # Decoder is causal
    )

    y_decoder = decoder_stack(x)
    assert y_decoder.shape == x.shape
    print(f"  Decoder stack (10 blocks, dilated, causal): {x.shape} -> {y_decoder.shape} ‚úì")

    # Parameter count
    encoder_params = sum(p.numel() for p in encoder_stack.parameters())
    decoder_params = sum(p.numel() for p in decoder_stack.parameters())
    print(f"  Encoder params: {encoder_params:,}")
    print(f"  Decoder params: {decoder_params:,}")

    print("ConvNeXtStack tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_convnext_block()
    _test_convnext_stack()
    print("All ConvNeXt tests passed! ‚úì")



==================================================
–§–ê–ô–õ: supertonic/models/duration_predictor.py
–†–û–ó–ú–Ü–†: 16.67 KB
==================================================

"""
Duration Predictor - –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ—ó —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ utterance

–ö–ª—é—á–æ–≤–∞ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è Supertonic v2: –ø–µ—Ä–µ–¥–±–∞—á–∞—î UTTERANCE-LEVEL duration,
–∞ –Ω–µ per-phoneme durations —è–∫ —É —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏—Ö TTS —Å–∏—Å—Ç–µ–º–∞—Ö.

–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (~0.5M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤):
- Text Encoder: ConvNeXt blocks + attention ‚Üí utterance embedding
- Reference Encoder: ConvNeXt blocks + attention ‚Üí reference embedding
- Concatenate + MLP ‚Üí scalar duration (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö –∞–±–æ —Ñ—Ä–µ–π–º–∞—Ö)

Training:
- L1 loss –Ω–∞ ground-truth duration
- –®–≤–∏–¥–∫–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: ~3000 iterations

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Tuple
from einops import rearrange

from supertonic.models.convnext import ConvNeXtBlock, ConvNeXtStack
from supertonic.models.attention import MultiHeadAttention


class DurationTextEncoder(nn.Module):
    """
    Text Encoder –¥–ª—è Duration Predictor.

    –ü—Ä–æ—Å—Ç—ñ—à–∞ –≤–µ—Ä—Å—ñ—è –Ω—ñ–∂ TextToLatent encoder:
    - ConvNeXt blocks
    - Self-attention –¥–ª—è global context
    - Mean pooling ‚Üí utterance embedding

    Args:
        vocab_size: Character vocabulary size
        embed_dim: Character embedding dimension
        hidden_dim: Hidden dimension
        num_convnext_blocks: Number of ConvNeXt blocks
        num_heads: Number of attention heads
    """

    def __init__(
        self,
        vocab_size: int = 512,
        embed_dim: int = 128,
        hidden_dim: int = 256,
        num_convnext_blocks: int = 4,
        kernel_size: int = 7,
        num_heads: int = 4
    ):
        super().__init__()

        self.hidden_dim = hidden_dim

        # Character embedding
        self.char_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # Projection
        self.input_proj = nn.Linear(embed_dim, hidden_dim)

        # ConvNeXt blocks
        self.convnext = ConvNeXtStack(
            dim=hidden_dim,
            num_blocks=num_convnext_blocks,
            intermediate_dim=hidden_dim * 4,
            kernel_size=kernel_size,
            causal=False
        )

        # Self-attention –¥–ª—è global context
        self.attention = MultiHeadAttention(
            dim=hidden_dim,
            num_heads=num_heads,
            use_rope=True
        )

        # Layer norm
        self.norm = nn.LayerNorm(hidden_dim)

    def forward(
        self,
        text: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Encode text to utterance embedding.

        Args:
            text: Character indices [B, L]
            text_mask: Optional mask [B, L]

        Returns:
            utterance_emb: [B, D]
        """
        # Embedding
        x = self.char_embed(text)  # [B, L, embed_dim]
        x = self.input_proj(x)  # [B, L, hidden_dim]

        # ConvNeXt
        x = x.transpose(1, 2)  # [B, D, L]
        x = self.convnext(x)
        x = x.transpose(1, 2)  # [B, L, D]

        # Self-attention
        x = x + self.attention(self.norm(x), mask=text_mask)

        # Mean pooling (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –º–∞—Å–∫–∏)
        if text_mask is not None:
            x = x * text_mask.unsqueeze(-1).float()
            utterance_emb = x.sum(dim=1) / text_mask.sum(dim=1, keepdim=True).float()
        else:
            utterance_emb = x.mean(dim=1)

        return utterance_emb


class DurationReferenceEncoder(nn.Module):
    """
    Reference Encoder –¥–ª—è Duration Predictor.

    –ö–æ–¥—É—î reference audio latent ‚Üí reference embedding.

    Args:
        input_dim: Latent dimension (144 compressed)
        hidden_dim: Hidden dimension
        num_convnext_blocks: Number of ConvNeXt blocks
        num_heads: Number of attention heads
    """

    def __init__(
        self,
        input_dim: int = 144,
        hidden_dim: int = 256,
        num_convnext_blocks: int = 4,
        kernel_size: int = 7,
        num_heads: int = 4
    ):
        super().__init__()

        self.hidden_dim = hidden_dim

        # Input projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # ConvNeXt blocks
        self.convnext = ConvNeXtStack(
            dim=hidden_dim,
            num_blocks=num_convnext_blocks,
            intermediate_dim=hidden_dim * 4,
            kernel_size=kernel_size,
            causal=False
        )

        # Self-attention
        self.attention = MultiHeadAttention(
            dim=hidden_dim,
            num_heads=num_heads,
            use_rope=True
        )

        # Layer norm
        self.norm = nn.LayerNorm(hidden_dim)

    def forward(
        self,
        ref_latent: torch.Tensor,
        ref_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Encode reference latent to embedding.

        Args:
            ref_latent: Reference latent [B, C, T]
            ref_mask: Optional mask [B, T]

        Returns:
            ref_emb: [B, D]
        """
        # Input projection
        x = ref_latent.transpose(1, 2)  # [B, T, C]
        x = self.input_proj(x)  # [B, T, D]

        # ConvNeXt
        x = x.transpose(1, 2)  # [B, D, T]
        x = self.convnext(x)
        x = x.transpose(1, 2)  # [B, T, D]

        # Self-attention
        x = x + self.attention(self.norm(x), mask=ref_mask)

        # Mean pooling
        if ref_mask is not None:
            x = x * ref_mask.unsqueeze(-1).float()
            ref_emb = x.sum(dim=1) / ref_mask.sum(dim=1, keepdim=True).float()
        else:
            ref_emb = x.mean(dim=1)

        return ref_emb


class DurationPredictor(nn.Module):
    """
    Duration Predictor - –ø–µ—Ä–µ–¥–±–∞—á–∞—î utterance-level duration.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    1. Text Encoder ‚Üí utterance embedding
    2. Reference Encoder ‚Üí reference embedding
    3. Concatenate ‚Üí MLP ‚Üí scalar duration

    –ö–ª—é—á–æ–≤–∞ –æ—Å–æ–±–ª–∏–≤—ñ—Å—Ç—å: –ø–µ—Ä–µ–¥–±–∞—á–∞—î –ó–ê–ì–ê–õ–¨–ù–£ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å,
    –Ω–µ per-phoneme durations. –¶–µ —Å–ø—Ä–æ—â—É—î –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É —Ç–∞
    –ø–æ–∫—Ä–∞—â—É—î prosody transfer –≤—ñ–¥ reference.

    Args:
        vocab_size: Character vocabulary size
        latent_dim: Reference latent dimension (144)
        hidden_dim: Hidden dimension for encoders
        output_unit: "frames" –∞–±–æ "seconds"
        hop_length: Hop length –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —Ñ—Ä–µ–π–º–∏‚Üî—Å–µ–∫—É–Ω–¥–∏
        sample_rate: Sample rate
        temporal_compression: Kc factor
    """

    def __init__(
        self,
        vocab_size: int = 512,
        latent_dim: int = 144,
        hidden_dim: int = 256,
        num_convnext_blocks: int = 4,
        kernel_size: int = 7,
        num_heads: int = 4,
        output_unit: str = "frames",  # "frames" –∞–±–æ "seconds"
        hop_length: int = 512,
        sample_rate: int = 44100,
        temporal_compression: int = 6
    ):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.output_unit = output_unit
        self.hop_length = hop_length
        self.sample_rate = sample_rate
        self.temporal_compression = temporal_compression

        # Frame duration in seconds
        self.frame_duration = hop_length / sample_rate * temporal_compression

        # Text encoder
        self.text_encoder = DurationTextEncoder(
            vocab_size=vocab_size,
            hidden_dim=hidden_dim,
            num_convnext_blocks=num_convnext_blocks,
            kernel_size=kernel_size,
            num_heads=num_heads
        )

        # Reference encoder
        self.reference_encoder = DurationReferenceEncoder(
            input_dim=latent_dim,
            hidden_dim=hidden_dim,
            num_convnext_blocks=num_convnext_blocks,
            kernel_size=kernel_size,
            num_heads=num_heads
        )

        # Duration MLP
        # Input: text_emb (D) + ref_emb (D) = 2D
        self.duration_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1),
            nn.Softplus()  # Ensure positive duration
        )

        # Optional: speech rate predictor (relative to reference)
        self.rate_predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # Output in [0, 1], scaled to [0.5, 2.0]
        )

    def forward(
        self,
        text: torch.Tensor,
        ref_latent: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None,
        ref_mask: Optional[torch.Tensor] = None,
        target_duration: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass.

        Args:
            text: Character indices [B, L]
            ref_latent: Reference latent [B, C, T]
            text_mask: Optional text mask [B, L]
            ref_mask: Optional reference mask [B, T]
            target_duration: Ground-truth duration [B] (for training)

        Returns:
            Dict with 'duration', 'rate', and optionally 'loss'
        """
        # Encode text and reference
        text_emb = self.text_encoder(text, text_mask)  # [B, D]
        ref_emb = self.reference_encoder(ref_latent, ref_mask)  # [B, D]

        # Concatenate
        combined = torch.cat([text_emb, ref_emb], dim=-1)  # [B, 2D]

        # Predict duration
        duration = self.duration_mlp(combined).squeeze(-1)  # [B]

        # Predict speech rate (optional)
        rate_raw = self.rate_predictor(combined).squeeze(-1)  # [B] in [0, 1]
        rate = 0.5 + rate_raw * 1.5  # Scale to [0.5, 2.0]

        output = {
            'duration': duration,
            'rate': rate,
            'text_embedding': text_emb,
            'reference_embedding': ref_emb
        }

        # Compute loss if target provided
        if target_duration is not None:
            loss = F.l1_loss(duration, target_duration)
            output['loss'] = loss

        return output

    def predict(
        self,
        text: torch.Tensor,
        ref_latent: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None,
        ref_mask: Optional[torch.Tensor] = None,
        rate_scale: float = 1.0
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Predict duration (inference mode).

        Args:
            text: Character indices [B, L]
            ref_latent: Reference latent [B, C, T]
            text_mask: Optional text mask
            ref_mask: Optional reference mask
            rate_scale: Manual rate adjustment (0.5-2.0)

        Returns:
            duration_frames: Predicted duration in frames [B]
            duration_seconds: Predicted duration in seconds [B]
        """
        output = self.forward(text, ref_latent, text_mask, ref_mask)

        # Apply rate scale
        duration = output['duration'] * rate_scale

        if self.output_unit == "frames":
            duration_frames = duration
            duration_seconds = duration * self.frame_duration
        else:
            duration_seconds = duration
            duration_frames = duration / self.frame_duration

        return duration_frames.long(), duration_seconds

    def frames_to_seconds(self, frames: torch.Tensor) -> torch.Tensor:
        """Convert frames to seconds."""
        return frames.float() * self.frame_duration

    def seconds_to_frames(self, seconds: torch.Tensor) -> torch.Tensor:
        """Convert seconds to frames."""
        return (seconds / self.frame_duration).long()


class DurationPredictorLoss(nn.Module):
    """
    Loss function –¥–ª—è Duration Predictor.

    Combines:
    - L1 loss –Ω–∞ duration
    - Optional: rate consistency loss

    Args:
        duration_weight: Weight for duration loss
        rate_weight: Weight for rate consistency loss
    """

    def __init__(
        self,
        duration_weight: float = 1.0,
        rate_weight: float = 0.1
    ):
        super().__init__()

        self.duration_weight = duration_weight
        self.rate_weight = rate_weight

    def forward(
        self,
        pred_duration: torch.Tensor,
        target_duration: torch.Tensor,
        pred_rate: Optional[torch.Tensor] = None,
        target_rate: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute loss.

        Args:
            pred_duration: Predicted duration [B]
            target_duration: Ground-truth duration [B]
            pred_rate: Predicted rate [B] (optional)
            target_rate: Target rate [B] (optional)

        Returns:
            Dict with 'loss', 'duration_loss', 'rate_loss'
        """
        # Duration loss (L1)
        duration_loss = F.l1_loss(pred_duration, target_duration)

        total_loss = self.duration_weight * duration_loss

        output = {
            'duration_loss': duration_loss,
        }

        # Rate loss (optional)
        if pred_rate is not None and target_rate is not None:
            rate_loss = F.l1_loss(pred_rate, target_rate)
            total_loss = total_loss + self.rate_weight * rate_loss
            output['rate_loss'] = rate_loss

        output['loss'] = total_loss
        return output


# ============================================================================
# Unit tests
# ============================================================================

def _test_duration_text_encoder():
    """–¢–µ—Å—Ç DurationTextEncoder."""
    print("Testing DurationTextEncoder...")

    encoder = DurationTextEncoder(
        vocab_size=512,
        hidden_dim=256,
        num_convnext_blocks=4
    )

    batch_size = 2
    text_len = 50
    text = torch.randint(0, 512, (batch_size, text_len))

    utterance_emb = encoder(text)
    assert utterance_emb.shape == (batch_size, 256)
    print(f"  Text: {text.shape} -> Utterance emb: {utterance_emb.shape} ‚úì")

    # With mask
    mask = torch.ones(batch_size, text_len, dtype=torch.bool)
    mask[0, 40:] = False
    utterance_emb_masked = encoder(text, mask)
    assert utterance_emb_masked.shape == (batch_size, 256)
    print(f"  With mask: {utterance_emb_masked.shape} ‚úì")

    num_params = sum(p.numel() for p in encoder.parameters())
    print(f"  Parameters: {num_params:,}")

    print("DurationTextEncoder tests passed! ‚úì\n")


def _test_duration_reference_encoder():
    """–¢–µ—Å—Ç DurationReferenceEncoder."""
    print("Testing DurationReferenceEncoder...")

    encoder = DurationReferenceEncoder(
        input_dim=144,
        hidden_dim=256,
        num_convnext_blocks=4
    )

    batch_size = 2
    t_ref = 100
    ref_latent = torch.randn(batch_size, 144, t_ref)

    ref_emb = encoder(ref_latent)
    assert ref_emb.shape == (batch_size, 256)
    print(f"  Ref latent: {ref_latent.shape} -> Ref emb: {ref_emb.shape} ‚úì")

    num_params = sum(p.numel() for p in encoder.parameters())
    print(f"  Parameters: {num_params:,}")

    print("DurationReferenceEncoder tests passed! ‚úì\n")


def _test_duration_predictor():
    """–¢–µ—Å—Ç –ø–æ–≤–Ω–æ–≥–æ DurationPredictor."""
    print("Testing DurationPredictor...")

    predictor = DurationPredictor(
        vocab_size=512,
        latent_dim=144,
        hidden_dim=256,
        num_convnext_blocks=4,
        output_unit="frames",
        hop_length=512,
        sample_rate=44100,
        temporal_compression=6
    )

    batch_size = 2
    text_len = 50
    t_ref = 100

    text = torch.randint(0, 512, (batch_size, text_len))
    ref_latent = torch.randn(batch_size, 144, t_ref)
    target_duration = torch.tensor([150.0, 200.0])  # frames

    # Training forward
    output = predictor(text, ref_latent, target_duration=target_duration)
    print(f"  Predicted duration: {output['duration']}")
    print(f"  Predicted rate: {output['rate']}")
    print(f"  Loss: {output['loss'].item():.6f}")

    # Inference
    duration_frames, duration_seconds = predictor.predict(text, ref_latent)
    print(f"  Inference - frames: {duration_frames}, seconds: {duration_seconds}")

    # Parameter count
    total_params = sum(p.numel() for p in predictor.parameters())
    text_params = sum(p.numel() for p in predictor.text_encoder.parameters())
    ref_params = sum(p.numel() for p in predictor.reference_encoder.parameters())

    print(f"\n  Total parameters: {total_params:,}")
    print(f"  Text encoder: {text_params:,}")
    print(f"  Reference encoder: {ref_params:,}")

    # Check it's around 0.5M as expected
    assert total_params < 1_000_000, f"Too many params: {total_params}"
    print(f"  ‚úì Parameter count is reasonable (~0.5M)")

    print("\nDurationPredictor tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_duration_text_encoder()
    _test_duration_reference_encoder()
    _test_duration_predictor()
    print("All Duration Predictor tests passed! ‚úì")



==================================================
–§–ê–ô–õ: supertonic/models/larope.py
–†–û–ó–ú–Ü–†: 14.93 KB
==================================================

"""
LARoPE: Length-Aware Rotary Position Embedding

–ö–ª—é—á–æ–≤–∞ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è Supertonic v2 –¥–ª—è text-speech alignment —É cross-attention.

–ü—Ä–æ–±–ª–µ–º–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RoPE:
- RoPE –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∞–±—Å–æ–ª—é—Ç–Ω—ñ –ø–æ–∑–∏—Ü—ñ–π–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
- –£ cross-attention text (–∫–æ—Ä–æ—Ç–∫–∏–π) —Ç–∞ speech (–¥–æ–≤–≥–∏–π) –º–∞—é—Ç—å —Ä—ñ–∑–Ω—É –¥–æ–≤–∂–∏–Ω—É
- –¶–µ –ø–æ—Ä—É—à—É—î relative position property

LARoPE —Ä—ñ—à–µ–Ω–Ω—è:
- –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –ø–æ–∑–∏—Ü—ñ—ó –∑–∞ –¥–æ–≤–∂–∏–Ω–æ—é –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
- normalized_pos = Œ≥ √ó (position / seq_length)
- Œ≥ = 10 —ñ–Ω–¥—É–∫—É—î diagonal bias –≤ attention maps
- –¶–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–º—É text-speech alignment

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper (2509.11084)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple
from einops import rearrange


class LARoPE(nn.Module):
    """
    Length-Aware Rotary Position Embedding.

    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –ø–æ–∑–∏—Ü—ñ–π–Ω—ñ embeddings –∑–∞ –¥–æ–≤–∂–∏–Ω–æ—é –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ,
    —â–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è cross-attention –º—ñ–∂ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ –∞—É–¥—ñ–æ —Ä—ñ–∑–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏.

    Args:
        dim: –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å embedding (–º–∞—î –±—É—Ç–∏ –ø–∞—Ä–Ω–∏–º)
        gamma: Scaling hyperparameter (default 10 ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è TTS)
        base: –ë–∞–∑–∞ –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–∏—Ö —Å–º—É–≥ (default 10000)
        max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è (optional)
    """

    def __init__(
        self,
        dim: int,
        gamma: float = 10.0,
        base: float = 10000.0,
        max_seq_len: int = 8192
    ):
        super().__init__()

        assert dim % 2 == 0, f"Dimension must be even, got {dim}"

        self.dim = dim
        self.gamma = gamma
        self.base = base
        self.max_seq_len = max_seq_len

        # Precompute inverse frequencies
        # Œ∏_i = base^(-2i/d) for i = 0, 1, ..., d/2-1
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def _compute_rope_embeddings(
        self,
        positions: torch.Tensor,  # [T] or [B, T]
        seq_length: torch.Tensor,  # scalar or [B]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –û–±—á–∏—Å–ª—é—î sin/cos embeddings –¥–ª—è LARoPE.

        Args:
            positions: –ü–æ–∑–∏—Ü—ñ–π–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
            seq_length: –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó

        Returns:
            cos, sin tensors –¥–ª—è rotation
        """
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–æ–∑–∏—Ü—ñ—ó: Œ≥ √ó (pos / L)
        # –¶–µ –∫–ª—é—á–æ–≤–∞ –≤—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ RoPE
        if positions.dim() == 1:
            normalized_pos = self.gamma * (positions.float() / seq_length)  # [T]
        else:
            # [B, T] / [B, 1] -> [B, T]
            normalized_pos = self.gamma * (positions.float() / seq_length.unsqueeze(-1))

        # –û–±—á–∏—Å–ª—é—î–º–æ –∫—É—Ç–∏: normalized_pos √ó Œ∏
        # [T] √ó [D/2] -> [T, D/2] –∞–±–æ [B, T] √ó [D/2] -> [B, T, D/2]
        if normalized_pos.dim() == 1:
            angles = normalized_pos.unsqueeze(-1) * self.inv_freq.unsqueeze(0)  # [T, D/2]
        else:
            angles = normalized_pos.unsqueeze(-1) * self.inv_freq  # [B, T, D/2]

        # cos —Ç–∞ sin –¥–ª—è rotation
        cos = torch.cos(angles)
        sin = torch.sin(angles)

        return cos, sin

    def forward(
        self,
        x: torch.Tensor,
        seq_length: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        –ó–∞—Å—Ç–æ—Å–æ–≤—É—î LARoPE –¥–æ input tensor.

        Args:
            x: Input tensor [B, T, D] –∞–±–æ [B, H, T, D]
            seq_length: –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ (—è–∫—â–æ None ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î T)

        Returns:
            Rotated tensor —Ç—ñ—î—ó –∂ —Ñ–æ—Ä–º–∏
        """
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏
        if x.dim() == 3:
            batch_size, seq_len, dim = x.shape
            x = x.unsqueeze(1)  # [B, 1, T, D]
            squeeze_output = True
        else:
            batch_size, num_heads, seq_len, dim = x.shape
            squeeze_output = False

        assert dim == self.dim, f"Dimension mismatch: {dim} vs {self.dim}"

        # –Ø–∫—â–æ seq_length –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–∫—Ç—É–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É
        if seq_length is None:
            seq_length = torch.tensor(seq_len, device=x.device, dtype=torch.float)

        # –ü–æ–∑–∏—Ü—ñ—ó: 0, 1, 2, ..., T-1
        positions = torch.arange(seq_len, device=x.device)

        # –û–±—á–∏—Å–ª—é—î–º–æ rotation embeddings
        cos, sin = self._compute_rope_embeddings(positions, seq_length)
        cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, T, D/2]
        sin = sin.unsqueeze(0).unsqueeze(0)  # [1, 1, T, D/2]

        # –†–æ–∑–¥—ñ–ª—è—î–º–æ –Ω–∞ –ø–∞—Ä–Ω—ñ/–Ω–µ–ø–∞—Ä–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
        x_even = x[..., 0::2]  # [B, H, T, D/2]
        x_odd = x[..., 1::2]   # [B, H, T, D/2]

        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ rotation
        # x_rotated = x √ó cos + rotate(x) √ó sin
        # rotate([x_even, x_odd]) = [-x_odd, x_even]
        x_rotated_even = x_even * cos - x_odd * sin
        x_rotated_odd = x_even * sin + x_odd * cos

        # –ó–±–∏—Ä–∞—î–º–æ –Ω–∞–∑–∞–¥
        x_rotated = torch.stack([x_rotated_even, x_rotated_odd], dim=-1)
        x_rotated = x_rotated.flatten(-2)  # [B, H, T, D]

        if squeeze_output:
            x_rotated = x_rotated.squeeze(1)  # [B, T, D]

        return x_rotated


def apply_larope(
    q: torch.Tensor,
    k: torch.Tensor,
    q_seq_len: torch.Tensor,
    k_seq_len: torch.Tensor,
    gamma: float = 10.0,
    base: float = 10000.0
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    –§—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è LARoPE –≤ cross-attention.

    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î length-normalized rotary embeddings –¥–æ query —Ç–∞ key,
    –∫–æ–∂–µ–Ω –∑ –≤–ª–∞—Å–Ω–æ—é –¥–æ–≤–∂–∏–Ω–æ—é –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó.

    Args:
        q: Query tensor [B, H, Lq, D]
        k: Key tensor [B, H, Lk, D]
        q_seq_len: Query sequence length (–¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó)
        k_seq_len: Key sequence length (–¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó)
        gamma: LARoPE scaling factor (default 10)
        base: RoPE base frequency

    Returns:
        Rotated (q, k) tensors

    –ü—Ä–∏–∫–ª–∞–¥:
        >>> q = torch.randn(2, 4, 100, 64)  # Speech features
        >>> k = torch.randn(2, 4, 20, 64)   # Text embeddings
        >>> q_rot, k_rot = apply_larope(q, k, 100, 20, gamma=10)
    """
    batch_size, num_heads, lq, dim = q.shape
    _, _, lk, _ = k.shape

    assert dim % 2 == 0, f"Dimension must be even, got {dim}"

    # Inverse frequencies
    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=q.device).float() / dim))

    # === Query rotation ===
    q_positions = torch.arange(lq, device=q.device)
    q_normalized = gamma * (q_positions.float() / float(q_seq_len))
    q_angles = q_normalized.unsqueeze(-1) * inv_freq  # [Lq, D/2]
    q_cos = torch.cos(q_angles).unsqueeze(0).unsqueeze(0)  # [1, 1, Lq, D/2]
    q_sin = torch.sin(q_angles).unsqueeze(0).unsqueeze(0)

    q_even, q_odd = q[..., 0::2], q[..., 1::2]
    q_rotated_even = q_even * q_cos - q_odd * q_sin
    q_rotated_odd = q_even * q_sin + q_odd * q_cos
    q_rotated = torch.stack([q_rotated_even, q_rotated_odd], dim=-1).flatten(-2)

    # === Key rotation ===
    k_positions = torch.arange(lk, device=k.device)
    k_normalized = gamma * (k_positions.float() / float(k_seq_len))
    k_angles = k_normalized.unsqueeze(-1) * inv_freq  # [Lk, D/2]
    k_cos = torch.cos(k_angles).unsqueeze(0).unsqueeze(0)  # [1, 1, Lk, D/2]
    k_sin = torch.sin(k_angles).unsqueeze(0).unsqueeze(0)

    k_even, k_odd = k[..., 0::2], k[..., 1::2]
    k_rotated_even = k_even * k_cos - k_odd * k_sin
    k_rotated_odd = k_even * k_sin + k_odd * k_cos
    k_rotated = torch.stack([k_rotated_even, k_rotated_odd], dim=-1).flatten(-2)

    return q_rotated, k_rotated


class LARoPECrossAttention(nn.Module):
    """
    Cross-Attention –º–æ–¥—É–ª—å –∑ LARoPE –¥–ª—è text-speech alignment.

    –ö—Ä–∏—Ç–∏—á–Ω–∞ –æ—Å–æ–±–ª–∏–≤—ñ—Å—Ç—å: query (speech) —Ç–∞ key (text) –º–∞—é—Ç—å —Ä—ñ–∑–Ω—É
    –¥–æ–≤–∂–∏–Ω—É, —Ç–æ–º—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ length-normalized positions.

    Args:
        dim: Model dimension
        num_heads: Number of attention heads
        head_dim: Dimension per head (default: dim // num_heads)
        gamma: LARoPE gamma (default: 10)
        dropout: Attention dropout
    """

    def __init__(
        self,
        dim: int,
        num_heads: int = 4,
        head_dim: Optional[int] = None,
        gamma: float = 10.0,
        dropout: float = 0.0,
        bias: bool = True
    ):
        super().__init__()

        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = head_dim or dim // num_heads
        self.gamma = gamma
        self.scale = self.head_dim ** -0.5

        inner_dim = self.num_heads * self.head_dim

        # Query projection (–¥–ª—è speech features)
        self.to_q = nn.Linear(dim, inner_dim, bias=bias)

        # Key, Value projections (–¥–ª—è text embeddings)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=bias)

        # Output projection
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )

        self.attn_dropout = nn.Dropout(dropout)

    def forward(
        self,
        x: torch.Tensor,           # Speech features [B, Lq, D]
        context: torch.Tensor,     # Text embeddings [B, Lk, D]
        x_mask: Optional[torch.Tensor] = None,
        context_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass –∑ LARoPE.

        Args:
            x: Query (speech features) [B, Lq, D]
            context: Key/Value (text embeddings) [B, Lk, D]
            x_mask: Optional mask for queries
            context_mask: Optional mask for keys/values

        Returns:
            Output tensor [B, Lq, D]
        """
        batch_size, lq, _ = x.shape
        _, lk, _ = context.shape

        # Project to Q, K, V
        q = self.to_q(x)
        kv = self.to_kv(context)
        k, v = kv.chunk(2, dim=-1)

        # Reshape to [B, H, L, D_head]
        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)
        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads)
        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads)

        # Apply LARoPE ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è alignment!
        q, k = apply_larope(q, k, lq, lk, gamma=self.gamma)

        # Scaled dot-product attention
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, Lq, Lk]

        # Apply mask if provided
        if context_mask is not None:
            # context_mask: [B, Lk] -> [B, 1, 1, Lk]
            mask = context_mask.unsqueeze(1).unsqueeze(2)
            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))

        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)

        # Weighted sum
        out = torch.matmul(attn_weights, v)  # [B, H, Lq, D_head]

        # Reshape back
        out = rearrange(out, 'b h l d -> b l (h d)')

        return self.to_out(out)


# ============================================================================
# Unit tests
# ============================================================================

def _test_larope():
    """–¢–µ—Å—Ç LARoPE –º–æ–¥—É–ª—è."""
    print("Testing LARoPE...")

    batch_size = 2
    seq_len = 100
    dim = 64

    larope = LARoPE(dim=dim, gamma=10.0)

    # Test basic forward
    x = torch.randn(batch_size, seq_len, dim)
    y = larope(x)
    assert y.shape == x.shape, f"Shape mismatch: {y.shape}"
    print(f"  Basic LARoPE: {x.shape} -> {y.shape} ‚úì")

    # Test with explicit seq_length
    y2 = larope(x, seq_length=torch.tensor(seq_len))
    assert y2.shape == x.shape
    print(f"  With explicit seq_length: {x.shape} -> {y2.shape} ‚úì")

    # Test 4D input (multi-head)
    x_4d = torch.randn(batch_size, 4, seq_len, dim)  # [B, H, T, D]
    y_4d = larope(x_4d)
    assert y_4d.shape == x_4d.shape
    print(f"  Multi-head LARoPE: {x_4d.shape} -> {y_4d.shape} ‚úì")

    print("LARoPE tests passed! ‚úì\n")


def _test_apply_larope():
    """–¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ LARoPE –¥–ª—è cross-attention."""
    print("Testing apply_larope...")

    batch_size = 2
    num_heads = 4
    dim_per_head = 64

    # Speech features (–¥–æ–≤—à–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å)
    lq = 100
    q = torch.randn(batch_size, num_heads, lq, dim_per_head)

    # Text embeddings (–∫–æ—Ä–æ—Ç—à–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å)
    lk = 20
    k = torch.randn(batch_size, num_heads, lk, dim_per_head)

    # Apply LARoPE –∑ —Ä—ñ–∑–Ω–∏–º–∏ –¥–æ–≤–∂–∏–Ω–∞–º–∏
    q_rot, k_rot = apply_larope(q, k, lq, lk, gamma=10.0)

    assert q_rot.shape == q.shape, f"Query shape mismatch"
    assert k_rot.shape == k.shape, f"Key shape mismatch"
    print(f"  Q: {q.shape} -> {q_rot.shape} ‚úì")
    print(f"  K: {k.shape} -> {k_rot.shape} ‚úì")

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —â–æ attention scores –º–∞—é—Ç—å –æ—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
    attn_scores = torch.matmul(q_rot, k_rot.transpose(-2, -1))
    attn_weights = F.softmax(attn_scores, dim=-1)

    # LARoPE –∑ Œ≥=10 –º–∞—î –¥–∞–≤–∞—Ç–∏ diagonal bias
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —â–æ attention concentration —î —Ä–æ–∑—É–º–Ω–æ—é
    avg_entropy = -(attn_weights * attn_weights.log().clamp(min=-100)).sum(-1).mean()
    print(f"  Average attention entropy: {avg_entropy:.3f}")

    print("apply_larope tests passed! ‚úì\n")


def _test_larope_cross_attention():
    """–¢–µ—Å—Ç LARoPE cross-attention –º–æ–¥—É–ª—è."""
    print("Testing LARoPECrossAttention...")

    batch_size = 2
    dim = 256
    num_heads = 4

    # Speech features
    lq = 100
    x = torch.randn(batch_size, lq, dim)

    # Text embeddings
    lk = 20
    context = torch.randn(batch_size, lk, dim)

    # Create module
    cross_attn = LARoPECrossAttention(
        dim=dim,
        num_heads=num_heads,
        gamma=10.0,
        dropout=0.0
    )

    # Forward pass
    out = cross_attn(x, context)
    assert out.shape == x.shape, f"Output shape mismatch: {out.shape}"
    print(f"  Input: {x.shape}, Context: {context.shape} -> Output: {out.shape} ‚úì")

    # Test with mask
    context_mask = torch.ones(batch_size, lk, dtype=torch.bool)
    context_mask[0, 15:] = False  # Mask last 5 tokens for first sample

    out_masked = cross_attn(x, context, context_mask=context_mask)
    assert out_masked.shape == x.shape
    print(f"  With mask: {out_masked.shape} ‚úì")

    # Parameter count
    num_params = sum(p.numel() for p in cross_attn.parameters())
    print(f"  Parameters: {num_params:,}")

    print("LARoPECrossAttention tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_larope()
    _test_apply_larope()
    _test_larope_cross_attention()
    print("All LARoPE tests passed! ‚úì")



==================================================
–§–ê–ô–õ: supertonic/models/speech_autoencoder.py
–†–û–ó–ú–Ü–†: 40.04 KB
==================================================

"""
Speech Autoencoder - Vocos-based –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è Supertonic v2

–ö–æ–¥—É—î –∞—É–¥—ñ–æ –≤ –Ω–∏–∑—å–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–∏–π –ª–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä —Ç–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É—é—î 44.1kHz waveform.

–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (~47M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤):
- Latent Encoder: mel(228) ‚Üí Conv+BN(512) ‚Üí 10 ConvNeXt ‚Üí Linear+LN(24)
- Latent Decoder: latent(24) ‚Üí Conv+BN(512) ‚Üí 10 dilated ConvNeXt ‚Üí iSTFT
- Discriminators: MPD (periods [2,3,5,7,11]) + MRD (FFT [512,1024,2048])

Latent Space:
- 24-dimensional continuous space (–Ω–µ VQ-VAE!)
- Temporal compression Kc=6 (6 —Ñ—Ä–µ–π–º—ñ–≤ ‚Üí 1 –≤–µ–∫—Ç–æ—Ä 144-dim)

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Vocos paper + Supertonic v2 paper
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, List, Tuple, Dict
from einops import rearrange

from supertonic.models.convnext import ConvNeXtBlock, ConvNeXtStack, LayerNorm1d


class MelSpectrogram(nn.Module):
    """
    Mel Spectrogram extractor –¥–ª—è Supertonic v2.

    –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:
    - sample_rate: 44100
    - n_fft: 2048
    - hop_length: 512
    - n_mels: 228

    Frame timing:
    - FFT window: 2048/44100 = 46.43ms
    - Hop: 512/44100 = 11.61ms
    """

    def __init__(
        self,
        sample_rate: int = 44100,
        n_fft: int = 2048,
        hop_length: int = 512,
        n_mels: int = 228,
        f_min: float = 0.0,
        f_max: Optional[float] = None,
        center: bool = True,
        power: float = 1.0,  # 1 = magnitude, 2 = power
        normalized: bool = False,
        norm: str = "slaney",
        mel_scale: str = "slaney"
    ):
        super().__init__()

        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        self.f_min = f_min
        self.f_max = f_max or sample_rate / 2
        self.center = center
        self.power = power
        self.normalized = normalized

        # –°—Ç–≤–æ—Ä—é—î–º–æ mel filterbank
        mel_fb = self._create_mel_filterbank(
            n_fft=n_fft,
            n_mels=n_mels,
            sample_rate=sample_rate,
            f_min=f_min,
            f_max=self.f_max,
            norm=norm
        )
        self.register_buffer("mel_fb", mel_fb)

        # Hann window
        window = torch.hann_window(n_fft)
        self.register_buffer("window", window)

    def _create_mel_filterbank(
        self,
        n_fft: int,
        n_mels: int,
        sample_rate: int,
        f_min: float,
        f_max: float,
        norm: str
    ) -> torch.Tensor:
        """–°—Ç–≤–æ—Ä—é—î mel filterbank matrix."""
        # Mel scale conversion
        def hz_to_mel(f):
            return 2595 * math.log10(1 + f / 700)

        def mel_to_hz(m):
            return 700 * (10 ** (m / 2595) - 1)

        # FFT bins
        n_freqs = n_fft // 2 + 1
        fft_freqs = torch.linspace(0, sample_rate / 2, n_freqs)

        # Mel points
        mel_min = hz_to_mel(f_min)
        mel_max = hz_to_mel(f_max)
        mel_points = torch.linspace(mel_min, mel_max, n_mels + 2)
        hz_points = torch.tensor([mel_to_hz(m) for m in mel_points])

        # Create filterbank
        mel_fb = torch.zeros(n_mels, n_freqs)

        for i in range(n_mels):
            left = hz_points[i]
            center = hz_points[i + 1]
            right = hz_points[i + 2]

            # Rising edge
            rising = (fft_freqs - left) / (center - left)
            # Falling edge
            falling = (right - fft_freqs) / (right - center)

            mel_fb[i] = torch.maximum(
                torch.zeros_like(fft_freqs),
                torch.minimum(rising, falling)
            )

        # Slaney normalization
        if norm == "slaney":
            enorm = 2.0 / (hz_points[2:n_mels + 2] - hz_points[:n_mels])
            mel_fb *= enorm.unsqueeze(1)

        return mel_fb

    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        """
        –û–±—á–∏—Å–ª—é—î mel spectrogram.

        Args:
            audio: [B, T] –∞–±–æ [B, 1, T]

        Returns:
            mel: [B, n_mels, T_frames]
        """
        if audio.dim() == 3:
            audio = audio.squeeze(1)

        # STFT
        if self.center:
            pad_amount = self.n_fft // 2
            audio = F.pad(audio, (pad_amount, pad_amount), mode='reflect')

        # Manual STFT –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
        # Unfold audio into frames
        frames = audio.unfold(1, self.n_fft, self.hop_length)  # [B, T_frames, n_fft]

        # Apply window
        frames = frames * self.window

        # FFT
        spec = torch.fft.rfft(frames, dim=-1)  # [B, T_frames, n_fft//2+1]
        spec = spec.transpose(1, 2)  # [B, n_fft//2+1, T_frames]

        # Magnitude
        if self.power == 1:
            spec_mag = spec.abs()
        else:
            spec_mag = spec.abs().pow(self.power)

        # Apply mel filterbank
        mel = torch.matmul(self.mel_fb, spec_mag)  # [B, n_mels, T_frames]

        # Log mel
        mel = torch.log(torch.clamp(mel, min=1e-5))

        return mel


class LatentEncoder(nn.Module):
    """
    Latent Encoder - –∫–æ–¥—É—î mel spectrogram –≤ 24-dim –ª–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    1. Conv1d(228 ‚Üí 512) + BatchNorm
    2. 10 ConvNeXt blocks (intermediate: 2048, kernel: 7)
    3. Linear(512 ‚Üí 24) + LayerNorm

    Output: 24-dimensional latent vectors
    """

    def __init__(
        self,
        input_dim: int = 228,
        hidden_dim: int = 512,
        output_dim: int = 24,
        num_blocks: int = 10,
        kernel_size: int = 7,
        intermediate_mult: int = 4,
        gradient_checkpointing: bool = False
    ):
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        # Initial projection: mel ‚Üí hidden
        self.input_conv = nn.Conv1d(input_dim, hidden_dim, kernel_size=1)
        self.input_norm = nn.BatchNorm1d(hidden_dim)

        # ConvNeXt stack
        self.convnext = ConvNeXtStack(
            dim=hidden_dim,
            num_blocks=num_blocks,
            intermediate_dim=hidden_dim * intermediate_mult,
            kernel_size=kernel_size,
            dilations=None,  # –í—Å—ñ dilation = 1
            causal=False,
            gradient_checkpointing=gradient_checkpointing
        )

        # Output projection: hidden ‚Üí latent
        self.output_linear = nn.Linear(hidden_dim, output_dim)
        self.output_norm = nn.LayerNorm(output_dim)

    def forward(self, mel: torch.Tensor) -> torch.Tensor:
        """
        Encode mel spectrogram to latent vectors.

        Args:
            mel: Mel spectrogram [B, n_mels, T]

        Returns:
            latent: Latent vectors [B, latent_dim, T]
        """
        # Initial projection
        x = self.input_conv(mel)  # [B, hidden, T]
        x = self.input_norm(x)

        # ConvNeXt blocks
        x = self.convnext(x)  # [B, hidden, T]

        # Output projection
        x = x.transpose(1, 2)  # [B, T, hidden]
        x = self.output_linear(x)  # [B, T, latent]
        x = self.output_norm(x)
        x = x.transpose(1, 2)  # [B, latent, T]

        return x


class WaveNeXtHead(nn.Module):
    """
    [DEPRECATED] WaveNeXt-style waveform head.
    
    WARNING: This approach causes metallic sound artifacts due to lack of 
    overlap-add between frames. Each frame is generated independently,
    causing phase discontinuities at frame boundaries.
    
    Use HiFiGANGenerator instead for clean audio!
    """
    
    def __init__(
        self,
        input_dim: int = 512,
        head_dim: int = 2048,
        hop_length: int = 256
    ):
        super().__init__()
        self.hop_length = hop_length
        self.norm = nn.BatchNorm1d(input_dim)
        self.conv = nn.Conv1d(input_dim, head_dim, kernel_size=3, padding=1)
        self.fc = nn.Linear(head_dim, hop_length)
        self.act = nn.PReLU(num_parameters=head_dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.norm(x)
        x = self.conv(x)
        x = self.act(x)
        x = x.transpose(1, 2)
        x = self.fc(x)
        batch_size, num_frames, _ = x.shape
        audio = x.reshape(batch_size, num_frames * self.hop_length)
        return audio


# ============================================================================
# HiFi-GAN Generator - RECOMMENDED for clean audio without metallic artifacts
# ============================================================================

class ResBlock(nn.Module):
    """
    HiFi-GAN Residual Block with multi-receptive field fusion.
    
    Uses dilated convolutions to capture different temporal patterns.
    The key insight: overlap from convolutions naturally smooths frame boundaries.
    """
    
    def __init__(
        self,
        channels: int,
        kernel_size: int = 3,
        dilations: Tuple[int, ...] = (1, 3, 5)
    ):
        super().__init__()
        
        self.convs1 = nn.ModuleList()
        self.convs2 = nn.ModuleList()
        
        for dilation in dilations:
            padding = (kernel_size * dilation - dilation) // 2
            self.convs1.append(
                nn.Sequential(
                    nn.LeakyReLU(0.1),
                    nn.Conv1d(channels, channels, kernel_size,
                              dilation=dilation, padding=padding)
                )
            )
            self.convs2.append(
                nn.Sequential(
                    nn.LeakyReLU(0.1),
                    nn.Conv1d(channels, channels, kernel_size,
                              dilation=1, padding=(kernel_size - 1) // 2)
                )
            )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for conv1, conv2 in zip(self.convs1, self.convs2):
            xt = conv1(x)
            xt = conv2(xt)
            x = x + xt
        return x


class HiFiGANGenerator(nn.Module):
    """
    HiFi-GAN Generator - Upsamples latent features to waveform.
    
    This is the KEY component that eliminates metallic sound!
    
    Why it works:
    1. ConvTranspose1d with proper kernel/stride creates OVERLAPPING windows
    2. Overlap-add is implicit in the transposed convolution math
    3. No explicit phase prediction - learns smooth transitions naturally
    
    Architecture:
    - Input projection: hidden_dim ‚Üí initial_channel
    - Series of upsample blocks: ConvTranspose1d + ResBlocks
    - Output projection: Conv1d ‚Üí 1 channel waveform
    
    For 22kHz with hop_length=256:
    - upsample_rates: [8, 8, 2, 2] ‚Üí product = 256 = hop_length ‚úì
    - upsample_kernel_sizes: [16, 16, 4, 4] ‚Üí kernel >= 2*stride ‚úì
    
    Args:
        input_dim: Input feature dimension (512 from ConvNeXt)
        upsample_rates: Upsampling factors per stage
        upsample_kernel_sizes: Kernel sizes for transposed convs
        upsample_initial_channel: Channels after first projection
        resblock_kernel_sizes: Kernel sizes for residual blocks
        resblock_dilation_sizes: Dilation patterns for residual blocks
    """
    
    def __init__(
        self,
        input_dim: int = 512,
        upsample_rates: List[int] = [8, 8, 2, 2],
        upsample_kernel_sizes: List[int] = [16, 16, 4, 4],
        upsample_initial_channel: int = 512,
        resblock_kernel_sizes: List[int] = [3, 7, 11],
        resblock_dilation_sizes: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
    ):
        super().__init__()
        
        self.num_upsamples = len(upsample_rates)
        self.num_kernels = len(resblock_kernel_sizes)
        
        # Validate: product of upsample_rates should equal hop_length
        self.hop_length = 1
        for r in upsample_rates:
            self.hop_length *= r
        
        # Input projection
        self.conv_pre = nn.Conv1d(input_dim, upsample_initial_channel, 7, padding=3)
        
        # Upsampling layers
        self.ups = nn.ModuleList()
        self.resblocks = nn.ModuleList()
        
        ch = upsample_initial_channel
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            # Transposed conv for upsampling
            # Key: kernel_size >= 2 * stride to ensure overlap!
            self.ups.append(
                nn.ConvTranspose1d(
                    ch, ch // 2,
                    kernel_size=k,
                    stride=u,
                    padding=(k - u) // 2
                )
            )
            ch = ch // 2
            
            # Multi-receptive field fusion (MRF) block
            for j, (kr, dr) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):
                self.resblocks.append(ResBlock(ch, kr, tuple(dr)))
        
        # Output projection
        self.conv_post = nn.Conv1d(ch, 1, 7, padding=3)
        
        # Weight initialization (important for stability!)
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):
            nn.init.normal_(m.weight, 0.0, 0.01)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Upsample features to waveform.
        
        Args:
            x: Features [B, input_dim, T] from ConvNeXt decoder
        
        Returns:
            audio: Waveform [B, T * hop_length]
        """
        x = self.conv_pre(x)
        
        for i, up in enumerate(self.ups):
            x = F.leaky_relu(x, 0.1)
            x = up(x)
            
            # Apply all resblocks for this upsample level, then average
            xs = None
            for j in range(self.num_kernels):
                idx = i * self.num_kernels + j
                if xs is None:
                    xs = self.resblocks[idx](x)
                else:
                    xs = xs + self.resblocks[idx](x)
            x = xs / self.num_kernels
        
        x = F.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)
        
        # Remove channel dimension: [B, 1, T] ‚Üí [B, T]
        return x.squeeze(1)


class ISTFTHead(nn.Module):
    """
    [DEPRECATED] iSTFT Head –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó waveform –∑ frame-level features.
    
    WARNING: This approach causes metallic sound due to explicit phase prediction.
    Use WaveNeXtHead instead!

    –ì–µ–Ω–µ—Ä—É—î STFT magnitude —Ç–∞ phase, –ø–æ—Ç—ñ–º –∑–∞—Å—Ç–æ—Å–æ–≤—É—î iSTFT.

    Args:
        input_dim: Input feature dimension
        n_fft: FFT size
        hop_length: Hop size
    """

    def __init__(
        self,
        input_dim: int = 512,
        n_fft: int = 2048,
        hop_length: int = 512
    ):
        super().__init__()

        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_freqs = n_fft // 2 + 1

        # Project to magnitude and phase
        self.mag_proj = nn.Linear(input_dim, self.n_freqs)
        self.phase_proj = nn.Linear(input_dim, self.n_freqs)

        # Hann window for iSTFT
        window = torch.hann_window(n_fft)
        self.register_buffer("window", window)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Convert frame features to waveform via iSTFT.

        Args:
            x: Frame features [B, T, D]

        Returns:
            audio: Waveform [B, T_audio]
        """
        batch_size, num_frames, _ = x.shape

        # Predict magnitude (exp –¥–ª—è positive values)
        # CRITICAL: Clamp raw values to prevent magnitude explosion
        # exp(5) ‚âà 148, exp(4) ‚âà 55 - reasonable max magnitudes for audio
        mag_raw = self.mag_proj(x)
        mag = mag_raw.clamp(min=-20.0, max=5.0).exp()  # [B, T, n_freqs]

        # Predict phase (normalized to [-œÄ, œÄ])
        phase = self.phase_proj(x)  # [B, T, n_freqs]
        phase = torch.tanh(phase) * math.pi

        # Complex STFT
        real = mag * torch.cos(phase)
        imag = mag * torch.sin(phase)
        spec = torch.complex(real, imag)  # [B, T, n_freqs]
        spec = spec.transpose(1, 2)  # [B, n_freqs, T]

        # iSTFT
        audio = torch.istft(
            spec,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.n_fft,
            window=self.window,
            center=True,
            normalized=False,
            onesided=True,
            length=None,
            return_complex=False
        )

        return audio


class LatentDecoder(nn.Module):
    """
    Latent Decoder - –¥–µ–∫–æ–¥—É—î –ª–∞—Ç–µ–Ω—Ç–∏ –≤ waveform.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (HiFi-GAN style for clean audio):
    1. Conv1d(24 ‚Üí 512) + BatchNorm
    2. 10 dilated ConvNeXt blocks (dilations: [1,2,4,1,2,4,1,1,1,1])
    3. HiFiGANGenerator ‚Üí waveform with overlap-add (NO metallic artifacts!)

    Key insight: HiFi-GAN's transposed convolutions with proper kernel/stride
    create overlapping windows that naturally smooth frame boundaries.
    This eliminates the phase discontinuities that cause metallic sound.

    –í—Å—ñ –∫–æ–Ω–≤–æ–ª—é—Ü—ñ—ó –ö–ê–£–ó–ê–õ–¨–ù–Ü –¥–ª—è streaming –ø—ñ–¥—Ç—Ä–∏–º–∫–∏.
    """

    def __init__(
        self,
        input_dim: int = 24,
        hidden_dim: int = 512,
        num_blocks: int = 10,
        kernel_size: int = 7,
        intermediate_mult: int = 4,
        dilations: Optional[List[int]] = None,
        n_fft: int = 2048,  # kept for config compat, not used
        hop_length: int = 256,
        causal: bool = True,
        gradient_checkpointing: bool = False,
        # HiFi-GAN specific parameters
        upsample_rates: Optional[List[int]] = None,
        upsample_kernel_sizes: Optional[List[int]] = None,
        upsample_initial_channel: int = 512,
        resblock_kernel_sizes: Optional[List[int]] = None,
        resblock_dilation_sizes: Optional[List[List[int]]] = None,
        use_hifigan: bool = True  # Set False to use old WaveNeXtHead
    ):
        super().__init__()

        if dilations is None:
            dilations = [1, 2, 4, 1, 2, 4, 1, 1, 1, 1]
        
        # Default HiFi-GAN config for 22kHz (hop_length=256)
        # CRITICAL: product of upsample_rates MUST equal hop_length!
        if upsample_rates is None:
            upsample_rates = [8, 8, 2, 2]  # 8*8*2*2 = 256 ‚úì
        if upsample_kernel_sizes is None:
            upsample_kernel_sizes = [16, 16, 4, 4]  # kernel >= 2*stride ‚úì
        if resblock_kernel_sizes is None:
            resblock_kernel_sizes = [3, 7, 11]
        if resblock_dilation_sizes is None:
            resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.hop_length = hop_length
        self.use_hifigan = use_hifigan

        # Initial projection: latent ‚Üí hidden
        self.input_conv = nn.Conv1d(input_dim, hidden_dim, kernel_size=1)
        self.input_norm = nn.BatchNorm1d(hidden_dim)

        # Dilated ConvNeXt stack (causal!)
        self.convnext = ConvNeXtStack(
            dim=hidden_dim,
            num_blocks=num_blocks,
            intermediate_dim=hidden_dim * intermediate_mult,
            kernel_size=kernel_size,
            dilations=dilations,
            causal=causal,
            gradient_checkpointing=gradient_checkpointing
        )

        # Choose waveform generation head
        if use_hifigan:
            # HiFi-GAN Generator - RECOMMENDED for clean audio!
            self.head = HiFiGANGenerator(
                input_dim=hidden_dim,
                upsample_rates=upsample_rates,
                upsample_kernel_sizes=upsample_kernel_sizes,
                upsample_initial_channel=upsample_initial_channel,
                resblock_kernel_sizes=resblock_kernel_sizes,
                resblock_dilation_sizes=resblock_dilation_sizes
            )
        else:
            # [DEPRECATED] WaveNeXt head - causes metallic artifacts!
            self.head = WaveNeXtHead(
                input_dim=hidden_dim,
                head_dim=hidden_dim * intermediate_mult,
                hop_length=hop_length
            )

    def forward(self, latent: torch.Tensor) -> torch.Tensor:
        """
        Decode latent vectors to waveform.

        Args:
            latent: Latent vectors [B, latent_dim, T]

        Returns:
            audio: Waveform [B, T_audio]
        """
        # Initial projection
        x = self.input_conv(latent)  # [B, hidden, T]
        x = self.input_norm(x)

        # ConvNeXt blocks
        x = self.convnext(x)  # [B, hidden, T]

        # Generate waveform via HiFi-GAN (or WaveNeXt fallback)
        audio = self.head(x)  # [B, T * hop_length]

        return audio


class PeriodDiscriminator(nn.Module):
    """
    Period Discriminator –¥–ª—è GAN training.
    
    From Supertonic paper (Appendix A.1.3):
    "Each MPD consists of six convolutional layers with hidden sizes 
    16, 64, 256, 512, 512, and 1."

    Args:
        period: Sampling period
        channels: Number of channels per layer (paper: [16, 64, 256, 512, 512])
    """

    def __init__(
        self,
        period: int,
        channels: List[int] = [16, 64, 256, 512, 512]  # Paper spec!
    ):
        super().__init__()

        self.period = period

        layers = []
        in_ch = 1

        for i, out_ch in enumerate(channels):
            stride = 3 if i < len(channels) - 1 else 1
            layers.append(
                nn.Sequential(
                    nn.Conv2d(
                        in_ch, out_ch,
                        kernel_size=(5, 1),
                        stride=(stride, 1),
                        padding=(2, 0)
                    ),
                    nn.LeakyReLU(0.1)
                )
            )
            in_ch = out_ch

        # Final conv
        layers.append(
            nn.Conv2d(in_ch, 1, kernel_size=(3, 1), padding=(1, 0))
        )

        self.layers = nn.ModuleList(layers)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """
        Forward pass.

        Args:
            x: Audio [B, 1, T]

        Returns:
            output: Discriminator output
            features: Intermediate features for feature matching
        """
        batch_size, _, t = x.shape

        # Pad to multiple of period
        if t % self.period != 0:
            pad = self.period - (t % self.period)
            x = F.pad(x, (0, pad), mode='reflect')
            t = t + pad

        # Reshape to 2D: [B, 1, T/p, p]
        x = x.view(batch_size, 1, t // self.period, self.period)

        features = []
        for layer in self.layers:
            x = layer(x)
            features.append(x)

        return x.flatten(1, -1), features[:-1]


class MultiPeriodDiscriminator(nn.Module):
    """
    Multi-Period Discriminator (MPD) –¥–ª—è Supertonic v2.

    Periods: [2, 3, 5, 7, 11] - –ø–æ–∫—Ä–∏–≤–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ —á–∞—Å—Ç–æ—Ç–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏.
    """

    def __init__(
        self,
        periods: List[int] = [2, 3, 5, 7, 11]
    ):
        super().__init__()

        self.discriminators = nn.ModuleList([
            PeriodDiscriminator(period=p) for p in periods
        ])

    def forward(
        self,
        x: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[List[torch.Tensor]]]:
        """
        Forward pass through all period discriminators.

        Args:
            x: Audio [B, T] –∞–±–æ [B, 1, T]

        Returns:
            outputs: List of discriminator outputs
            features: List of feature lists
        """
        if x.dim() == 2:
            x = x.unsqueeze(1)

        outputs = []
        features = []

        for disc in self.discriminators:
            out, feat = disc(x)
            outputs.append(out)
            features.append(feat)

        return outputs, features


class SpectrogramDiscriminator(nn.Module):
    """
    Spectrogram-based Discriminator for MRD.
    
    From Supertonic paper (Appendix A.1.3, Table 7):
    "For MRDs, log-scaled linear spectrograms serve as input"
    
    Architecture (Table 7):
    - Conv2D: 1‚Üí16, kernel (5,5), stride (1,1)
    - Conv2D: 16‚Üí16, kernel (5,5), stride (2,1)
    - Conv2D: 16‚Üí16, kernel (5,5), stride (2,1)
    - Conv2D: 16‚Üí16, kernel (5,5), stride (2,1)
    - Conv2D: 16‚Üí16, kernel (5,5), stride (1,1)
    - Conv2D: 16‚Üí1, kernel (3,3), stride (1,1)
    
    Args:
        n_fft: FFT size for spectrogram
    """

    def __init__(self, n_fft: int = 1024):
        super().__init__()
        
        self.n_fft = n_fft
        self.hop_length = n_fft // 4  # Paper: "hop sizes are set to one-quarter"
        
        # Hann window
        self.register_buffer("window", torch.hann_window(n_fft))
        
        # Conv2D layers from Table 7
        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),
                nn.LeakyReLU(0.1)
            ),
            nn.Sequential(
                nn.Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2)),
                nn.LeakyReLU(0.1)
            ),
            nn.Sequential(
                nn.Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2)),
                nn.LeakyReLU(0.1)
            ),
            nn.Sequential(
                nn.Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2)),
                nn.LeakyReLU(0.1)
            ),
            nn.Sequential(
                nn.Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),
                nn.LeakyReLU(0.1)
            ),
            nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        ])
    
    def compute_spectrogram(self, x: torch.Tensor) -> torch.Tensor:
        """Compute log-scaled linear spectrogram."""
        # x: [B, T] or [B, 1, T]
        if x.dim() == 3:
            x = x.squeeze(1)
        
        # STFT
        spec = torch.stft(
            x, 
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.n_fft,
            window=self.window,
            center=True,
            return_complex=True
        )
        
        # Magnitude ‚Üí log scale
        mag = spec.abs()
        log_mag = torch.log(mag.clamp(min=1e-5))
        
        # [B, F, T] ‚Üí [B, 1, F, T]
        return log_mag.unsqueeze(1)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """Forward pass."""
        # Compute log spectrogram
        x = self.compute_spectrogram(x)
        
        features = []
        for layer in self.layers:
            x = layer(x)
            features.append(x)

        return x.flatten(1, -1), features[:-1]


class MultiResolutionDiscriminator(nn.Module):
    """
    Multi-Resolution Discriminator (MRD) –¥–ª—è Supertonic v2.
    
    From paper (Appendix A.1.3):
    "For MRDs, log-scaled linear spectrograms serve as input, 
    with three different FFT sizes: 512, 1024, and 2048."

    FFT sizes: [512, 1024, 2048]
    """

    def __init__(
        self,
        fft_sizes: List[int] = [512, 1024, 2048]
    ):
        super().__init__()

        self.fft_sizes = fft_sizes

        # One spectrogram discriminator per FFT size
        self.discriminators = nn.ModuleList([
            SpectrogramDiscriminator(n_fft=fft) for fft in fft_sizes
        ])

    def forward(
        self,
        x: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[List[torch.Tensor]]]:
        """Forward pass through all resolution discriminators."""
        outputs = []
        features = []

        for disc in self.discriminators:
            out, feat = disc(x)
            outputs.append(out)
            features.append(feat)

        return outputs, features


# Keep old ScaleDiscriminator for backward compatibility (deprecated)
class ScaleDiscriminator(nn.Module):
    """[DEPRECATED] Use SpectrogramDiscriminator instead."""

    def __init__(
        self,
        channels: List[int] = [128, 128, 256, 512, 1024, 1024, 1024]
    ):
        super().__init__()

        layers = []
        in_ch = 1

        for i, out_ch in enumerate(channels):
            if i == 0:
                layer = nn.Conv1d(
                    in_ch, out_ch,
                    kernel_size=15, stride=1, padding=7
                )
            elif i == len(channels) - 1:
                layer = nn.Conv1d(
                    in_ch, out_ch,
                    kernel_size=5, stride=1, padding=2
                )
            else:
                layer = nn.Conv1d(
                    in_ch, out_ch,
                    kernel_size=41, stride=4, padding=20, groups=4
                )

            layers.append(nn.Sequential(layer, nn.LeakyReLU(0.1)))
            in_ch = out_ch

        layers.append(nn.Conv1d(in_ch, 1, kernel_size=3, padding=1))
        self.layers = nn.ModuleList(layers)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        features = []
        for layer in self.layers:
            x = layer(x)
            features.append(x)
        return x.flatten(1, -1), features[:-1]


class SpeechAutoencoder(nn.Module):
    """
    Speech Autoencoder - –≥–æ–ª–æ–≤–Ω–∏–π –º–æ–¥—É–ª—å –¥–ª—è –∫–æ–¥—É–≤–∞–Ω–Ω—è/–¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (~47M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤):
    - MelSpectrogram: audio ‚Üí mel(228)
    - LatentEncoder: mel ‚Üí latent(24)
    - LatentDecoder: latent ‚Üí audio

    –î–ª—è GAN training:
    - MultiPeriodDiscriminator (MPD)
    - MultiResolutionDiscriminator (MRD)

    Temporal compression Kc=6:
    - –°—Ç–µ–∫ 6 —Ñ—Ä–µ–π–º—ñ–≤ ‚Üí 144-dim –≤–µ–∫—Ç–æ—Ä (–¥–ª—è flow-matching)

    Args:
        sample_rate: Audio sample rate
        n_fft: FFT size
        hop_length: Hop size
        n_mels: Number of mel bands
        latent_dim: Latent space dimension
        temporal_compression: Kc factor
        hidden_dim: Hidden dimension for encoder/decoder
        num_blocks: Number of ConvNeXt blocks
    """

    def __init__(
        self,
        sample_rate: int = 44100,
        n_fft: int = 2048,
        hop_length: int = 512,
        n_mels: int = 228,
        latent_dim: int = 24,
        temporal_compression: int = 6,
        hidden_dim: int = 512,
        num_encoder_blocks: int = 10,
        num_decoder_blocks: int = 10,
        decoder_dilations: Optional[List[int]] = None
    ):
        super().__init__()

        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        self.latent_dim = latent_dim
        self.temporal_compression = temporal_compression
        self.compressed_dim = latent_dim * temporal_compression  # 24 * 6 = 144

        if decoder_dilations is None:
            decoder_dilations = [1, 2, 4, 1, 2, 4, 1, 1, 1, 1]

        # Mel spectrogram extractor
        self.mel_spec = MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels
        )

        # Encoder
        self.encoder = LatentEncoder(
            input_dim=n_mels,
            hidden_dim=hidden_dim,
            output_dim=latent_dim,
            num_blocks=num_encoder_blocks
        )

        # Decoder
        self.decoder = LatentDecoder(
            input_dim=latent_dim,
            hidden_dim=hidden_dim,
            num_blocks=num_decoder_blocks,
            dilations=decoder_dilations,
            n_fft=n_fft,
            hop_length=hop_length,
            causal=True
        )

        # Discriminators (–¥–ª—è GAN training)
        self.mpd = MultiPeriodDiscriminator()
        self.mrd = MultiResolutionDiscriminator()

        # Statistics –¥–ª—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—ó –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
        self.register_buffer("latent_mean", torch.zeros(latent_dim))
        self.register_buffer("latent_std", torch.ones(latent_dim))

    def encode(self, audio: torch.Tensor) -> torch.Tensor:
        """
        Encode audio to latent vectors.

        Args:
            audio: Waveform [B, T]

        Returns:
            latent: [B, latent_dim, T_frames]
        """
        mel = self.mel_spec(audio)
        latent = self.encoder(mel)
        return latent

    def decode(self, latent: torch.Tensor) -> torch.Tensor:
        """
        Decode latent vectors to audio.

        Args:
            latent: [B, latent_dim, T_frames]

        Returns:
            audio: [B, T]
        """
        return self.decoder(latent)

    def forward(self, audio: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Full forward pass: encode ‚Üí decode.

        Args:
            audio: Waveform [B, T]

        Returns:
            Dict with 'latent', 'audio_recon', 'mel', 'mel_recon'
        """
        # Encode
        mel = self.mel_spec(audio)
        latent = self.encoder(mel)

        # Decode
        audio_recon = self.decoder(latent)

        # Mel of reconstructed audio
        mel_recon = self.mel_spec(audio_recon)

        return {
            'latent': latent,
            'audio_recon': audio_recon,
            'mel': mel,
            'mel_recon': mel_recon
        }

    def compress_latent(self, latent: torch.Tensor) -> torch.Tensor:
        """
        Temporal compression: stack Kc frames into one vector.

        Args:
            latent: [B, C, T] where C=24

        Returns:
            compressed: [B, C*Kc, T/Kc] where C*Kc=144
        """
        batch_size, c, t = latent.shape
        kc = self.temporal_compression

        # Pad to multiple of Kc
        if t % kc != 0:
            pad = kc - (t % kc)
            latent = F.pad(latent, (0, pad))
            t = t + pad

        # Reshape: [B, C, T] ‚Üí [B, C, T/Kc, Kc] ‚Üí [B, C*Kc, T/Kc]
        latent = latent.view(batch_size, c, t // kc, kc)
        latent = latent.permute(0, 1, 3, 2)  # [B, C, Kc, T/Kc]
        latent = latent.reshape(batch_size, c * kc, t // kc)

        return latent

    def decompress_latent(self, compressed: torch.Tensor) -> torch.Tensor:
        """
        Temporal decompression: unstack Kc frames.

        Args:
            compressed: [B, C*Kc, T/Kc] where C*Kc=144

        Returns:
            latent: [B, C, T] where C=24
        """
        batch_size, ckc, t_compressed = compressed.shape
        kc = self.temporal_compression
        c = ckc // kc

        # Reshape: [B, C*Kc, T/Kc] ‚Üí [B, C, Kc, T/Kc] ‚Üí [B, C, T]
        compressed = compressed.view(batch_size, c, kc, t_compressed)
        latent = compressed.permute(0, 1, 3, 2)  # [B, C, T/Kc, Kc]
        latent = latent.reshape(batch_size, c, t_compressed * kc)

        return latent

    def normalize_latent(self, latent: torch.Tensor) -> torch.Tensor:
        """Normalize latent using precomputed statistics."""
        mean = self.latent_mean.view(1, -1, 1)
        std = self.latent_std.view(1, -1, 1)
        return (latent - mean) / (std + 1e-8)

    def denormalize_latent(self, latent: torch.Tensor) -> torch.Tensor:
        """Denormalize latent using precomputed statistics."""
        mean = self.latent_mean.view(1, -1, 1)
        std = self.latent_std.view(1, -1, 1)
        return latent * std + mean

    def update_latent_statistics(self, latents: torch.Tensor):
        """Update running statistics for latent normalization."""
        # Channel-wise mean and std
        mean = latents.mean(dim=(0, 2))
        std = latents.std(dim=(0, 2))

        # EMA update
        momentum = 0.1
        self.latent_mean = (1 - momentum) * self.latent_mean + momentum * mean
        self.latent_std = (1 - momentum) * self.latent_std + momentum * std

    def discriminate(
        self,
        audio: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[List[torch.Tensor]]]:
        """
        Apply discriminators.

        Returns:
            mpd_outputs, mpd_features, mrd_outputs, mrd_features
        """
        mpd_out, mpd_feat = self.mpd(audio)
        mrd_out, mrd_feat = self.mrd(audio)
        return mpd_out, mpd_feat, mrd_out, mrd_feat


# ============================================================================
# Unit tests
# ============================================================================

def _test_mel_spectrogram():
    """–¢–µ—Å—Ç MelSpectrogram."""
    print("Testing MelSpectrogram...")

    mel_spec = MelSpectrogram(
        sample_rate=44100,
        n_fft=2048,
        hop_length=512,
        n_mels=228
    )

    # Random audio
    batch_size = 2
    duration = 2.0  # seconds
    audio = torch.randn(batch_size, int(44100 * duration))

    mel = mel_spec(audio)

    expected_frames = int(44100 * duration / 512) + 1
    print(f"  Audio: {audio.shape}")
    print(f"  Mel: {mel.shape}")
    print(f"  Expected frames: ~{expected_frames}")

    assert mel.shape[1] == 228, f"Expected 228 mels, got {mel.shape[1]}"
    print("MelSpectrogram tests passed! ‚úì\n")


def _test_latent_encoder():
    """–¢–µ—Å—Ç LatentEncoder."""
    print("Testing LatentEncoder...")

    encoder = LatentEncoder(
        input_dim=228,
        hidden_dim=512,
        output_dim=24,
        num_blocks=10
    )

    batch_size = 2
    seq_len = 100
    mel = torch.randn(batch_size, 228, seq_len)

    latent = encoder(mel)

    assert latent.shape == (batch_size, 24, seq_len)
    print(f"  Mel: {mel.shape} -> Latent: {latent.shape} ‚úì")

    num_params = sum(p.numel() for p in encoder.parameters())
    print(f"  Parameters: {num_params:,}")

    print("LatentEncoder tests passed! ‚úì\n")


def _test_latent_decoder():
    """–¢–µ—Å—Ç LatentDecoder."""
    print("Testing LatentDecoder...")

    decoder = LatentDecoder(
        input_dim=24,
        hidden_dim=512,
        num_blocks=10,
        dilations=[1, 2, 4, 1, 2, 4, 1, 1, 1, 1],
        n_fft=2048,
        hop_length=512,
        causal=True
    )

    batch_size = 2
    seq_len = 100
    latent = torch.randn(batch_size, 24, seq_len)

    audio = decoder(latent)

    expected_len = (seq_len - 1) * 512
    print(f"  Latent: {latent.shape}")
    print(f"  Audio: {audio.shape}")
    print(f"  Expected audio length: ~{expected_len}")

    num_params = sum(p.numel() for p in decoder.parameters())
    print(f"  Parameters: {num_params:,}")

    print("LatentDecoder tests passed! ‚úì\n")


def _test_discriminators():
    """–¢–µ—Å—Ç –¥–∏—Å–∫—Ä–∏–º—ñ–Ω–∞—Ç–æ—Ä—ñ–≤."""
    print("Testing Discriminators...")

    mpd = MultiPeriodDiscriminator(periods=[2, 3, 5, 7, 11])
    mrd = MultiResolutionDiscriminator(fft_sizes=[512, 1024, 2048])

    batch_size = 2
    audio = torch.randn(batch_size, 44100)  # 1 second

    mpd_out, mpd_feat = mpd(audio)
    print(f"  MPD outputs: {len(mpd_out)}, shapes: {[o.shape for o in mpd_out]}")

    mrd_out, mrd_feat = mrd(audio)
    print(f"  MRD outputs: {len(mrd_out)}, shapes: {[o.shape for o in mrd_out]}")

    mpd_params = sum(p.numel() for p in mpd.parameters())
    mrd_params = sum(p.numel() for p in mrd.parameters())
    print(f"  MPD parameters: {mpd_params:,}")
    print(f"  MRD parameters: {mrd_params:,}")

    print("Discriminator tests passed! ‚úì\n")


def _test_speech_autoencoder():
    """–¢–µ—Å—Ç –ø–æ–≤–Ω–æ–≥–æ Speech Autoencoder."""
    print("Testing SpeechAutoencoder...")

    autoencoder = SpeechAutoencoder(
        sample_rate=44100,
        n_fft=2048,
        hop_length=512,
        n_mels=228,
        latent_dim=24,
        temporal_compression=6,
        hidden_dim=512,
        num_encoder_blocks=10,
        num_decoder_blocks=10
    )

    batch_size = 2
    duration = 1.0
    audio = torch.randn(batch_size, int(44100 * duration))

    # Full forward
    output = autoencoder(audio)
    print(f"  Input audio: {audio.shape}")
    print(f"  Latent: {output['latent'].shape}")
    print(f"  Mel: {output['mel'].shape}")
    print(f"  Reconstructed audio: {output['audio_recon'].shape}")

    # Test compression
    latent = output['latent']
    compressed = autoencoder.compress_latent(latent)
    print(f"  Compressed latent: {latent.shape} -> {compressed.shape}")

    decompressed = autoencoder.decompress_latent(compressed)
    print(f"  Decompressed: {compressed.shape} -> {decompressed.shape}")

    # Parameter count
    total_params = sum(p.numel() for p in autoencoder.parameters())
    encoder_params = sum(p.numel() for p in autoencoder.encoder.parameters())
    decoder_params = sum(p.numel() for p in autoencoder.decoder.parameters())

    print(f"\n  Total parameters: {total_params:,}")
    print(f"  Encoder: {encoder_params:,}")
    print(f"  Decoder: {decoder_params:,}")

    print("\nSpeechAutoencoder tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_mel_spectrogram()
    _test_latent_encoder()
    _test_latent_decoder()
    _test_discriminators()
    _test_speech_autoencoder()
    print("All Speech Autoencoder tests passed! ‚úì")



==================================================
–§–ê–ô–õ: supertonic/models/text_to_latent.py
–†–û–ó–ú–Ü–†: 30.65 KB
==================================================

"""
Text-to-Latent Module - –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –ª–∞—Ç–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É —á–µ—Ä–µ–∑ conditional flow matching

–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (~19M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤):
1. Reference Encoder: Linear(144‚Üí128) ‚Üí 6 ConvNeXt ‚Üí 2 cross-attn ‚Üí 50 vectors
2. Text Encoder: char_emb(128) ‚Üí 6 ConvNeXt ‚Üí 4 self-attn(RoPE) ‚Üí 2 cross-attn
3. Vector Field (VF) Estimator: ConvNeXt + dilations + time + cross-attn(LARoPE)

Flow-matching:
- CFM –∑ œÉ_min=1e-8
- Classifier-Free Guidance (p_uncond=0.05)
- LARoPE (Œ≥=10) –¥–ª—è text-speech alignment

Temporal compression:
- Input: compressed latents (144-dim, T/6 frames)
- Output: velocity field –¥–ª—è ODE solving

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper (2509.11084)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, List, Tuple, Dict
from einops import rearrange

from supertonic.models.convnext import ConvNeXtBlock, ConvNeXtStack
from supertonic.models.attention import (
    MultiHeadAttention,
    SelfAttentionBlock,
    CrossAttentionBlock,
    TransformerBlock
)
from supertonic.models.larope import LARoPE, apply_larope, LARoPECrossAttention


class CharacterEmbedding(nn.Module):
    """
    Character-level embedding –¥–ª—è multilingual text.

    NO G2P REQUIRED ‚Äî –º–æ–¥–µ–ª—å —Å–∞–º–∞ –Ω–∞–≤—á–∞—î—Ç—å—Å—è pronunciation!

    –ü—ñ–¥—Ç—Ä–∏–º—É—î:
    - Latin (en, es, pt, fr)
    - Cyrillic (uk, ru)
    - Korean Hangul (ko)
    - CJK characters
    - Punctuation, numbers, special symbols

    Args:
        vocab_size: –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞ (default 512 –¥–ª—è extended multilingual)
        embed_dim: –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å embedding
        padding_idx: Index for padding token
    """

    def __init__(
        self,
        vocab_size: int = 512,
        embed_dim: int = 128,
        padding_idx: int = 0
    ):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.padding_idx = padding_idx

        # Character embedding table
        self.embedding = nn.Embedding(
            vocab_size,
            embed_dim,
            padding_idx=padding_idx
        )

        # Optional: language embedding (4-dim —è–∫ —É paper)
        self.lang_embedding = nn.Embedding(10, 4)  # 10 languages max

        # Projection if lang embedding used
        self.proj = nn.Linear(embed_dim + 4, embed_dim)

    def forward(
        self,
        chars: torch.Tensor,
        lang_id: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Embed characters.

        Args:
            chars: Character indices [B, L]
            lang_id: Optional language ID [B]

        Returns:
            embeddings: [B, L, D]
        """
        x = self.embedding(chars)  # [B, L, D]

        if lang_id is not None:
            lang_emb = self.lang_embedding(lang_id)  # [B, 4]
            lang_emb = lang_emb.unsqueeze(1).expand(-1, x.size(1), -1)  # [B, L, 4]
            x = torch.cat([x, lang_emb], dim=-1)  # [B, L, D+4]
            x = self.proj(x)  # [B, L, D]

        return x


class ReferenceEncoder(nn.Module):
    """
    Reference Encoder - –∫–æ–¥—É—î reference audio –¥–ª—è speaker conditioning.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    1. Linear(144 ‚Üí 128)
    2. 6 ConvNeXt blocks (kernel=5, intermediate=512)
    3. 2 cross-attention layers
    4. Output: 50 fixed-size reference vectors

    Input: Compressed latents –≤—ñ–¥ Speech Autoencoder (144-dim)
    Output: 50 reference vectors (128-dim each)
    """

    def __init__(
        self,
        input_dim: int = 144,
        hidden_dim: int = 128,
        num_convnext_blocks: int = 6,
        num_cross_attn_layers: int = 2,
        num_output_vectors: int = 50,
        kernel_size: int = 5,
        intermediate_mult: int = 4,
        num_heads: int = 4,
        gamma: float = 10.0
    ):
        super().__init__()

        self.num_output_vectors = num_output_vectors
        self.hidden_dim = hidden_dim

        # Input projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # ConvNeXt blocks
        self.convnext = ConvNeXtStack(
            dim=hidden_dim,
            num_blocks=num_convnext_blocks,
            intermediate_dim=hidden_dim * intermediate_mult,
            kernel_size=kernel_size,
            dilations=None,
            causal=False
        )

        # Learnable query vectors (50 fixed vectors)
        self.query_vectors = nn.Parameter(
            torch.randn(1, num_output_vectors, hidden_dim) * 0.02
        )

        # Cross-attention layers
        self.cross_attn_layers = nn.ModuleList([
            CrossAttentionBlock(
                dim=hidden_dim,
                num_heads=num_heads,
                gamma=gamma
            )
            for _ in range(num_cross_attn_layers)
        ])

        # Output normalization
        self.output_norm = nn.LayerNorm(hidden_dim)

    def forward(
        self,
        ref_latent: torch.Tensor,
        ref_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Encode reference audio latents.

        Args:
            ref_latent: Reference latents [B, C=144, T]
            ref_mask: Optional mask [B, T]

        Returns:
            ref_vectors: [B, 50, D=128]
        """
        batch_size = ref_latent.shape[0]

        # Input projection [B, T, 144] ‚Üí [B, T, 128]
        x = ref_latent.transpose(1, 2)  # [B, T, C]
        x = self.input_proj(x)

        # ConvNeXt processing
        x = x.transpose(1, 2)  # [B, C, T]
        x = self.convnext(x)
        x = x.transpose(1, 2)  # [B, T, C]

        # Cross-attention: query vectors attend to encoded features
        queries = self.query_vectors.expand(batch_size, -1, -1)  # [B, 50, D]

        for cross_attn in self.cross_attn_layers:
            queries = cross_attn(queries, x, context_mask=ref_mask)

        # Output normalization
        ref_vectors = self.output_norm(queries)

        return ref_vectors


class TextEncoder(nn.Module):
    """
    Text Encoder - –∫–æ–¥—É—î —Ç–µ–∫—Å—Ç –∑ reference conditioning.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    1. Character embedding ‚Üí 128-dim
    2. 6 ConvNeXt blocks (kernel=5)
    3. 4 self-attention blocks (512 filters, 4 heads, RoPE)
    4. 2 cross-attention layers –∑ reference vectors (LARoPE)

    Args:
        vocab_size: Character vocabulary size
        embed_dim: Embedding dimension
        hidden_dim: Hidden dimension after expansion
        num_convnext_blocks: Number of ConvNeXt blocks
        num_self_attn_blocks: Number of self-attention blocks
        num_cross_attn_layers: Number of cross-attention layers
        num_heads: Number of attention heads
        kernel_size: ConvNeXt kernel size
        gamma: LARoPE gamma parameter
    """

    def __init__(
        self,
        vocab_size: int = 512,
        embed_dim: int = 128,
        hidden_dim: int = 512,
        num_convnext_blocks: int = 6,
        num_self_attn_blocks: int = 4,
        num_cross_attn_layers: int = 2,
        num_heads: int = 4,
        kernel_size: int = 5,
        gamma: float = 10.0
    ):
        super().__init__()

        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim

        # Character embedding
        self.char_embed = CharacterEmbedding(
            vocab_size=vocab_size,
            embed_dim=embed_dim
        )

        # Projection to hidden dim
        self.input_proj = nn.Linear(embed_dim, hidden_dim)

        # ConvNeXt blocks
        self.convnext = ConvNeXtStack(
            dim=hidden_dim,
            num_blocks=num_convnext_blocks,
            intermediate_dim=hidden_dim * 4,
            kernel_size=kernel_size,
            dilations=None,
            causal=False
        )

        # Self-attention blocks –∑ RoPE
        self.self_attn_blocks = nn.ModuleList([
            SelfAttentionBlock(
                dim=hidden_dim,
                num_heads=num_heads,
                use_rope=True
            )
            for _ in range(num_self_attn_blocks)
        ])

        # Cross-attention –∑ reference vectors (LARoPE)
        self.cross_attn_layers = nn.ModuleList([
            CrossAttentionBlock(
                dim=hidden_dim,
                context_dim=128,  # Reference vectors dim
                num_heads=num_heads,
                gamma=gamma
            )
            for _ in range(num_cross_attn_layers)
        ])

        # Output normalization
        self.output_norm = nn.LayerNorm(hidden_dim)

    def forward(
        self,
        text: torch.Tensor,
        ref_vectors: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None,
        lang_id: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Encode text with reference conditioning.

        Args:
            text: Character indices [B, L]
            ref_vectors: Reference vectors [B, 50, 128]
            text_mask: Optional mask [B, L]
            lang_id: Optional language ID [B]

        Returns:
            text_encoding: [B, L, D=512]
        """
        # Character embedding
        x = self.char_embed(text, lang_id)  # [B, L, 128]

        # Project to hidden dim
        x = self.input_proj(x)  # [B, L, 512]

        # ConvNeXt processing
        x = x.transpose(1, 2)  # [B, D, L]
        x = self.convnext(x)
        x = x.transpose(1, 2)  # [B, L, D]

        # Self-attention blocks
        for self_attn in self.self_attn_blocks:
            x = self_attn(x, mask=text_mask)

        # Cross-attention –∑ reference vectors
        for cross_attn in self.cross_attn_layers:
            x = cross_attn(x, ref_vectors)

        # Output normalization
        x = self.output_norm(x)

        return x


class TimeEmbedding(nn.Module):
    """
    Time embedding –¥–ª—è flow-matching.

    Sinusoidal embedding + MLP, —Å—Ö–æ–∂–µ –Ω–∞ diffusion models.

    Args:
        dim: Output dimension
        max_period: Maximum period for sinusoidal embedding
    """

    def __init__(self, dim: int, max_period: int = 10000):
        super().__init__()

        self.dim = dim
        self.max_period = max_period

        # MLP –¥–ª—è time embedding
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.SiLU(),
            nn.Linear(dim * 4, dim)
        )

    def forward(self, t: torch.Tensor) -> torch.Tensor:
        """
        Embed timestep.

        Args:
            t: Timestep [B] in range [0, 1]

        Returns:
            embedding: [B, D]
        """
        half_dim = self.dim // 2
        freqs = torch.exp(
            -math.log(self.max_period) *
            torch.arange(half_dim, device=t.device) / half_dim
        )

        # [B] √ó [D/2] ‚Üí [B, D/2]
        args = t.unsqueeze(-1) * freqs.unsqueeze(0)

        # Sinusoidal embedding
        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)

        # MLP
        emb = self.mlp(emb)

        return emb


class VectorFieldEstimator(nn.Module):
    """
    Vector Field (VF) Estimator –¥–ª—è conditional flow matching.

    –ü–µ—Ä–µ–¥–±–∞—á–∞—î velocity field v(z_t, z_ref, text, t) –¥–ª—è ODE solving.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    - ConvNeXt blocks –∑ dilations
    - Time conditioning —á–µ—Ä–µ–∑ global embedding addition
    - Text/reference conditioning —á–µ—Ä–µ–∑ cross-attention –∑ LARoPE

    Args:
        latent_dim: Input latent dimension (144 compressed)
        hidden_dim: Hidden dimension
        text_dim: Text encoding dimension
        num_blocks: Number of ConvNeXt blocks
        kernel_size: ConvNeXt kernel size
        dilations: Dilation factors
        num_heads: Number of attention heads
        gamma: LARoPE gamma
    """

    def __init__(
        self,
        latent_dim: int = 144,
        hidden_dim: int = 512,
        text_dim: int = 512,
        num_blocks: int = 8,
        kernel_size: int = 7,
        dilations: Optional[List[int]] = None,
        num_heads: int = 4,
        gamma: float = 10.0
    ):
        super().__init__()

        if dilations is None:
            dilations = [1, 2, 4, 8, 1, 2, 4, 8]

        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim

        # Input projection
        self.input_proj = nn.Conv1d(latent_dim, hidden_dim, kernel_size=1)

        # Time embedding
        self.time_embed = TimeEmbedding(hidden_dim)

        # ConvNeXt blocks –∑ cross-attention
        self.blocks = nn.ModuleList()
        for i, dilation in enumerate(dilations):
            self.blocks.append(
                VFBlock(
                    dim=hidden_dim,
                    text_dim=text_dim,
                    kernel_size=kernel_size,
                    dilation=dilation,
                    num_heads=num_heads,
                    gamma=gamma
                )
            )

        # Output projection
        self.output_norm = nn.LayerNorm(hidden_dim)
        self.output_proj = nn.Linear(hidden_dim, latent_dim)

    def forward(
        self,
        z_t: torch.Tensor,           # Noisy latent [B, 144, T]
        z_ref: torch.Tensor,         # Reference latent (masked) [B, 144, T]
        text_encoding: torch.Tensor, # Text encoding [B, L, 512]
        t: torch.Tensor,             # Timestep [B]
        text_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Estimate velocity field.

        Args:
            z_t: Noisy latent at time t
            z_ref: Reference latent (with masking)
            text_encoding: Encoded text
            t: Timestep in [0, 1]
            text_mask: Optional text mask

        Returns:
            velocity: Predicted velocity [B, 144, T]
        """
        batch_size = z_t.shape[0]

        # Concatenate z_t and z_ref for context
        # x = torch.cat([z_t, z_ref], dim=1)  # [B, 288, T]
        x = z_t + z_ref  # Additive conditioning (simpler)

        # Input projection
        x = self.input_proj(x)  # [B, hidden, T]

        # Time embedding (global)
        t_emb = self.time_embed(t)  # [B, hidden]

        # Process through blocks
        x = x.transpose(1, 2)  # [B, T, hidden]
        for block in self.blocks:
            x = block(x, text_encoding, t_emb, text_mask)

        # Output projection
        x = self.output_norm(x)
        velocity = self.output_proj(x)  # [B, T, 144]
        velocity = velocity.transpose(1, 2)  # [B, 144, T]

        return velocity


class VFBlock(nn.Module):
    """
    Vector Field Block - ConvNeXt + time conditioning + cross-attention.

    Args:
        dim: Hidden dimension
        text_dim: Text encoding dimension
        kernel_size: ConvNeXt kernel size
        dilation: Dilation factor
        num_heads: Number of attention heads
        gamma: LARoPE gamma
    """

    def __init__(
        self,
        dim: int,
        text_dim: int,
        kernel_size: int = 7,
        dilation: int = 1,
        num_heads: int = 4,
        gamma: float = 10.0
    ):
        super().__init__()

        # ConvNeXt block
        self.convnext = ConvNeXtBlock(
            dim=dim,
            intermediate_dim=dim * 4,
            kernel_size=kernel_size,
            dilation=dilation,
            causal=False
        )

        # Time conditioning (scale + shift)
        self.time_proj = nn.Sequential(
            nn.SiLU(),
            nn.Linear(dim, dim * 2)
        )

        # Cross-attention –∑ text (LARoPE)
        self.cross_attn = CrossAttentionBlock(
            dim=dim,
            context_dim=text_dim,
            num_heads=num_heads,
            gamma=gamma
        )

        # Normalization
        self.norm = nn.LayerNorm(dim)

    def forward(
        self,
        x: torch.Tensor,             # [B, T, D]
        text_encoding: torch.Tensor, # [B, L, D_text]
        t_emb: torch.Tensor,         # [B, D]
        text_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Forward pass."""
        # ConvNeXt (transpose for conv)
        x = x.transpose(1, 2)  # [B, D, T]
        x = self.convnext(x)
        x = x.transpose(1, 2)  # [B, T, D]

        # Time conditioning (AdaLN style)
        t_proj = self.time_proj(t_emb)  # [B, D*2]
        scale, shift = t_proj.chunk(2, dim=-1)
        scale = scale.unsqueeze(1)  # [B, 1, D]
        shift = shift.unsqueeze(1)

        x = self.norm(x) * (1 + scale) + shift

        # Cross-attention –∑ text
        x = self.cross_attn(x, text_encoding, context_mask=text_mask)

        return x


class TextToLatent(nn.Module):
    """
    Text-to-Latent Module - –≥–æ–ª–æ–≤–Ω–∏–π –º–æ–¥—É–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –ª–∞—Ç–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É.

    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (~19M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤):
    - Reference Encoder ‚Üí 50 reference vectors
    - Text Encoder ‚Üí text encoding
    - Vector Field Estimator ‚Üí velocity for flow-matching

    Flow-matching training:
    - z_t = (1 - (1-œÉ)t)¬∑z_0 + t¬∑z_1
    - target = z_1 - (1-œÉ)¬∑z_0
    - loss = |m ¬∑ (v_Œ∏ - target)|‚ÇÅ

    Inference:
    - ODE solving –∑ Euler method
    - CFG –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ

    Args:
        latent_dim: Latent dimension (24 uncompressed, 144 compressed)
        vocab_size: Character vocabulary size
        hidden_dim: Hidden dimension for encoders
        vf_hidden_dim: Hidden dimension for VF estimator
        sigma_min: Minimum sigma for flow-matching
        p_uncond: Probability of unconditional training (CFG)
        cfg_scale: Classifier-free guidance scale
    """

    def __init__(
        self,
        latent_dim: int = 144,  # Compressed: 24 * 6
        vocab_size: int = 512,
        text_embed_dim: int = 128,
        text_hidden_dim: int = 512,
        ref_hidden_dim: int = 128,
        vf_hidden_dim: int = 512,
        num_ref_vectors: int = 50,
        sigma_min: float = 1e-8,
        p_uncond: float = 0.05,
        cfg_scale: float = 3.0,
        gamma: float = 10.0
    ):
        super().__init__()

        self.latent_dim = latent_dim
        self.sigma_min = sigma_min
        self.p_uncond = p_uncond
        self.cfg_scale = cfg_scale

        # Reference Encoder
        self.reference_encoder = ReferenceEncoder(
            input_dim=latent_dim,
            hidden_dim=ref_hidden_dim,
            num_convnext_blocks=6,
            num_cross_attn_layers=2,
            num_output_vectors=num_ref_vectors,
            gamma=gamma
        )

        # Text Encoder
        self.text_encoder = TextEncoder(
            vocab_size=vocab_size,
            embed_dim=text_embed_dim,
            hidden_dim=text_hidden_dim,
            num_convnext_blocks=6,
            num_self_attn_blocks=4,
            num_cross_attn_layers=2,
            gamma=gamma
        )

        # Vector Field Estimator
        self.vector_field = VectorFieldEstimator(
            latent_dim=latent_dim,
            hidden_dim=vf_hidden_dim,
            text_dim=text_hidden_dim,
            num_blocks=8,
            gamma=gamma
        )

        # Learnable unconditional embeddings (–¥–ª—è CFG)
        self.uncond_ref = nn.Parameter(torch.randn(1, num_ref_vectors, ref_hidden_dim) * 0.02)
        self.uncond_text = nn.Parameter(torch.randn(1, 1, text_hidden_dim) * 0.02)

    def encode_reference(
        self,
        ref_latent: torch.Tensor,
        ref_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode reference audio."""
        return self.reference_encoder(ref_latent, ref_mask)

    def encode_text(
        self,
        text: torch.Tensor,
        ref_vectors: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None,
        lang_id: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode text with reference conditioning."""
        return self.text_encoder(text, ref_vectors, text_mask, lang_id)

    def estimate_velocity(
        self,
        z_t: torch.Tensor,
        z_ref: torch.Tensor,
        text_encoding: torch.Tensor,
        t: torch.Tensor,
        text_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Estimate velocity field."""
        return self.vector_field(z_t, z_ref, text_encoding, t, text_mask)

    def forward(
        self,
        z_1: torch.Tensor,           # Target latent [B, 144, T]
        text: torch.Tensor,          # Text [B, L]
        ref_latent: torch.Tensor,    # Reference latent [B, 144, T_ref]
        text_mask: Optional[torch.Tensor] = None,
        ref_mask: Optional[torch.Tensor] = None,
        lang_id: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Training forward pass.

        Implements flow-matching loss computation:
        1. Sample t ~ U[0,1]
        2. Sample z_0 ~ N(0,I)
        3. Interpolate z_t
        4. Compute target velocity
        5. Estimate velocity
        6. Compute masked L1 loss

        Args:
            z_1: Target latent (from autoencoder)
            text: Text tokens
            ref_latent: Reference audio latent
            text_mask: Optional text mask
            ref_mask: Optional reference mask
            lang_id: Optional language ID

        Returns:
            Dict with 'loss', 'velocity_pred', 'velocity_target'
        """
        batch_size = z_1.shape[0]
        device = z_1.device

        # === Encode reference and text ===
        ref_vectors = self.encode_reference(ref_latent, ref_mask)
        text_encoding = self.encode_text(text, ref_vectors, text_mask, lang_id)

        # === CFG: occasionally use unconditional embeddings ===
        if self.training and self.p_uncond > 0:
            uncond_mask = torch.rand(batch_size, device=device) < self.p_uncond
            if uncond_mask.any():
                # Replace with unconditional embeddings
                ref_vectors = torch.where(
                    uncond_mask.view(-1, 1, 1),
                    self.uncond_ref.expand(batch_size, -1, -1),
                    ref_vectors
                )
                # –î–ª—è text encoding –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ–∫–æ–¥—É–≤–∞—Ç–∏
                # (—Å–ø—Ä–æ—â–µ–Ω–Ω—è: –ø—Ä–æ—Å—Ç–æ –∑–∞–º—ñ–Ω—é—î–º–æ –Ω–∞ uncond_text)
                uncond_text_expanded = self.uncond_text.expand(
                    batch_size, text_encoding.size(1), -1
                )
                text_encoding = torch.where(
                    uncond_mask.view(-1, 1, 1),
                    uncond_text_expanded,
                    text_encoding
                )

        # === Sample timestep t ~ U[0, 1] ===
        t = torch.rand(batch_size, device=device)

        # === Sample noise z_0 ~ N(0, I) ===
        z_0 = torch.randn_like(z_1)

        # === Interpolate: z_t = (1 - (1-œÉ)t)¬∑z_0 + t¬∑z_1 ===
        sigma = self.sigma_min
        z_t = (1 - (1 - sigma) * t.view(-1, 1, 1)) * z_0 + t.view(-1, 1, 1) * z_1

        # === Target velocity: z_1 - (1-œÉ)¬∑z_0 ===
        velocity_target = z_1 - (1 - sigma) * z_0

        # === Create reference mask (random crop for anti-leakage) ===
        mask = self._create_reference_mask(z_1.shape, device)
        z_ref = (1 - mask) * z_1  # Keep unmasked parts of target

        # === Estimate velocity ===
        velocity_pred = self.estimate_velocity(z_t, z_ref, text_encoding, t, text_mask)

        # === Compute masked L1 loss ===
        loss = (mask * (velocity_pred - velocity_target).abs()).mean()

        return {
            'loss': loss,
            'velocity_pred': velocity_pred,
            'velocity_target': velocity_target,
            't': t
        }

    def _create_reference_mask(
        self,
        shape: Tuple[int, ...],
        device: torch.device,
        mask_ratio_range: Tuple[float, float] = (0.3, 0.7)
    ) -> torch.Tensor:
        """
        Create reference mask for training.

        Random contiguous region is masked (ones), rest is unmasked (zeros).
        This prevents information leakage from reference.
        """
        batch_size, c, t = shape

        masks = []
        for _ in range(batch_size):
            # Random mask ratio
            ratio = torch.rand(1).item() * (mask_ratio_range[1] - mask_ratio_range[0])
            ratio += mask_ratio_range[0]

            mask_len = int(t * ratio)
            start = torch.randint(0, t - mask_len + 1, (1,)).item()

            mask = torch.zeros(c, t, device=device)
            mask[:, start:start + mask_len] = 1.0
            masks.append(mask)

        return torch.stack(masks)

    @torch.no_grad()
    def generate(
        self,
        text: torch.Tensor,
        ref_latent: torch.Tensor,
        num_frames: int,
        nfe: int = 32,
        cfg_scale: Optional[float] = None,
        text_mask: Optional[torch.Tensor] = None,
        ref_mask: Optional[torch.Tensor] = None,
        lang_id: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Generate latents using Euler ODE solver.

        Args:
            text: Text tokens [B, L]
            ref_latent: Reference latent [B, 144, T_ref]
            num_frames: Number of output frames
            nfe: Number of function evaluations (Euler steps)
            cfg_scale: CFG scale (None = use default)
            text_mask: Optional text mask
            ref_mask: Optional reference mask
            lang_id: Optional language ID

        Returns:
            z: Generated latents [B, 144, num_frames]
        """
        batch_size = text.shape[0]
        device = text.device
        cfg_scale = cfg_scale or self.cfg_scale

        # Encode reference and text (conditional)
        ref_vectors = self.encode_reference(ref_latent, ref_mask)
        text_encoding = self.encode_text(text, ref_vectors, text_mask, lang_id)

        # Unconditional embeddings
        ref_vectors_uncond = self.uncond_ref.expand(batch_size, -1, -1)
        text_encoding_uncond = self.uncond_text.expand(batch_size, text_encoding.size(1), -1)

        # Initialize from noise
        z = torch.randn(batch_size, self.latent_dim, num_frames, device=device)

        # Reference latent (no masking at inference)
        z_ref = torch.zeros_like(z)

        # Euler integration from t=0 to t=1
        dt = 1.0 / nfe
        for step in range(nfe):
            t = torch.full((batch_size,), step * dt, device=device)

            # Conditional velocity
            v_cond = self.estimate_velocity(z, z_ref, text_encoding, t, text_mask)

            # Unconditional velocity
            v_uncond = self.estimate_velocity(z, z_ref, text_encoding_uncond, t, text_mask)

            # CFG combination
            velocity = v_uncond + cfg_scale * (v_cond - v_uncond)

            # Euler step
            z = z + velocity * dt

        return z


# ============================================================================
# Unit tests
# ============================================================================

def _test_character_embedding():
    """–¢–µ—Å—Ç CharacterEmbedding."""
    print("Testing CharacterEmbedding...")

    embed = CharacterEmbedding(vocab_size=512, embed_dim=128)

    batch_size = 2
    seq_len = 50
    chars = torch.randint(0, 512, (batch_size, seq_len))

    out = embed(chars)
    assert out.shape == (batch_size, seq_len, 128)
    print(f"  {chars.shape} -> {out.shape} ‚úì")

    # With language ID
    lang_id = torch.tensor([0, 5])  # English, Ukrainian
    out_lang = embed(chars, lang_id)
    assert out_lang.shape == (batch_size, seq_len, 128)
    print(f"  With lang_id: {out_lang.shape} ‚úì")

    print("CharacterEmbedding tests passed! ‚úì\n")


def _test_reference_encoder():
    """–¢–µ—Å—Ç ReferenceEncoder."""
    print("Testing ReferenceEncoder...")

    encoder = ReferenceEncoder(
        input_dim=144,
        hidden_dim=128,
        num_output_vectors=50
    )

    batch_size = 2
    t_ref = 100
    ref_latent = torch.randn(batch_size, 144, t_ref)

    ref_vectors = encoder(ref_latent)
    assert ref_vectors.shape == (batch_size, 50, 128)
    print(f"  {ref_latent.shape} -> {ref_vectors.shape} ‚úì")

    num_params = sum(p.numel() for p in encoder.parameters())
    print(f"  Parameters: {num_params:,}")

    print("ReferenceEncoder tests passed! ‚úì\n")


def _test_text_encoder():
    """–¢–µ—Å—Ç TextEncoder."""
    print("Testing TextEncoder...")

    encoder = TextEncoder(
        vocab_size=512,
        embed_dim=128,
        hidden_dim=512
    )

    batch_size = 2
    text_len = 50
    text = torch.randint(0, 512, (batch_size, text_len))
    ref_vectors = torch.randn(batch_size, 50, 128)

    text_encoding = encoder(text, ref_vectors)
    assert text_encoding.shape == (batch_size, text_len, 512)
    print(f"  Text: {text.shape}, Ref: {ref_vectors.shape} -> {text_encoding.shape} ‚úì")

    num_params = sum(p.numel() for p in encoder.parameters())
    print(f"  Parameters: {num_params:,}")

    print("TextEncoder tests passed! ‚úì\n")


def _test_vector_field_estimator():
    """–¢–µ—Å—Ç VectorFieldEstimator."""
    print("Testing VectorFieldEstimator...")

    vf = VectorFieldEstimator(
        latent_dim=144,
        hidden_dim=512,
        text_dim=512,
        num_blocks=8
    )

    batch_size = 2
    t_latent = 50
    text_len = 30

    z_t = torch.randn(batch_size, 144, t_latent)
    z_ref = torch.randn(batch_size, 144, t_latent)
    text_encoding = torch.randn(batch_size, text_len, 512)
    t = torch.rand(batch_size)

    velocity = vf(z_t, z_ref, text_encoding, t)
    assert velocity.shape == z_t.shape
    print(f"  z_t: {z_t.shape}, text: {text_encoding.shape} -> velocity: {velocity.shape} ‚úì")

    num_params = sum(p.numel() for p in vf.parameters())
    print(f"  Parameters: {num_params:,}")

    print("VectorFieldEstimator tests passed! ‚úì\n")


def _test_text_to_latent():
    """–¢–µ—Å—Ç –ø–æ–≤–Ω–æ–≥–æ TextToLatent –º–æ–¥—É–ª—è."""
    print("Testing TextToLatent...")

    model = TextToLatent(
        latent_dim=144,
        vocab_size=512,
        text_embed_dim=128,
        text_hidden_dim=512,
        vf_hidden_dim=512
    )

    batch_size = 2
    text_len = 50
    t_latent = 60
    t_ref = 100

    text = torch.randint(0, 512, (batch_size, text_len))
    z_1 = torch.randn(batch_size, 144, t_latent)
    ref_latent = torch.randn(batch_size, 144, t_ref)

    # Training forward
    output = model(z_1, text, ref_latent)
    print(f"  Loss: {output['loss'].item():.6f}")
    print(f"  Velocity pred: {output['velocity_pred'].shape}")
    print(f"  Velocity target: {output['velocity_target'].shape}")

    # Generation
    model.eval()
    generated = model.generate(
        text=text,
        ref_latent=ref_latent,
        num_frames=t_latent,
        nfe=8  # Quick test
    )
    print(f"  Generated: {generated.shape} ‚úì")

    # Parameter count
    total_params = sum(p.numel() for p in model.parameters())
    ref_params = sum(p.numel() for p in model.reference_encoder.parameters())
    text_params = sum(p.numel() for p in model.text_encoder.parameters())
    vf_params = sum(p.numel() for p in model.vector_field.parameters())

    print(f"\n  Total parameters: {total_params:,}")
    print(f"  Reference encoder: {ref_params:,}")
    print(f"  Text encoder: {text_params:,}")
    print(f"  Vector field: {vf_params:,}")

    print("\nTextToLatent tests passed! ‚úì\n")


if __name__ == "__main__":
    _test_character_embedding()
    _test_reference_encoder()
    _test_text_encoder()
    _test_vector_field_estimator()
    _test_text_to_latent()
    print("All Text-to-Latent tests passed! ‚úì")



==================================================
–§–ê–ô–õ: supertonic/utils/__init__.py
–†–û–ó–ú–Ü–†: 0.1 KB
==================================================

"""Supertonic utilities."""
from .training_logger import TrainingLogger

__all__ = ["TrainingLogger"]



==================================================
–§–ê–ô–õ: supertonic/utils/training_logger.py
–†–û–ó–ú–Ü–†: 15.1 KB
==================================================

"""
Beautiful Training Logger for Supertonic v2

Features:
  - Rich console output with colors and tables
  - Real-time metrics dashboard
  - GPU monitoring
  - ETA calculation
  - Loss trend visualization
"""

import os
import sys
import time
from datetime import datetime, timedelta
from collections import deque
from typing import Dict, Optional, List, Any
from dataclasses import dataclass, field

try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, BarColumn, TimeRemainingColumn, TextColumn, SpinnerColumn
    from rich.live import Live
    from rich.layout import Layout
    from rich.text import Text
    from rich import box
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


@dataclass
class MetricsHistory:
    """Stores metrics history for trend analysis."""
    window_size: int = 100
    g_loss: deque = field(default_factory=lambda: deque(maxlen=100))
    d_loss: deque = field(default_factory=lambda: deque(maxlen=100))
    recon_loss: deque = field(default_factory=lambda: deque(maxlen=100))
    adv_loss: deque = field(default_factory=lambda: deque(maxlen=100))
    fm_loss: deque = field(default_factory=lambda: deque(maxlen=100))
    
    # Timing
    step_times: deque = field(default_factory=lambda: deque(maxlen=50))


class TrainingLogger:
    """Beautiful training logger with rich console output."""
    
    def __init__(
        self,
        total_iterations: int,
        log_interval: int = 50,
        checkpoint_interval: int = 1000,
        validation_interval: int = 5000,
        disc_start_steps: int = 5000,
        rank: int = 0,
        world_size: int = 1
    ):
        self.total_iterations = total_iterations
        self.log_interval = log_interval
        self.checkpoint_interval = checkpoint_interval
        self.validation_interval = validation_interval
        self.disc_start_steps = disc_start_steps
        self.rank = rank
        self.world_size = world_size
        self.is_main = (rank == 0)
        
        self.start_time = time.time()
        self.last_log_time = time.time()
        self.start_iteration = 0  # For accurate it/s after resume
        self.history = MetricsHistory()
        
        self.console = Console() if RICH_AVAILABLE and self.is_main else None
        
        if self.is_main:
            self._print_banner()
    
    def _print_banner(self):
        """Print beautiful startup banner."""
        if self.console:
            banner = """
[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                               ‚ïë
‚ïë   [bold white]üîä SUPERTONIC v2[/bold white] - Ukrainian TTS Training                                  ‚ïë
‚ïë                                                                               ‚ïë
‚ïë   [yellow]Architecture:[/yellow] WaveNeXt Speech Autoencoder                                   ‚ïë
‚ïë   [yellow]Paper:[/yellow]        arXiv:2503.23108v3                                            ‚ïë
‚ïë   [yellow]Stage:[/yellow]        Stage 1 - Speech Autoencoder Training                         ‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]
"""
            self.console.print(banner)
        else:
            print("=" * 80)
            print("üîä SUPERTONIC v2 - Ukrainian TTS Training")
            print("=" * 80)
    
    def print_config(self, config: Dict[str, Any]):
        """Print training configuration in a nice table."""
        if not self.is_main:
            return
        
        if self.console:
            table = Table(title="Training Configuration", box=box.ROUNDED)
            table.add_column("Parameter", style="cyan", width=30)
            table.add_column("Value", style="green", width=25)
            
            table.add_row("Total Iterations", f"{self.total_iterations:,}")
            table.add_row("Batch Size (per GPU)", str(config.get('batch_size', 'N/A')))
            table.add_row("Effective Batch Size", f"{config.get('batch_size', 0) * self.world_size}")
            table.add_row("Learning Rate", f"{config.get('learning_rate', 'N/A')}")
            table.add_row("GPUs", f"{self.world_size}√ó")
            table.add_row("", "")
            table.add_row("Checkpoint Every", f"{self.checkpoint_interval:,} steps")
            table.add_row("Validation Every", f"{self.validation_interval:,} steps")
            table.add_row("Log Every", f"{self.log_interval} steps")
            table.add_row("Discriminator Starts", f"Step {self.disc_start_steps:,}")
            table.add_row("", "")
            table.add_row("Œª_recon", str(config.get('loss_weights', {}).get('reconstruction', 45)))
            table.add_row("Œª_adv", str(config.get('loss_weights', {}).get('adversarial', 1)))
            table.add_row("Œª_fm", str(config.get('loss_weights', {}).get('feature_matching', 0.1)))
            
            self.console.print(table)
            print()
        else:
            print(f"Training Config:")
            print(f"  Total iterations: {self.total_iterations:,}")
            print(f"  Batch size: {config.get('batch_size', 'N/A')} √ó {self.world_size} GPUs")
            print(f"  Learning rate: {config.get('learning_rate', 'N/A')}")
    
    def log_step(self, iteration: int, losses: Dict[str, float]):
        """Log a training step with beautiful formatting."""
        if not self.is_main:
            return
        
        # Update history
        self.history.g_loss.append(losses.get('g_loss', 0))
        self.history.d_loss.append(losses.get('d_loss', 0))
        self.history.recon_loss.append(losses.get('recon_loss', 0))
        self.history.adv_loss.append(losses.get('adv_loss', 0))
        self.history.fm_loss.append(losses.get('fm_loss', 0))
        
        # Timing
        current_time = time.time()
        step_time = current_time - self.last_log_time
        self.history.step_times.append(step_time / self.log_interval)
        self.last_log_time = current_time
        
        # Only log at intervals
        if iteration % self.log_interval != 0:
            return
        
        # Calculate stats
        elapsed = current_time - self.start_time
        steps_done = max(1, iteration - self.start_iteration)  # Steps since start/resume
        steps_per_sec = steps_done / elapsed if elapsed > 0 else 0
        remaining_steps = self.total_iterations - iteration
        eta_seconds = remaining_steps / steps_per_sec if steps_per_sec > 0 else 0
        
        progress_pct = 100 * iteration / self.total_iterations
        
        # Average losses
        avg_g = sum(self.history.g_loss) / len(self.history.g_loss) if self.history.g_loss else 0
        avg_d = sum(self.history.d_loss) / len(self.history.d_loss) if self.history.d_loss else 0
        avg_recon = sum(self.history.recon_loss) / len(self.history.recon_loss) if self.history.recon_loss else 0
        
        # Loss trends (compare last 10 with previous 10)
        def get_trend(history: deque) -> str:
            if len(history) < 20:
                return "‚Üí"
            recent = sum(list(history)[-10:]) / 10
            previous = sum(list(history)[-20:-10]) / 10
            if recent < previous * 0.95:
                return "‚Üì"  # Improving
            elif recent > previous * 1.05:
                return "‚Üë"  # Getting worse
            return "‚Üí"  # Stable
        
        g_trend = get_trend(self.history.g_loss)
        recon_trend = get_trend(self.history.recon_loss)
        
        # Discriminator status
        disc_status = "üü¢ Active" if iteration >= self.disc_start_steps else f"‚è≥ Warmup ({iteration}/{self.disc_start_steps})"
        
        if self.console:
            # Build progress bar
            bar_width = 30
            filled = int(bar_width * progress_pct / 100)
            bar = "‚ñà" * filled + "‚ñë" * (bar_width - filled)
            
            # Color-coded losses
            g_color = "green" if avg_g < 5 else "yellow" if avg_g < 10 else "red"
            recon_color = "green" if avg_recon < 0.5 else "yellow" if avg_recon < 1 else "red"
            
            # Build output
            self.console.print(
                f"[cyan][{iteration:>8,}/{self.total_iterations:,}][/cyan] "
                f"[white]{bar}[/white] "
                f"[bold]{progress_pct:5.1f}%[/bold] ‚îÇ "
                f"[{g_color}]G:{avg_g:.3f}{g_trend}[/{g_color}] "
                f"D:{avg_d:.3f} "
                f"[{recon_color}]R:{avg_recon:.4f}{recon_trend}[/{recon_color}] ‚îÇ "
                f"[dim]{steps_per_sec:.1f} it/s[/dim] ‚îÇ "
                f"ETA: {self._format_time(eta_seconds)} ‚îÇ "
                f"{disc_status}"
            )
        else:
            # Fallback plain output
            print(
                f"[{iteration:>8,}/{self.total_iterations:,}] "
                f"{progress_pct:5.1f}% | "
                f"G:{avg_g:.3f} D:{avg_d:.3f} R:{avg_recon:.4f} | "
                f"{steps_per_sec:.1f} it/s | "
                f"ETA: {self._format_time(eta_seconds)}"
            )
    
    def log_validation(self, iteration: int, metrics: Dict[str, float]):
        """Log validation results."""
        if not self.is_main:
            return
        
        if self.console:
            self.console.print()
            table = Table(title=f"üîç Validation @ Step {iteration:,}", box=box.SIMPLE)
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="green")
            
            for key, value in metrics.items():
                table.add_row(key, f"{value:.6f}")
            
            self.console.print(table)
            self.console.print()
        else:
            print(f"\n=== Validation @ {iteration} ===")
            for key, value in metrics.items():
                print(f"  {key}: {value:.6f}")
            print()
    
    def log_checkpoint(self, iteration: int, path: str):
        """Log checkpoint save."""
        if not self.is_main:
            return
        
        if self.console:
            self.console.print(
                f"[bold green]üíæ Checkpoint saved:[/bold green] "
                f"[cyan]{path}[/cyan] "
                f"[dim](step {iteration:,})[/dim]"
            )
        else:
            print(f"üíæ Saved checkpoint: {path}")
    
    def log_resume(self, iteration: int, path: str):
        """Log checkpoint resume."""
        if not self.is_main:
            return
        
        # CRITICAL: Record start iteration for accurate it/s calculation
        self.start_iteration = iteration
        self.start_time = time.time()  # Reset timer on resume
        
        if self.console:
            self.console.print(
                f"[bold yellow]üìÇ Resumed from:[/bold yellow] "
                f"[cyan]{path}[/cyan] "
                f"[dim](step {iteration:,})[/dim]"
            )
        else:
            print(f"üìÇ Resumed from: {path} (step {iteration})")
    
    def log_gpu_status(self):
        """Log GPU memory usage."""
        if not self.is_main:
            return
        
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
                 '--format=csv,noheader,nounits'],
                capture_output=True, text=True
            )
            
            if result.returncode == 0 and self.console:
                self.console.print("\n[bold]GPU Status:[/bold]")
                for line in result.stdout.strip().split('\n'):
                    parts = line.split(',')
                    if len(parts) >= 4:
                        idx, mem_used, mem_total, util = [p.strip() for p in parts]
                        pct = int(100 * int(mem_used) / int(mem_total))
                        bar = "‚ñà" * (pct // 5) + "‚ñë" * (20 - pct // 5)
                        self.console.print(
                            f"  GPU {idx}: [{bar}] {mem_used}/{mem_total}MB ({util}%)"
                        )
                self.console.print()
        except Exception:
            pass  # Silently fail if nvidia-smi not available
    
    def log_training_complete(self, final_iteration: int):
        """Log training completion."""
        if not self.is_main:
            return
        
        total_time = time.time() - self.start_time
        
        if self.console:
            self.console.print()
            panel = Panel(
                f"[bold green]‚úÖ Training Complete![/bold green]\n\n"
                f"[cyan]Final iteration:[/cyan] {final_iteration:,}\n"
                f"[cyan]Total time:[/cyan] {self._format_time(total_time)}\n"
                f"[cyan]Avg speed:[/cyan] {final_iteration / total_time:.1f} it/s",
                title="üéâ Success",
                border_style="green"
            )
            self.console.print(panel)
        else:
            print(f"\n{'='*60}")
            print(f"‚úÖ Training Complete!")
            print(f"   Final iteration: {final_iteration:,}")
            print(f"   Total time: {self._format_time(total_time)}")
            print(f"{'='*60}\n")
    
    def _format_time(self, seconds: float) -> str:
        """Format seconds into human readable time."""
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            return f"{seconds/60:.0f}m"
        elif seconds < 86400:
            hours = int(seconds // 3600)
            mins = int((seconds % 3600) // 60)
            return f"{hours}h {mins}m"
        else:
            days = int(seconds // 86400)
            hours = int((seconds % 86400) // 3600)
            return f"{days}d {hours}h"


# Simple progress bar fallback
class SimpleProgressBar:
    """Simple progress bar when rich is not available."""
    
    def __init__(self, total: int, initial: int = 0, desc: str = ""):
        self.total = total
        self.current = initial
        self.desc = desc
        self.start_time = time.time()
    
    def update(self, n: int = 1):
        self.current += n
    
    def set_postfix(self, metrics: Dict[str, str]):
        elapsed = time.time() - self.start_time
        rate = self.current / elapsed if elapsed > 0 else 0
        pct = 100 * self.current / self.total
        
        metrics_str = " | ".join(f"{k}={v}" for k, v in metrics.items())
        print(f"\r[{self.current}/{self.total}] {pct:.1f}% | {rate:.1f} it/s | {metrics_str}", end="")
    
    def close(self):
        print()



==================================================
–§–ê–ô–õ: train_autoencoder.py
–†–û–ó–ú–Ü–†: 31.03 KB
==================================================

"""
Training Script –¥–ª—è Speech Autoencoder

–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Vocos-based autoencoder –¥–ª—è –∫–æ–¥—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä.

–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:
- Optimizer: AdamW, lr=2e-4
- Total iterations: 1.5M
- Batch size: 16
- Loss: Œª_recon=45, Œª_adv=1, Œª_fm=0.1
- AMP: bfloat16

Hardware:
- –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ: 4√ó RTX 4090/5090 (24GB each)
- –ú—ñ–Ω—ñ–º—É–º: 1√ó RTX 3090 –∑ gradient accumulation

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Vocos, HiFi-GAN, Supertonic v2 paper
"""

import os
import sys
import argparse
import time
import gc
import psutil
from pathlib import Path
from typing import Optional, Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import autocast  # Updated API (torch 2.0+)
from torch.cuda.amp import GradScaler
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist

from omegaconf import OmegaConf
import wandb

# RAM cleanup threshold (in GB)
RAM_CLEANUP_THRESHOLD_GB = 300

def check_and_cleanup_ram(force: bool = False) -> bool:
    """Check RAM usage and cleanup if above threshold.
    
    Returns True if cleanup was performed.
    """
    mem = psutil.virtual_memory()
    used_gb = mem.used / (1024 ** 3)
    
    if force or used_gb > RAM_CLEANUP_THRESHOLD_GB:
        gc.collect()
        return True
    return False

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.speech_autoencoder import (
    SpeechAutoencoder,
    LatentEncoder,
    LatentDecoder,
    MultiPeriodDiscriminator,
    MultiResolutionDiscriminator
)
from supertonic.losses.autoencoder_loss import AutoencoderLoss
from supertonic.data.dataset import AutoencoderDataset
from supertonic.data.collate import autoencoder_collate_fn
from supertonic.data.preprocessing import AudioProcessor
from supertonic.utils.training_logger import TrainingLogger


def setup_distributed():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î distributed training."""
    if "RANK" in os.environ:
        rank = int(os.environ["RANK"])
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        torch.cuda.set_device(local_rank)
        
        return rank, local_rank, world_size
    else:
        return 0, 0, 1


def cleanup_distributed():
    """Cleanup distributed training."""
    if dist.is_initialized():
        dist.destroy_process_group()


def save_checkpoint(
    path: Path,
    iteration: int,
    encoder: nn.Module,
    decoder: nn.Module,
    mpd: nn.Module,
    mrd: nn.Module,
    optimizer_g: torch.optim.Optimizer,
    optimizer_d: torch.optim.Optimizer,
    scaler: GradScaler,
    config: Dict
):
    """–ó–±–µ—Ä—ñ–≥–∞—î checkpoint."""
    checkpoint = {
        "iteration": iteration,
        "encoder": encoder.state_dict(),
        "decoder": decoder.state_dict(),
        "mpd": mpd.state_dict(),
        "mrd": mrd.state_dict(),
        "optimizer_g": optimizer_g.state_dict(),
        "optimizer_d": optimizer_d.state_dict(),
        "scaler": scaler.state_dict(),
        "config": config
    }
    torch.save(checkpoint, path)
    print(f"Saved checkpoint at iteration {iteration}")


def load_checkpoint(
    path: Path,
    encoder: nn.Module,
    decoder: nn.Module,
    mpd: nn.Module,
    mrd: nn.Module,
    optimizer_g: Optional[torch.optim.Optimizer] = None,
    optimizer_d: Optional[torch.optim.Optimizer] = None,
    scaler: Optional[GradScaler] = None,
    partial_resume: bool = False
) -> int:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î checkpoint.
    
    Args:
        partial_resume: If True, loads only matching weights (useful when 
                       architecture changed, e.g., WaveNeXt ‚Üí HiFi-GAN).
                       Resets iteration to 0 and skips optimizer loading.
    """
    checkpoint = torch.load(path, map_location="cpu")
    
    def adapt_state_dict(state_dict, model):
        """
        Adapt state_dict keys to match model structure.
        Handles both DDP‚ÜíDDP, DDP‚Üísingle, single‚ÜíDDP, single‚Üísingle.
        """
        # Check if model expects 'module.' prefix (DDP)
        model_keys = list(model.state_dict().keys())
        model_expects_module = len(model_keys) > 0 and model_keys[0].startswith("module.")
        
        # Check if checkpoint has 'module.' prefix
        ckpt_keys = list(state_dict.keys())
        ckpt_has_module = len(ckpt_keys) > 0 and ckpt_keys[0].startswith("module.")
        
        new_dict = {}
        for k, v in state_dict.items():
            if model_expects_module and not ckpt_has_module:
                # single GPU checkpoint ‚Üí DDP model: add prefix
                new_key = "module." + k
            elif not model_expects_module and ckpt_has_module:
                # DDP checkpoint ‚Üí single GPU model: remove prefix
                new_key = k[7:] if k.startswith("module.") else k
            else:
                # Same format
                new_key = k
            new_dict[new_key] = v
        return new_dict
    
    # Adapt state dicts to match current model structure (handles DDP ‚Üî single GPU)
    checkpoint["encoder"] = adapt_state_dict(checkpoint["encoder"], encoder)
    checkpoint["decoder"] = adapt_state_dict(checkpoint["decoder"], decoder)
    checkpoint["mpd"] = adapt_state_dict(checkpoint["mpd"], mpd)
    checkpoint["mrd"] = adapt_state_dict(checkpoint["mrd"], mrd)
    
    if partial_resume:
        # ========== PARTIAL RESUME (Architecture Change) ==========
        # Load Encoder fully (it's compatible)
        encoder.load_state_dict(checkpoint["encoder"])
        print("‚úÖ Encoder weights loaded fully!")
        
        # Load Decoder partially (filter mismatched layers)
        decoder_dict = decoder.state_dict()
        pretrained_decoder = checkpoint["decoder"]
        
        matched_layers = {}
        mismatched_layers = []
        
        for k, v in pretrained_decoder.items():
            if k in decoder_dict:
                if v.shape == decoder_dict[k].shape:
                    matched_layers[k] = v
                else:
                    mismatched_layers.append(f"{k}: {v.shape} ‚Üí {decoder_dict[k].shape}")
            else:
                mismatched_layers.append(f"{k}: not found in new model")
        
        decoder_dict.update(matched_layers)
        decoder.load_state_dict(decoder_dict)
        
        print(f"‚ö†Ô∏è Decoder: {len(matched_layers)}/{len(pretrained_decoder)} layers loaded")
        if mismatched_layers:
            print(f"   Skipped layers (architecture changed):")
            for layer in mismatched_layers[:10]:  # Show first 10
                print(f"     - {layer}")
            if len(mismatched_layers) > 10:
                print(f"     ... and {len(mismatched_layers) - 10} more")
        
        # Load discriminators partially (they may have different fft_sizes)
        def load_partial_state_dict(model, state_dict, name):
            model_dict = model.state_dict()
            matched = {}
            skipped = []
            for k, v in state_dict.items():
                if k in model_dict:
                    if v.shape == model_dict[k].shape:
                        matched[k] = v
                    else:
                        skipped.append(f"{k}: {v.shape} ‚Üí {model_dict[k].shape}")
                else:
                    skipped.append(f"{k}: not found")
            model_dict.update(matched)
            model.load_state_dict(model_dict)
            if skipped:
                print(f"‚ö†Ô∏è {name}: {len(matched)}/{len(state_dict)} layers loaded")
            else:
                print(f"‚úÖ {name}: fully loaded")
            return len(skipped)
        
        load_partial_state_dict(mpd, checkpoint["mpd"], "MPD")
        load_partial_state_dict(mrd, checkpoint["mrd"], "MRD")
        
        # Skip optimizer loading - parameters changed!
        print("‚ö†Ô∏è Optimizer states NOT loaded (new parameters)")
        print("‚ö†Ô∏è Starting from iteration 0")
        
        return 0  # Reset iteration for fine-tuning
    
    else:
        # ========== FULL RESUME (Same Architecture) ==========
        encoder.load_state_dict(checkpoint["encoder"])
        decoder.load_state_dict(checkpoint["decoder"])
        mpd.load_state_dict(checkpoint["mpd"])
        mrd.load_state_dict(checkpoint["mrd"])
        
        if optimizer_g is not None:
            optimizer_g.load_state_dict(checkpoint["optimizer_g"])
        if optimizer_d is not None:
            optimizer_d.load_state_dict(checkpoint["optimizer_d"])
        if scaler is not None:
            scaler.load_state_dict(checkpoint["scaler"])
        
        return checkpoint["iteration"]
    
    return checkpoint["iteration"]


def train_step(
    batch: Dict[str, torch.Tensor],
    encoder: nn.Module,
    decoder: nn.Module,
    mpd: nn.Module,
    mrd: nn.Module,
    optimizer_g: torch.optim.Optimizer,
    optimizer_d: torch.optim.Optimizer,
    loss_fn: AutoencoderLoss,
    scaler: GradScaler,
    device: torch.device,
    use_amp: bool = True,
    iteration: int = 0,
    disc_start_steps: int = 0,
    disc_crop_length: int = 4189  # Paper: 0.19s at 22050Hz
) -> Dict[str, float]:
    """
    –û–¥–∏–Ω training step (SupertonicTTS paper style).
    
    Args:
        iteration: Current training iteration
        disc_start_steps: Steps before discriminator starts (warmup)
        disc_crop_length: Random crop length for discriminator (paper: 0.19s)
    
    Returns:
        Dict –∑ loss values
    """
    audio = batch["audio"].to(device)
    mel = batch["mel"].to(device)
    
    # Determine if discriminator should be active
    disc_active = iteration >= disc_start_steps
    
    # ==================== Discriminator Step ====================
    d_loss_val = 0.0
    mpd_real_features = []
    mrd_real_features = []
    disc_crop_start = None  # Save crop index for generator step
    
    if disc_active:
        optimizer_d.zero_grad(set_to_none=True)
        
        with autocast('cuda', enabled=use_amp, dtype=torch.bfloat16):
            # Encode ‚Üí Decode (no grad for D step)
            with torch.no_grad():
                latent = encoder(mel)
                generated_audio = decoder(latent)
            
            # Match lengths
            min_len = min(audio.size(-1), generated_audio.size(-1))
            audio_trim = audio[..., :min_len]
            generated_trim = generated_audio[..., :min_len].detach()
            
            # Paper: "we randomly cropped segments of real and generated speech to 0.19s"
            # Random crop for discriminator training - save index for generator step!
            if disc_crop_length > 0 and min_len > disc_crop_length:
                max_start = min_len - disc_crop_length
                disc_crop_start = torch.randint(0, max_start, (1,)).item()
                audio_trim = audio_trim[..., disc_crop_start:disc_crop_start + disc_crop_length]
                generated_trim = generated_trim[..., disc_crop_start:disc_crop_start + disc_crop_length]
            
            # Discriminator forward
            mpd_real_outputs, mpd_real_features = mpd(audio_trim)
            mpd_fake_outputs, _ = mpd(generated_trim)
            
            mrd_real_outputs, mrd_real_features = mrd(audio_trim)
            mrd_fake_outputs, _ = mrd(generated_trim)
            
            # Combine outputs
            real_outputs = mpd_real_outputs + mrd_real_outputs
            fake_outputs = mpd_fake_outputs + mrd_fake_outputs
            
            # Discriminator loss
            d_losses = loss_fn.discriminator_loss(real_outputs, fake_outputs)
            d_loss = d_losses["total"]
        
        scaler.scale(d_loss).backward()
        scaler.step(optimizer_d)
        d_loss_val = d_losses["total"].item()
        
        # Clear D-step intermediates
        del mpd_fake_outputs, mrd_fake_outputs, fake_outputs, d_loss
    
    # ==================== Generator Step ====================
    optimizer_g.zero_grad(set_to_none=True)
    
    with autocast('cuda', enabled=use_amp, dtype=torch.bfloat16):
        # Encode ‚Üí Decode (with gradients this time)
        latent = encoder(mel)
        generated_audio = decoder(latent)
        
        # Match lengths
        min_len = min(audio.size(-1), generated_audio.size(-1))
        audio_trim = audio[..., :min_len]
        generated_trim = generated_audio[..., :min_len]
        
        # Discriminator forward only if disc is active
        fake_outputs = []
        fake_features = []
        real_features_detached = []
        
        if disc_active:
            # Apply same crop as discriminator step for feature matching!
            if disc_crop_start is not None and disc_crop_length > 0:
                generated_disc = generated_trim[..., disc_crop_start:disc_crop_start + disc_crop_length]
            else:
                generated_disc = generated_trim
            
            mpd_fake_outputs, mpd_fake_features = mpd(generated_disc)
            mrd_fake_outputs, mrd_fake_features = mrd(generated_disc)
            
            fake_outputs = mpd_fake_outputs + mrd_fake_outputs
            fake_features = mpd_fake_features + mrd_fake_features
            
            real_features_detached = [
                [f.detach() for f in feat_list] 
                for feat_list in (mpd_real_features + mrd_real_features)
            ]
        
        # Generator loss (use_adv=False during warmup)
        g_losses = loss_fn.generator_loss(
            real_audio=audio_trim,
            generated_audio=generated_trim,
            disc_fake_outputs=fake_outputs,
            real_features=real_features_detached,
            fake_features=fake_features,
            use_adv=disc_active  # NEW: disable adversarial during warmup
        )
        g_loss = g_losses["total"]
    
    scaler.scale(g_loss).backward()
    scaler.step(optimizer_g)
    scaler.update()
    
    # Clean up ALL intermediates to prevent fragmentation
    del audio, mel, latent, generated_audio, audio_trim, generated_trim
    if disc_active:
        del mpd_real_features, mrd_real_features, real_features_detached
        del fake_outputs, fake_features
        if 'mpd_fake_outputs' in dir():
            del mpd_fake_outputs, mrd_fake_outputs, mpd_fake_features, mrd_fake_features
    
    return {
        "d_loss": d_loss_val,  # Fixed: use d_loss_val (0 during warmup)
        "g_loss": g_losses["total"].item(),
        "recon_loss": g_losses["reconstruction"].item(),
        "adv_loss": g_losses["adversarial"].item(),
        "fm_loss": g_losses["feature_matching"].item()
    }


@torch.no_grad()
def validate(
    dataloader: DataLoader,
    encoder: nn.Module,
    decoder: nn.Module,
    loss_fn: AutoencoderLoss,
    device: torch.device,
    max_samples: int = 100
) -> Dict[str, float]:
    """Validation loop."""
    encoder.eval()
    decoder.eval()
    
    total_recon_loss = 0.0
    num_samples = 0
    
    for batch in dataloader:
        if num_samples >= max_samples:
            break
        
        audio = batch["audio"].to(device)
        mel = batch["mel"].to(device)
        
        latent = encoder(mel)
        generated = decoder(latent)
        
        min_len = min(audio.size(-1), generated.size(-1))
        recon_loss = F.l1_loss(generated[..., :min_len], audio[..., :min_len])
        
        total_recon_loss += recon_loss.item() * audio.size(0)
        num_samples += audio.size(0)
    
    encoder.train()
    decoder.train()
    
    return {
        "val_recon_loss": total_recon_loss / max(num_samples, 1)
    }


def main(args):
    """Main training function."""
    # Setup distributed
    rank, local_rank, world_size = setup_distributed()
    is_main = rank == 0
    device = torch.device(f"cuda:{local_rank}")
    
    # Load config
    config = OmegaConf.load(args.config)
    
    # Override with CLI args
    if args.batch_size:
        config.train_autoencoder.batch_size = args.batch_size
    if args.lr:
        config.train_autoencoder.learning_rate = args.lr
    
    # Logging
    if is_main and not args.no_wandb:
        wandb.init(
            project=config.logging.project,
            name=f"autoencoder_{time.strftime('%Y%m%d_%H%M%S')}",
            config=OmegaConf.to_container(config)
        )
    
    # Create output directories
    checkpoint_dir = Path(config.output.checkpoint_dir) / "autoencoder"
    sample_dir = Path(config.output.sample_dir) / "autoencoder"
    
    if is_main:
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        sample_dir.mkdir(parents=True, exist_ok=True)
    
    # Audio processor
    audio_processor = AudioProcessor(
        sample_rate=config.audio.sample_rate,
        n_fft=config.audio.n_fft,
        hop_length=config.audio.hop_length,
        n_mels=config.audio.n_mels
    )
    
    # Get segment_length from config (CRITICAL for OOM prevention!)
    segment_length = config.training.get("segment_length", 176400)  # default 4 sec at 44.1kHz
    if is_main:
        print(f"Using segment_length: {segment_length} samples ({segment_length/config.audio.sample_rate:.1f} sec)")
    
    # Get data directory (CLI override or config)
    data_dir = args.data_dir if args.data_dir else config.paths.get("data_dir", "data")
    if is_main:
        print(f"Data directory: {data_dir}")
    
    # Override output directory if provided
    if args.output_dir:
        config.output.checkpoint_dir = f"{args.output_dir}/checkpoints"
        config.output.sample_dir = f"{args.output_dir}/samples"
        if is_main:
            print(f"Output directory: {args.output_dir}")
    
    # Dataset with segment cropping and RAM caching
    cache_audio = config.train_autoencoder.get("cache_audio", False)
    if is_main:
        print(f"Audio caching in RAM: {cache_audio}")
    
    train_dataset = AutoencoderDataset(
        manifest_path=config.data.train_manifest,
        audio_processor=audio_processor,
        max_duration=config.data.max_audio_duration,
        min_duration=config.data.min_audio_duration,
        return_mel=True,
        segment_length=segment_length,  # Random crop for memory efficiency
        data_dir=data_dir,
        cache_audio=cache_audio  # Cache all audio in RAM!
    )
    
    val_dataset = AutoencoderDataset(
        manifest_path=config.data.val_manifest,
        audio_processor=audio_processor,
        max_duration=config.data.max_audio_duration,
        min_duration=config.data.min_audio_duration,
        return_mel=True,
        segment_length=segment_length,
        data_dir=data_dir,
        cache_audio=cache_audio  # Cache all audio in RAM!
    )
    
    # DataLoader
    if world_size > 1:
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank
        )
        shuffle = False
    else:
        train_sampler = None
        shuffle = True
    
    # Get num_workers from config (try train_autoencoder first, then training.autoencoder)
    num_workers = config.train_autoencoder.get("num_workers", 
                    config.training.autoencoder.get("num_workers", 4))
    prefetch = config.train_autoencoder.get("prefetch_factor", 4)
    if is_main:
        print(f"DataLoader: {num_workers} workers, prefetch={prefetch}")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.train_autoencoder.batch_size,
        shuffle=shuffle,
        sampler=train_sampler,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=autoencoder_collate_fn,
        drop_last=True,
        persistent_workers=num_workers > 0,  # Keep workers alive (use few workers to limit leak)
        prefetch_factor=prefetch if num_workers > 0 else None
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.train_autoencoder.batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=autoencoder_collate_fn,
        persistent_workers=num_workers > 0,  # Keep workers alive
        prefetch_factor=prefetch if num_workers > 0 else None
    )
    
    # Models
    gradient_checkpointing = config.optimization.get("gradient_checkpointing", False)
    if is_main:
        print(f"Gradient checkpointing: {gradient_checkpointing}")
    
    # Get autoencoder config (support both config.autoencoder and config.model.autoencoder)
    ae_config = config.get("autoencoder", config.get("model", {}).get("autoencoder", {}))
    
    encoder = LatentEncoder(
        input_dim=ae_config.encoder.input_dim,
        hidden_dim=ae_config.encoder.hidden_dim,
        output_dim=ae_config.encoder.output_dim,
        num_blocks=ae_config.encoder.num_blocks,
        kernel_size=ae_config.encoder.kernel_size,
        gradient_checkpointing=gradient_checkpointing
    ).to(device)
    
    decoder = LatentDecoder(
        input_dim=ae_config.decoder.input_dim,
        hidden_dim=ae_config.decoder.hidden_dim,
        num_blocks=ae_config.decoder.num_blocks,
        kernel_size=ae_config.decoder.kernel_size,
        dilations=ae_config.decoder.dilations,
        n_fft=config.audio.n_fft,              # CRITICAL: use config values!
        hop_length=config.audio.hop_length,    # CRITICAL: use config values!
        causal=ae_config.decoder.causal,
        gradient_checkpointing=gradient_checkpointing,
        # HiFi-GAN parameters (eliminates metallic sound!)
        use_hifigan=ae_config.decoder.get("use_hifigan", True),
        upsample_rates=ae_config.decoder.get("upsample_rates", [8, 8, 2, 2]),
        upsample_kernel_sizes=ae_config.decoder.get("upsample_kernel_sizes", [16, 16, 4, 4]),
        upsample_initial_channel=ae_config.decoder.get("upsample_initial_channel", 512),
        resblock_kernel_sizes=ae_config.decoder.get("resblock_kernel_sizes", [3, 7, 11]),
        resblock_dilation_sizes=ae_config.decoder.get("resblock_dilation_sizes", [[1, 3, 5], [1, 3, 5], [1, 3, 5]])
    ).to(device)
    
    if is_main:
        use_hifigan = ae_config.decoder.get("use_hifigan", True)
        print(f"=" * 60)
        print(f"MODEL CONFIGURATION CHECK:")
        print(f"  Encoder input_dim (n_mels): {ae_config.encoder.input_dim}")
        print(f"  Decoder n_fft: {config.audio.n_fft}")
        print(f"  Decoder hop_length: {config.audio.hop_length}")
        print(f"  Decoder head: {'HiFi-GAN üé∏ (clean audio)' if use_hifigan else 'WaveNeXt ‚ö†Ô∏è (may have metallic sound)'}")
        if use_hifigan:
            upsample_rates = ae_config.decoder.get("upsample_rates", [8, 8, 2, 2])
            print(f"  HiFi-GAN upsample_rates: {upsample_rates} (product={eval('*'.join(map(str, upsample_rates)))})")
        print(f"  Audio sample_rate: {config.audio.sample_rate}")
        print(f"  MRD fft_sizes: {list(ae_config.discriminator.mrd_fft_sizes)}")
        print(f"=" * 60)
    
    mpd = MultiPeriodDiscriminator(
        periods=ae_config.discriminator.mpd_periods
    ).to(device)
    
    mrd = MultiResolutionDiscriminator(
        fft_sizes=ae_config.discriminator.mrd_fft_sizes
    ).to(device)
    
    # DDP wrapping
    if world_size > 1:
        encoder = DDP(encoder, device_ids=[local_rank])
        decoder = DDP(decoder, device_ids=[local_rank])
        mpd = DDP(mpd, device_ids=[local_rank])
        mrd = DDP(mrd, device_ids=[local_rank])
    
    # Loss - simplified for WaveNeXt head (no SC/LogMag/Waveform needed)
    loss_weights = config.train_autoencoder.loss_weights
    loss_fn = AutoencoderLoss(
        lambda_mel=loss_weights.reconstruction,
        lambda_adv=loss_weights.adversarial,
        lambda_fm=loss_weights.feature_matching,
        fft_sizes=list(ae_config.discriminator.mrd_fft_sizes),
        sample_rate=config.audio.sample_rate,
        n_mels=config.audio.n_mels
    )
    
    if is_main:
        print(f"Loss config: Œª_recon={loss_weights.reconstruction}, Œª_adv={loss_weights.adversarial}, Œª_fm={loss_weights.feature_matching}")
        disc_crop = config.train_autoencoder.get("disc_crop_length", 4189)
        disc_crop_sec = disc_crop / config.audio.sample_rate
        print(f"Disc crop: {disc_crop} samples ({disc_crop_sec:.3f}s) - Paper: 0.19s")
    
    # Optimizers
    optimizer_g = torch.optim.AdamW(
        list(encoder.parameters()) + list(decoder.parameters()),
        lr=config.train_autoencoder.learning_rate,
        betas=tuple(config.train_autoencoder.optimizer.betas),
        weight_decay=config.train_autoencoder.optimizer.weight_decay
    )
    
    optimizer_d = torch.optim.AdamW(
        list(mpd.parameters()) + list(mrd.parameters()),
        lr=config.train_autoencoder.learning_rate,
        betas=tuple(config.train_autoencoder.optimizer.betas),
        weight_decay=config.train_autoencoder.optimizer.weight_decay
    )
    
    # AMP scaler
    scaler = GradScaler(enabled=config.train_autoencoder.amp.enabled)
    
    # Resume from checkpoint
    start_iteration = 0
    if args.resume:
        start_iteration = load_checkpoint(
            Path(args.resume),
            encoder, decoder, mpd, mrd,
            optimizer_g, optimizer_d, scaler,
            partial_resume=args.partial_resume  # Use partial resume for architecture changes
        )
    
    # Training loop
    total_iterations = config.train_autoencoder.total_iterations
    checkpoint_interval = config.train_autoencoder.checkpoint_interval
    validation_interval = config.train_autoencoder.validation_interval
    log_interval = config.train_autoencoder.get("log_interval", config.logging.log_interval)
    
    # Get discriminator warmup steps from config
    disc_start_steps = config.train_autoencoder.get("discriminator_start_steps", 2000)
    
    # Paper: "we randomly cropped segments of real and generated speech to 0.19s"
    # 0.19s at 22050Hz = 4189 samples
    disc_crop_length = config.train_autoencoder.get("disc_crop_length", 4189)
    
    # Initialize beautiful logger
    logger = TrainingLogger(
        total_iterations=total_iterations,
        log_interval=log_interval,
        checkpoint_interval=checkpoint_interval,
        validation_interval=validation_interval,
        disc_start_steps=disc_start_steps,
        rank=rank,
        world_size=world_size
    )
    
    # Print config
    logger.print_config({
        'batch_size': config.train_autoencoder.batch_size,
        'learning_rate': config.train_autoencoder.learning_rate,
        'loss_weights': OmegaConf.to_container(config.train_autoencoder.loss_weights)
    })
    
    # Log resume
    if args.resume:
        logger.log_resume(start_iteration, args.resume)
    
    # Log GPU status at start
    logger.log_gpu_status()
    
    iteration = start_iteration
    epoch = 0
    
    while iteration < total_iterations:
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        for batch in train_loader:
            if iteration >= total_iterations:
                break
            
            # Training step (with warmup support)
            losses = train_step(
                batch, encoder, decoder, mpd, mrd,
                optimizer_g, optimizer_d, loss_fn, scaler,
                device, 
                use_amp=config.train_autoencoder.amp.enabled,
                iteration=iteration,
                disc_start_steps=disc_start_steps,
                disc_crop_length=disc_crop_length
            )
            
            iteration += 1
            
            # Logging with beautiful logger
            logger.log_step(iteration, losses)
            
            # Wandb logging
            if is_main and iteration % log_interval == 0 and not args.no_wandb:
                wandb.log(losses, step=iteration)
            
            # Validation
            if is_main and iteration % validation_interval == 0:
                val_metrics = validate(val_loader, encoder, decoder, loss_fn, device)
                
                logger.log_validation(iteration, val_metrics)
                
                if not args.no_wandb:
                    wandb.log(val_metrics, step=iteration)
            
            # Checkpoint
            if is_main and iteration % checkpoint_interval == 0:
                ckpt_path = checkpoint_dir / f"checkpoint_{iteration}.pt"
                save_checkpoint(
                    ckpt_path,
                    iteration, encoder, decoder, mpd, mrd,
                    optimizer_g, optimizer_d, scaler,
                    OmegaConf.to_container(config)
                )
                logger.log_checkpoint(iteration, str(ckpt_path))
            
            # RAM cleanup - check every 100 steps, cleanup if > 300 GB
            if iteration % 100 == 0:
                if check_and_cleanup_ram():
                    torch.cuda.empty_cache()
                    if is_main and iteration % 500 == 0:
                        mem = psutil.virtual_memory()
                        print(f"üßπ RAM cleanup: {mem.used / (1024**3):.1f} GB used")
        
        epoch += 1
    
    # Final checkpoint
    if is_main:
        ckpt_path = checkpoint_dir / "checkpoint_final.pt"
        save_checkpoint(
            ckpt_path,
            iteration, encoder, decoder, mpd, mrd,
            optimizer_g, optimizer_d, scaler,
            OmegaConf.to_container(config)
        )
        logger.log_checkpoint(iteration, str(ckpt_path))
        logger.log_training_complete(iteration)
        
        if not args.no_wandb:
            wandb.finish()
    
    cleanup_distributed()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Speech Autoencoder")
    parser.add_argument("--config", type=str, default="config/22khz_optimal.yaml",
                        help="Path to config file")
    parser.add_argument("--resume", type=str, default=None,
                        help="Path to checkpoint to resume from")
    parser.add_argument("--partial-resume", action="store_true",
                        help="Load only matching weights (use after architecture change, e.g., WaveNeXt ‚Üí HiFi-GAN)")
    parser.add_argument("--batch-size", type=int, default=None,
                        help="Override batch size")
    parser.add_argument("--lr", type=float, default=None,
                        help="Override learning rate")
    parser.add_argument("--no-wandb", action="store_true",
                        help="Disable wandb logging")
    parser.add_argument("--data_dir", "--data-dir", type=str, default=None,
                        help="Override data directory")
    parser.add_argument("--output_dir", "--output-dir", type=str, default=None,
                        help="Override output directory")
    
    args = parser.parse_args()
    main(args)



==================================================
–§–ê–ô–õ: train_duration_predictor.py
–†–û–ó–ú–Ü–†: 11 KB
==================================================

"""
Training Script –¥–ª—è Duration Predictor

–®–≤–∏–¥–∫–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è duration predictor (~3000 iterations).

–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:
- Utterance-level duration prediction (–Ω–µ per-phoneme!)
- L1 loss –Ω–∞ ground-truth duration
- –®–≤–∏–¥–∫–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: ~3000 iterations (–∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω)

–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:
- Optimizer: AdamW, lr=5e-4
- Iterations: 3000
- Batch size: 64

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper
"""

import os
import sys
import argparse
import time
from pathlib import Path
from typing import Optional, Dict

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast

from omegaconf import OmegaConf
from tqdm import tqdm
import wandb

sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.duration_predictor import DurationPredictor
from supertonic.models.speech_autoencoder import LatentEncoder
from supertonic.losses.duration_loss import DurationLoss
from supertonic.losses.flow_matching_loss import compress_latents
from supertonic.data.dataset import DurationDataset
from supertonic.data.collate import duration_collate_fn
from supertonic.data.preprocessing import AudioProcessor
from supertonic.data.tokenizer import CharacterTokenizer


def save_checkpoint(
    path: Path,
    iteration: int,
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    config: Dict
):
    """–ó–±–µ—Ä—ñ–≥–∞—î checkpoint."""
    checkpoint = {
        "iteration": iteration,
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "config": config
    }
    torch.save(checkpoint, path)
    print(f"Saved checkpoint at iteration {iteration}")


def load_checkpoint(
    path: Path,
    model: nn.Module,
    optimizer: Optional[torch.optim.Optimizer] = None
) -> int:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î checkpoint."""
    checkpoint = torch.load(path, map_location="cpu")
    model.load_state_dict(checkpoint["model"])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint["optimizer"])
    
    return checkpoint["iteration"]


def train_step(
    batch: Dict[str, torch.Tensor],
    model: nn.Module,
    latent_encoder: nn.Module,
    optimizer: torch.optim.Optimizer,
    loss_fn: DurationLoss,
    device: torch.device
) -> Dict[str, float]:
    """–û–¥–∏–Ω training step."""
    text_ids = batch["text_ids"].to(device)
    text_mask = batch["text_mask"].to(device)
    reference_mel = batch["reference_mel"].to(device)
    reference_mask = batch["reference_mask"].to(device)
    target_duration = batch["durations"].to(device)
    
    optimizer.zero_grad()
    
    # Encode reference to latent
    with torch.no_grad():
        ref_latent = latent_encoder(reference_mel)
        ref_compressed = compress_latents(ref_latent, compression_factor=6)
    
    # Predict duration
    predicted_duration = model(
        text_ids=text_ids,
        text_mask=text_mask,
        reference_latent=ref_compressed,
        reference_mask=reference_mask
    )
    
    # Loss
    losses = loss_fn(predicted_duration, target_duration)
    loss = losses["total"]
    
    loss.backward()
    optimizer.step()
    
    return {
        "loss": losses["total"].item(),
        "l1_loss": losses["l1"].item(),
        "percent_error": losses["percent_error"].item()
    }


@torch.no_grad()
def validate(
    dataloader: DataLoader,
    model: nn.Module,
    latent_encoder: nn.Module,
    loss_fn: DurationLoss,
    device: torch.device,
    max_samples: int = 200
) -> Dict[str, float]:
    """Validation loop."""
    model.eval()
    latent_encoder.eval()
    
    total_loss = 0.0
    total_percent_error = 0.0
    num_samples = 0
    
    for batch in dataloader:
        if num_samples >= max_samples:
            break
        
        text_ids = batch["text_ids"].to(device)
        text_mask = batch["text_mask"].to(device)
        reference_mel = batch["reference_mel"].to(device)
        reference_mask = batch["reference_mask"].to(device)
        target_duration = batch["durations"].to(device)
        
        ref_latent = latent_encoder(reference_mel)
        ref_compressed = compress_latents(ref_latent, compression_factor=6)
        
        predicted_duration = model(
            text_ids=text_ids,
            text_mask=text_mask,
            reference_latent=ref_compressed,
            reference_mask=reference_mask
        )
        
        losses = loss_fn(predicted_duration, target_duration)
        
        total_loss += losses["total"].item() * text_ids.size(0)
        total_percent_error += losses["percent_error"].item() * text_ids.size(0)
        num_samples += text_ids.size(0)
    
    model.train()
    
    return {
        "val_loss": total_loss / max(num_samples, 1),
        "val_percent_error": total_percent_error / max(num_samples, 1)
    }


def main(args):
    """Main training function."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load config
    config = OmegaConf.load(args.config)
    
    # Override
    if args.batch_size:
        config.train_duration.batch_size = args.batch_size
    if args.lr:
        config.train_duration.learning_rate = args.lr
    
    # Logging
    if not args.no_wandb:
        wandb.init(
            project=config.logging.project,
            name=f"duration_predictor_{time.strftime('%Y%m%d_%H%M%S')}",
            config=OmegaConf.to_container(config)
        )
    
    # Output directories
    checkpoint_dir = Path(config.output.checkpoint_dir) / "duration"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    # Audio processor & tokenizer
    audio_processor = AudioProcessor(
        sample_rate=config.audio.sample_rate,
        n_fft=config.audio.n_fft,
        hop_length=config.audio.hop_length,
        n_mels=config.audio.n_mels
    )
    
    tokenizer = CharacterTokenizer(languages=config.languages.supported)
    
    # Dataset
    train_dataset = DurationDataset(
        manifest_path=config.data.train_manifest,
        audio_processor=audio_processor,
        tokenizer=tokenizer,
        duration_unit="frames"
    )
    
    val_dataset = DurationDataset(
        manifest_path=config.data.val_manifest,
        audio_processor=audio_processor,
        tokenizer=tokenizer,
        duration_unit="frames"
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.train_duration.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        collate_fn=duration_collate_fn,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.train_duration.batch_size,
        shuffle=False,
        num_workers=4,
        collate_fn=duration_collate_fn
    )
    
    # Load pretrained latent encoder
    latent_encoder = LatentEncoder(
        input_dim=config.autoencoder.encoder.input_dim,
        hidden_dim=config.autoencoder.encoder.hidden_dim,
        output_dim=config.autoencoder.encoder.output_dim,
        num_blocks=config.autoencoder.encoder.num_blocks
    ).to(device)
    
    if args.autoencoder_checkpoint:
        ae_ckpt = torch.load(args.autoencoder_checkpoint, map_location=device)
        latent_encoder.load_state_dict(ae_ckpt["encoder"])
        print(f"Loaded latent encoder from {args.autoencoder_checkpoint}")
    
    latent_encoder.eval()
    for param in latent_encoder.parameters():
        param.requires_grad = False
    
    # Duration Predictor
    model = DurationPredictor(
        vocab_size=tokenizer.vocab_size,
        hidden_dim=config.duration_predictor.hidden_dim,
        num_convnext_blocks=config.duration_predictor.num_convnext_blocks,
        kernel_size=config.duration_predictor.kernel_size
    ).to(device)
    
    # Loss
    loss_fn = DurationLoss()
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.train_duration.learning_rate,
        betas=tuple(config.train_duration.optimizer.betas),
        weight_decay=config.train_duration.optimizer.weight_decay
    )
    
    # Resume
    start_iteration = 0
    if args.resume:
        start_iteration = load_checkpoint(Path(args.resume), model, optimizer)
        print(f"Resumed from iteration {start_iteration}")
    
    # Training loop
    total_iterations = config.train_duration.total_iterations
    checkpoint_interval = config.train_duration.checkpoint_interval
    log_interval = 50
    
    iteration = start_iteration
    epoch = 0
    
    pbar = tqdm(total=total_iterations, initial=start_iteration, desc="Training Duration")
    
    while iteration < total_iterations:
        for batch in train_loader:
            if iteration >= total_iterations:
                break
            
            losses = train_step(
                batch, model, latent_encoder, optimizer, loss_fn, device
            )
            
            iteration += 1
            
            # Logging
            if iteration % log_interval == 0:
                if not args.no_wandb:
                    wandb.log(losses, step=iteration)
                
                pbar.set_postfix({
                    "loss": f"{losses['loss']:.4f}",
                    "err%": f"{losses['percent_error']:.1f}"
                })
                pbar.update(log_interval)
            
            # Checkpoint
            if iteration % checkpoint_interval == 0:
                save_checkpoint(
                    checkpoint_dir / f"checkpoint_{iteration}.pt",
                    iteration, model, optimizer,
                    OmegaConf.to_container(config)
                )
                
                # Validation
                val_metrics = validate(
                    val_loader, model, latent_encoder, loss_fn, device
                )
                
                if not args.no_wandb:
                    wandb.log(val_metrics, step=iteration)
                
                print(f"\nValidation: {val_metrics}")
        
        epoch += 1
    
    # Final checkpoint
    save_checkpoint(
        checkpoint_dir / "checkpoint_final.pt",
        iteration, model, optimizer,
        OmegaConf.to_container(config)
    )
    
    pbar.close()
    
    if not args.no_wandb:
        wandb.finish()
    
    print("Training completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Duration Predictor")
    parser.add_argument("--config", type=str, default="config/default.yaml")
    parser.add_argument("--resume", type=str, default=None)
    parser.add_argument("--autoencoder-checkpoint", type=str, required=True)
    parser.add_argument("--batch-size", type=int, default=None)
    parser.add_argument("--lr", type=float, default=None)
    parser.add_argument("--no-wandb", action="store_true")
    
    args = parser.parse_args()
    main(args)



==================================================
–§–ê–ô–õ: train_text_to_latent.py
–†–û–ó–ú–Ü–†: 21.22 KB
==================================================

"""
Training Script –¥–ª—è Text-to-Latent Module

–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è flow-matching –º–æ–¥–µ–ª—ñ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –ª–∞—Ç–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É.

–ö–ª—é—á–æ–≤—ñ –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:
- Context-Sharing Batch Expansion (B=64, Ke=4)
- Conditional Flow Matching (CFM) –∑ œÉ_min=1e-8
- Classifier-Free Guidance (p_uncond=0.05)
- LARoPE (Œ≥=10) –¥–ª—è text-speech alignment
- Learning rate halving –∫–æ–∂–Ω—ñ 300k iterations

–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:
- Optimizer: AdamW, lr=5e-4
- Total iterations: 700k
- Effective batch size: 256 (64 √ó 4)

Hardware:
- –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ: 4√ó RTX 4090/5090
- –ú—ñ–Ω—ñ–º—É–º: 1√ó RTX 3090 –∑ B=16, Ke=4

–†–µ—Ñ–µ—Ä–µ–Ω—Å: Supertonic v2 paper (2509.11084), Matcha-TTS
"""

import os
import sys
import argparse
import time
from pathlib import Path
from typing import Optional, Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist

from omegaconf import OmegaConf
from tqdm import tqdm
import wandb

sys.path.insert(0, str(Path(__file__).parent.parent))

from supertonic.models.text_to_latent import (
    TextToLatent,
    ReferenceEncoder,
    TextEncoder,
    VectorFieldEstimator
)
from supertonic.models.speech_autoencoder import LatentEncoder
from supertonic.losses.flow_matching_loss import (
    FlowMatchingLoss,
    compress_latents,
    decompress_latents,
    ODESolver
)
from supertonic.data.dataset import TTSDataset, ContextSharingDataset
from supertonic.data.collate import tts_collate_fn
from supertonic.data.preprocessing import AudioProcessor
from supertonic.data.tokenizer import CharacterTokenizer


def setup_distributed():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î distributed training."""
    if "RANK" in os.environ:
        rank = int(os.environ["RANK"])
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        torch.cuda.set_device(local_rank)
        
        return rank, local_rank, world_size
    else:
        return 0, 0, 1


def cleanup_distributed():
    if dist.is_initialized():
        dist.destroy_process_group()


def save_checkpoint(
    path: Path,
    iteration: int,
    text_to_latent: nn.Module,
    optimizer: torch.optim.Optimizer,
    scaler: GradScaler,
    config: Dict
):
    """–ó–±–µ—Ä—ñ–≥–∞—î checkpoint."""
    checkpoint = {
        "iteration": iteration,
        "model": text_to_latent.state_dict(),
        "optimizer": optimizer.state_dict(),
        "scaler": scaler.state_dict(),
        "config": config
    }
    torch.save(checkpoint, path)
    print(f"Saved checkpoint at iteration {iteration}")


def load_checkpoint(
    path: Path,
    text_to_latent: nn.Module,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scaler: Optional[GradScaler] = None
) -> int:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î checkpoint."""
    checkpoint = torch.load(path, map_location="cpu")
    
    text_to_latent.load_state_dict(checkpoint["model"])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint["optimizer"])
    if scaler is not None:
        scaler.load_state_dict(checkpoint["scaler"])
    
    return checkpoint["iteration"]


def context_sharing_batch_expansion(
    batch: Dict[str, torch.Tensor],
    expansion_factor: int = 4
) -> Dict[str, torch.Tensor]:
    """
    Context-Sharing Batch Expansion.
    
    –ö–ª—é—á–æ–≤–∞ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è Supertonic v2:
    - –î–ª—è –∫–æ–∂–Ω–æ–≥–æ sample: –æ–¥–∏–Ω —Ä–∞–∑ –∫–æ–¥—É—î–º–æ text/reference
    - –ü–æ—Ç—ñ–º –∑–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ Ke —Ä—ñ–∑–Ω–∏—Ö noise samples –∑ —Ä—ñ–∑–Ω–∏–º–∏ timesteps
    
    –¶–µ –∑–º–µ–Ω—à—É—î memory usage —Ç–∞ —Å—Ç–∞–±—ñ–ª—ñ–∑—É—î alignment learning.
    
    Args:
        batch: Original batch
        expansion_factor: Ke - number of expansions
        
    Returns:
        Expanded batch
    """
    expanded = {}
    
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            # Repeat each sample Ke times
            expanded[key] = value.repeat_interleave(expansion_factor, dim=0)
        else:
            expanded[key] = value
    
    # Add expansion indices for different noise/timesteps
    batch_size = batch["mel"].size(0)
    expansion_idx = torch.arange(expansion_factor).repeat(batch_size)
    expanded["expansion_idx"] = expansion_idx
    
    return expanded


def train_step(
    batch: Dict[str, torch.Tensor],
    text_to_latent: nn.Module,
    text_to_latent_raw: nn.Module,  # Unwrapped model for method access
    latent_encoder: nn.Module,
    optimizer: torch.optim.Optimizer,
    loss_fn: FlowMatchingLoss,
    scaler: GradScaler,
    device: torch.device,
    expansion_factor: int = 4,
    use_amp: bool = True
) -> Dict[str, float]:
    """
    –û–¥–∏–Ω training step –∑ context-sharing.
    """
    # Move to device
    mel = batch["mel"].to(device)
    text_ids = batch["text_ids"].to(device)
    text_mask = batch["text_mask"].to(device)
    reference_mel = batch["reference_mel"].to(device)
    reference_mask = batch["reference_mask"].to(device)
    lang_ids = batch.get("lang_ids", None)
    if lang_ids is not None:
        lang_ids = lang_ids.to(device)
    
    optimizer.zero_grad()
    
    with autocast(enabled=use_amp, dtype=torch.bfloat16):
        # 1. Encode target audio to latents (–∑ pretrained autoencoder)
        with torch.no_grad():
            latent_encoder.eval()
            target_latent = latent_encoder(mel)  # [B, 24, T]
            
            # Temporal compression: [B, 24, T] ‚Üí [B, 144, T/6]
            target_compressed = compress_latents(target_latent, compression_factor=6)
        
        # 2. Context-sharing batch expansion
        # Expand batch Ke times (same text/reference, different noise)
        batch_size = mel.size(0)
        expanded_size = batch_size * expansion_factor
        
        # Expand conditioning (shared across noise samples)
        text_ids_exp = text_ids.repeat_interleave(expansion_factor, dim=0)
        text_mask_exp = text_mask.repeat_interleave(expansion_factor, dim=0)
        reference_mel_exp = reference_mel.repeat_interleave(expansion_factor, dim=0)
        reference_mask_exp = reference_mask.repeat_interleave(expansion_factor, dim=0)
        target_exp = target_compressed.repeat_interleave(expansion_factor, dim=0)
        
        if lang_ids is not None:
            lang_ids_exp = lang_ids.repeat_interleave(expansion_factor, dim=0)
        else:
            lang_ids_exp = None
        
        # 3. Compress reference latents
        with torch.no_grad():
            ref_latent = latent_encoder(reference_mel_exp)
            ref_compressed = compress_latents(ref_latent, compression_factor=6)
        
        # 4. Encode reference FIRST (needed for text encoding)
        # Note: ref_mask is not used as reference encoder outputs fixed 50 vectors
        reference_encoding = text_to_latent_raw.encode_reference(
            ref_compressed,
            ref_mask=None  # Skip mask - encoder produces fixed output anyway
        )
        
        # 5. Encode text WITH reference conditioning
        text_encoding = text_to_latent_raw.encode_text(
            text_ids_exp,
            reference_encoding,  # ref_vectors from reference encoder
            text_mask_exp,
            lang_id=lang_ids_exp
        )
        
        # 6. Flow-matching loss
        losses = loss_fn(
            model=text_to_latent_raw.vector_field,
            z1=target_exp,
            text_encoding=text_encoding,
            reference_encoding=reference_encoding,
            text_mask=text_mask_exp
        )
        
        loss = losses["total"]
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    
    return {
        "loss": losses["total"].item(),
        "flow_matching_loss": losses["flow_matching"].item(),
        "velocity_error": losses["mean_velocity_error"].item()
    }


def adjust_learning_rate(
    optimizer: torch.optim.Optimizer,
    iteration: int,
    base_lr: float,
    halve_interval: int
):
    """
    Learning rate halving –∫–æ–∂–Ω—ñ halve_interval iterations.
    """
    num_halvings = iteration // halve_interval
    lr = base_lr * (0.5 ** num_halvings)
    
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr
    
    return lr


@torch.no_grad()
def validate(
    dataloader: DataLoader,
    text_to_latent: nn.Module,
    text_to_latent_raw: nn.Module,  # Unwrapped model for method access
    latent_encoder: nn.Module,
    loss_fn: FlowMatchingLoss,
    device: torch.device,
    max_samples: int = 50
) -> Dict[str, float]:
    """Validation loop."""
    text_to_latent.eval()
    latent_encoder.eval()
    
    total_loss = 0.0
    num_samples = 0
    
    for batch in dataloader:
        if num_samples >= max_samples:
            break
        
        mel = batch["mel"].to(device)
        text_ids = batch["text_ids"].to(device)
        text_mask = batch["text_mask"].to(device)
        reference_mel = batch["reference_mel"].to(device)
        
        # Encode to latents
        target_latent = latent_encoder(mel)
        target_compressed = compress_latents(target_latent, compression_factor=6)
        
        ref_latent = latent_encoder(reference_mel)
        ref_compressed = compress_latents(ref_latent, compression_factor=6)
        
        # Encode conditioning - reference FIRST
        reference_encoding = text_to_latent_raw.encode_reference(ref_compressed)
        text_encoding = text_to_latent_raw.encode_text(text_ids, reference_encoding, text_mask)
        
        # Flow-matching loss
        losses = loss_fn(
            model=text_to_latent_raw.vector_field,
            z1=target_compressed,
            text_encoding=text_encoding,
            reference_encoding=reference_encoding,
            text_mask=text_mask
        )
        
        total_loss += losses["total"].item() * mel.size(0)
        num_samples += mel.size(0)
    
    text_to_latent.train()
    
    return {
        "val_loss": total_loss / max(num_samples, 1)
    }


def main(args):
    """Main training function."""
    # Setup distributed
    rank, local_rank, world_size = setup_distributed()
    is_main = rank == 0
    device = torch.device(f"cuda:{local_rank}")
    
    # Load config
    config = OmegaConf.load(args.config)
    
    # Override with CLI args
    if args.batch_size:
        config.train_tts.batch_size = args.batch_size
    if args.lr:
        config.train_tts.learning_rate = args.lr
    
    # Override manifest paths
    train_manifest = args.train_manifest if args.train_manifest else config.data.train_manifest
    val_manifest = args.val_manifest if args.val_manifest else config.data.val_manifest
    data_dir = args.data_dir if args.data_dir else config.paths.get("data_dir", "data")
    
    if is_main:
        print(f"Train manifest: {train_manifest}")
        print(f"Val manifest: {val_manifest}")
        print(f"Data directory: {data_dir}")
    
    # Logging
    if is_main and not args.no_wandb:
        wandb.init(
            project=config.logging.project,
            name=f"text_to_latent_{time.strftime('%Y%m%d_%H%M%S')}",
            config=OmegaConf.to_container(config)
        )
    
    # Output directories
    if args.output_dir:
        output_base = Path(args.output_dir)
        checkpoint_dir = output_base / "checkpoints"
        sample_dir = output_base / "samples"
    else:
        checkpoint_dir = Path(config.output.checkpoint_dir) / "text_to_latent"
        sample_dir = Path(config.output.sample_dir) / "text_to_latent"
    
    if is_main:
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        sample_dir.mkdir(parents=True, exist_ok=True)
    
    # Audio processor & tokenizer
    audio_processor = AudioProcessor(
        sample_rate=config.audio.sample_rate,
        n_fft=config.audio.n_fft,
        hop_length=config.audio.hop_length,
        n_mels=config.audio.n_mels
    )
    
    tokenizer = CharacterTokenizer(
        languages=config.languages.supported
    )
    
    # Dataset
    train_dataset = TTSDataset(
        manifest_path=train_manifest,
        audio_processor=audio_processor,
        tokenizer=tokenizer,
        max_duration=config.data.max_audio_duration,
        min_duration=config.data.min_audio_duration,
        max_text_length=config.data.max_text_length,
        data_dir=data_dir
    )
    
    val_dataset = TTSDataset(
        manifest_path=val_manifest,
        audio_processor=audio_processor,
        tokenizer=tokenizer,
        data_dir=data_dir
    )
    
    if is_main:
        train_speakers = len(set(s.get("speaker_id", "unknown") for s in train_dataset.samples))
        print(f"Train samples: {len(train_dataset)}, Speakers: {train_speakers}")
        print(f"Val samples: {len(val_dataset)}")
    
    # DataLoader (NO expansion here - we do it in train_step)
    if world_size > 1:
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank
        )
        shuffle = False
    else:
        train_sampler = None
        shuffle = True
    
    num_workers = getattr(args, 'num_workers', 8)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.train_tts.batch_size,
        shuffle=shuffle,
        sampler=train_sampler,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=tts_collate_fn,
        drop_last=True,
        prefetch_factor=4,
        persistent_workers=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.train_tts.batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=tts_collate_fn
    )
    
    # Load pretrained latent encoder (frozen)
    ae_config = config.model.autoencoder
    latent_encoder = LatentEncoder(
        input_dim=ae_config.encoder.input_dim,
        hidden_dim=ae_config.encoder.hidden_dim,
        output_dim=ae_config.encoder.output_dim,
        num_blocks=ae_config.encoder.num_blocks,
        kernel_size=ae_config.encoder.kernel_size,
        intermediate_mult=ae_config.encoder.get("intermediate_mult", 4)
    ).to(device)
    
    if args.autoencoder_checkpoint:
        ae_ckpt = torch.load(args.autoencoder_checkpoint, map_location=device)
        encoder_state = ae_ckpt["encoder"]
        # Remove "module." prefix if saved with DDP
        encoder_state = {k.replace("module.", ""): v for k, v in encoder_state.items()}
        latent_encoder.load_state_dict(encoder_state)
        print(f"Loaded latent encoder from {args.autoencoder_checkpoint}")
    
    latent_encoder.eval()
    for param in latent_encoder.parameters():
        param.requires_grad = False
    
    # Text-to-Latent model config
    ttl_config = config.model.text_to_latent
    
    text_to_latent = TextToLatent(
        latent_dim=144,  # Compressed: 24 * 6
        vocab_size=tokenizer.vocab_size,
        text_embed_dim=ttl_config.text_encoder.embed_dim,
        text_hidden_dim=ttl_config.text_encoder.hidden_dim,
        ref_hidden_dim=ttl_config.reference_encoder.hidden_dim,
        vf_hidden_dim=ttl_config.vector_field.hidden_dim,
        num_ref_vectors=ttl_config.reference_encoder.num_output_vectors,
        gamma=config.larope.gamma,
        sigma_min=config.flow_matching.sigma_min,
        p_uncond=config.flow_matching.p_uncond,
        cfg_scale=config.flow_matching.cfg_scale
    ).to(device)
    
    # DDP wrapping
    if world_size > 1:
        text_to_latent = DDP(text_to_latent, device_ids=[local_rank])
    
    # Get unwrapped model for method access (DDP wraps in .module)
    text_to_latent_raw = text_to_latent.module if hasattr(text_to_latent, 'module') else text_to_latent
    
    # Loss
    loss_fn = FlowMatchingLoss(
        sigma_min=config.flow_matching.sigma_min,
        p_uncond=config.flow_matching.p_uncond
    )
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        text_to_latent.parameters(),
        lr=config.train_tts.learning_rate,
        betas=tuple(config.train_tts.optimizer.betas),
        weight_decay=config.train_tts.optimizer.weight_decay
    )
    
    # AMP scaler
    scaler = GradScaler(enabled=config.train_tts.amp.enabled)
    
    # Resume
    start_iteration = 0
    if args.resume:
        start_iteration = load_checkpoint(
            Path(args.resume), text_to_latent, optimizer, scaler
        )
        print(f"Resumed from iteration {start_iteration}")
    
    # Training loop
    total_iterations = config.train_tts.total_iterations
    checkpoint_interval = config.train_tts.checkpoint_interval
    validation_interval = config.train_tts.validation_interval
    log_interval = config.logging.log_interval
    lr_halve_interval = config.train_tts.lr_halve_interval
    expansion_factor = config.train_tts.expansion_factor
    base_lr = config.train_tts.learning_rate
    
    iteration = start_iteration
    epoch = 0
    
    if is_main:
        pbar = tqdm(total=total_iterations, initial=start_iteration, desc="Training TTS")
    
    while iteration < total_iterations:
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        for batch in train_loader:
            if iteration >= total_iterations:
                break
            
            # Adjust learning rate
            current_lr = adjust_learning_rate(
                optimizer, iteration, base_lr, lr_halve_interval
            )
            
            # Training step (–∑ context-sharing –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ)
            losses = train_step(
                batch, text_to_latent, text_to_latent_raw, latent_encoder,
                optimizer, loss_fn, scaler, device,
                expansion_factor=expansion_factor,
                use_amp=config.train_tts.amp.enabled
            )
            
            iteration += 1
            
            # Logging
            if is_main and iteration % log_interval == 0:
                losses["learning_rate"] = current_lr
                
                if not args.no_wandb:
                    wandb.log(losses, step=iteration)
                
                pbar.set_postfix({
                    "loss": f"{losses['loss']:.4f}",
                    "lr": f"{current_lr:.2e}"
                })
                pbar.update(log_interval)
            
            # Validation
            if is_main and iteration % validation_interval == 0:
                val_metrics = validate(
                    val_loader, text_to_latent, text_to_latent_raw, latent_encoder,
                    loss_fn, device
                )
                
                if not args.no_wandb:
                    wandb.log(val_metrics, step=iteration)
                
                print(f"\nValidation at {iteration}: {val_metrics}")
            
            # Checkpoint
            if is_main and iteration % checkpoint_interval == 0:
                save_checkpoint(
                    checkpoint_dir / f"checkpoint_{iteration}.pt",
                    iteration, text_to_latent, optimizer, scaler,
                    OmegaConf.to_container(config)
                )
        
        epoch += 1
    
    # Final checkpoint
    if is_main:
        save_checkpoint(
            checkpoint_dir / "checkpoint_final.pt",
            iteration, text_to_latent, optimizer, scaler,
            OmegaConf.to_container(config)
        )
        
        pbar.close()
        
        if not args.no_wandb:
            wandb.finish()
    
    cleanup_distributed()
    print("Training completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Text-to-Latent Module")
    parser.add_argument("--config", type=str, default="config/default.yaml")
    parser.add_argument("--resume", type=str, default=None)
    parser.add_argument("--autoencoder-checkpoint", type=str, required=True,
                        help="Path to pretrained autoencoder checkpoint")
    parser.add_argument("--train-manifest", type=str, default=None,
                        help="Override train manifest path")
    parser.add_argument("--val-manifest", type=str, default=None,
                        help="Override val manifest path")
    parser.add_argument("--data_dir", "--data-dir", type=str, default=None,
                        help="Base directory for audio files")
    parser.add_argument("--output_dir", "--output-dir", type=str, default=None,
                        help="Output directory for checkpoints")
    parser.add_argument("--batch-size", type=int, default=None)
    parser.add_argument("--num-workers", type=int, default=8,
                        help="Number of DataLoader workers")
    parser.add_argument("--lr", type=float, default=None)
    parser.add_argument("--no-wandb", action="store_true")
    
    args = parser.parse_args()
    main(args)



================================================================================
# –ö–Ü–ù–ï–¶–¨ –§–ê–ô–õ–£
# –û–±—Ä–æ–±–ª–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤: 32
================================================================================

