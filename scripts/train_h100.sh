#!/bin/bash
# =============================================================================
# Supertonic v2 TTS - H100 Quick Start Script
# =============================================================================
# Ğ¨Ğ²Ğ¸Ğ´ĞºĞ¸Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ H100 SXM Ğ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼Ğ¸
# Ğ’ĞºĞ»ÑÑ‡Ğ°Ñ”: Ñ‡Ğ°ÑÑ‚Ñ– checkpoints, Ğ¼Ğ¾Ğ½Ñ–Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³, auto-resume
# =============================================================================

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     Supertonic v2 TTS - H100 SXM Optimized                 â•‘"
echo "â•‘     ~2-3 Ğ´Ğ½Ñ– Ğ·Ğ°Ğ¼Ñ–ÑÑ‚ÑŒ 12-14 Ğ½Ğ° RTX 4090!                    â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# =============================================================================
# 1. ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° GPU
# =============================================================================
echo ""
echo "=== GPU Check ==="
nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv
echo ""

# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ‡Ğ¸ Ñ†Ğµ H100
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
if [[ "$GPU_NAME" == *"H100"* ]]; then
    echo "âœ… H100 detected! Using optimized config."
    CONFIG="config/h100_optimized.yaml"
else
    echo "âš ï¸ Not H100, using default config."
    CONFIG="config/default.yaml"
fi

# =============================================================================
# 2. CUDA Optimizations Ğ´Ğ»Ñ H100
# =============================================================================
echo ""
echo "=== Applying H100 CUDA Optimizations ==="

# TF32 Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ñ— ÑˆĞ²Ğ¸Ğ´ĞºĞ¾ÑÑ‚Ñ–
export CUDA_TF32_OVERRIDE=1
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1

# NCCL optimizations
export NCCL_IB_DISABLE=0
export NCCL_P2P_LEVEL=NVL

# Memory allocator
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

echo "âœ… CUDA optimizations applied"

# =============================================================================
# 3. Ğ’ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹
# =============================================================================
echo ""
echo "=== Installing Dependencies ==="
pip install --upgrade pip
pip install -r requirements.txt

# Flash Attention 2 Ğ´Ğ»Ñ H100 (Ğ²Ğ°Ğ¶Ğ»Ğ¸Ğ²Ğ¾!)
pip install flash-attn --no-build-isolation

echo "âœ… Dependencies installed"

# =============================================================================
# 4. Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ–Ğ²
# =============================================================================
echo ""
echo "=== Downloading Datasets ==="
python scripts/download_datasets.py --minimal

# =============================================================================
# 5. ĞŸÑ–Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° manifests
# =============================================================================
echo ""
echo "=== Preparing Manifests ==="
python scripts/prepare_manifest.py --data-dir data/raw --output-dir data/manifests

# =============================================================================
# 6. WandB Login (Ğ¾Ğ¿Ñ†Ñ–Ğ¹Ğ½Ğ¾)
# =============================================================================
echo ""
echo "=== WandB Setup ==="
if [ -n "$WANDB_API_KEY" ]; then
    wandb login "$WANDB_API_KEY"
    echo "âœ… WandB configured"
else
    echo "âš ï¸ WANDB_API_KEY not set. Logging to local files."
    echo "   Set it with: export WANDB_API_KEY=your_key"
fi

# =============================================================================
# 7. Training Pipeline
# =============================================================================
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     Starting Training Pipeline                             â•‘"
echo "â•‘     Estimated time: ~2-3 days on H100                      â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Ğ¤ÑƒĞ½ĞºÑ†Ñ–Ñ Ğ´Ğ»Ñ auto-resume
run_with_resume() {
    SCRIPT=$1
    CHECKPOINT_DIR=$2
    EXTRA_ARGS=$3
    
    # Ğ¨ÑƒĞºĞ°Ñ”Ğ¼Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ğ¹ checkpoint
    LATEST_CKPT=$(ls -t ${CHECKPOINT_DIR}/checkpoint_*.pt 2>/dev/null | head -1 || echo "")
    
    if [ -n "$LATEST_CKPT" ]; then
        echo "ğŸ“‚ Found checkpoint: $LATEST_CKPT"
        echo "   Resuming training..."
        python $SCRIPT --config $CONFIG --resume $LATEST_CKPT $EXTRA_ARGS
    else
        echo "ğŸ†• Starting fresh training..."
        python $SCRIPT --config $CONFIG $EXTRA_ARGS
    fi
}

# -----------------------------------------------------------------------------
# Ğ•Ñ‚Ğ°Ğ¿ 1: Autoencoder (~1 Ğ´ĞµĞ½ÑŒ Ğ½Ğ° H100)
# -----------------------------------------------------------------------------
echo ""
echo "=== Stage 1/3: Autoencoder Training ==="
echo "    Batch size: 48 | Iterations: 1.5M"
echo "    Estimated time: ~20-24 hours"
echo ""

run_with_resume "train_autoencoder.py" "checkpoints/autoencoder" ""

# -----------------------------------------------------------------------------
# Ğ•Ñ‚Ğ°Ğ¿ 2: Text-to-Latent (~1-1.5 Ğ´Ğ½Ñ– Ğ½Ğ° H100)
# -----------------------------------------------------------------------------
echo ""
echo "=== Stage 2/3: Text-to-Latent Training ==="
echo "    Batch size: 128Ã—4=512 | Iterations: 700k"
echo "    Estimated time: ~24-36 hours"
echo ""

AE_CKPT=$(ls -t checkpoints/autoencoder/checkpoint_*.pt | head -1)
run_with_resume "train_text_to_latent.py" "checkpoints/tts" "--autoencoder-checkpoint $AE_CKPT"

# -----------------------------------------------------------------------------
# Ğ•Ñ‚Ğ°Ğ¿ 3: Duration Predictor (~10-15 Ñ…Ğ²Ğ¸Ğ»Ğ¸Ğ½ Ğ½Ğ° H100)
# -----------------------------------------------------------------------------
echo ""
echo "=== Stage 3/3: Duration Predictor Training ==="
echo "    Batch size: 256 | Iterations: 3k"
echo "    Estimated time: ~10-15 minutes"
echo ""

run_with_resume "train_duration_predictor.py" "checkpoints/duration" ""

# =============================================================================
# 8. Done!
# =============================================================================
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     ğŸ‰ Training Complete!                                  â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Checkpoints saved in:"
echo "  - checkpoints/autoencoder/"
echo "  - checkpoints/tts/"
echo "  - checkpoints/duration/"
echo ""
echo "Test inference:"
echo "  python inference.py --text 'ĞŸÑ€Ğ¸Ğ²Ñ–Ñ‚, ÑĞº ÑĞ¿Ñ€Ğ°Ğ²Ğ¸?' \\"
echo "      --reference samples/reference.wav \\"
echo "      --output output.wav"
echo ""
echo "Export to ONNX:"
echo "  python export_onnx.py --checkpoint-dir checkpoints"
